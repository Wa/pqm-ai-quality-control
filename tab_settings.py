import streamlit as st
import requests
import json
import os
from pathlib import Path
from config import CONFIG

def get_user_settings_file(session_id):
    """Get the path to the user's settings file."""
    settings_dir = Path("user_settings")
    settings_dir.mkdir(exist_ok=True)
    return settings_dir / f"user_{session_id}_settings.json"

def save_user_settings(session_id, settings):
    """Save user settings to a JSON file."""
    try:
        settings_file = get_user_settings_file(session_id)
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜è®¾ç½®å¤±è´¥: {e}")
        return False

def load_user_settings(session_id):
    """Load user settings from JSON file."""
    try:
        settings_file = get_user_settings_file(session_id)
        if settings_file.exists():
            with open(settings_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # Return default settings if file doesn't exist
            return {
                'llm_backend': 'ollama',
                'ollama_model': CONFIG['llm']['ollama_model'],
                'openai_model': CONFIG['llm']['openai_model'],
                'ollama_temperature': 0.7,
                'ollama_top_p': 0.9,
                'ollama_top_k': 40,
                'ollama_repeat_penalty': 1.1,
                'ollama_num_ctx': 65536,
                'ollama_num_thread': 4,
                'openai_temperature': 0.7,
                'openai_top_p': 1.0,
                'openai_max_tokens': 65536,
                'openai_presence_penalty': 0.0,
                'openai_frequency_penalty': 0.0
            }
    except Exception as e:
        st.error(f"åŠ è½½è®¾ç½®å¤±è´¥: {e}")
        # Return default settings on error
        return {
            'llm_backend': 'ollama',
            'ollama_model': CONFIG['llm']['ollama_model'],
            'openai_model': CONFIG['llm']['openai_model'],
            'ollama_temperature': 0.7,
            'ollama_top_p': 0.9,
            'ollama_top_k': 40,
            'ollama_repeat_penalty': 1.1,
            'ollama_num_ctx': 65536,
            'ollama_num_thread': 4,
            'openai_temperature': 0.7,
            'openai_top_p': 1.0,
            'openai_max_tokens': 65536,
            'openai_presence_penalty': 0.0,
            'openai_frequency_penalty': 0.0
        }

def save_current_settings(session_id):
    """Save all current session state settings to file."""
    current_settings = {
        'llm_backend': st.session_state.get(f'llm_backend_{session_id}', 'ollama'),
        'ollama_model': st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model']),
        'openai_model': st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model']),
        'ollama_temperature': st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
        'ollama_top_p': st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
        'ollama_top_k': st.session_state.get(f'ollama_top_k_{session_id}', 40),
        'ollama_repeat_penalty': st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
        'ollama_num_ctx': st.session_state.get(f'ollama_num_ctx_{session_id}', 65536),
        'ollama_num_thread': st.session_state.get(f'ollama_num_thread_{session_id}', 4),
        'openai_temperature': st.session_state.get(f'openai_temperature_{session_id}', 0.7),
        'openai_top_p': st.session_state.get(f'openai_top_p_{session_id}', 1.0),
        'openai_max_tokens': st.session_state.get(f'openai_max_tokens_{session_id}', 65536),
        'openai_presence_penalty': st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
        'openai_frequency_penalty': st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0)
    }
    return save_user_settings(session_id, current_settings)

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_ollama_models(host: str):
    """Get available Ollama models from the specified server."""
    try:
        response = requests.get(f"{host}/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return [m.get('name') or m.get('model') for m in models]
        else:
            return []
    except Exception as e:
        st.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨: {e}")
        return []

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_ollama_model_info(model_name, host: str):
    """Get detailed information about a specific Ollama model on the specified server."""
    try:
        response = requests.post(
            f"{host}/api/show",
            json={"name": model_name},
            timeout=3
        )
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except Exception:
        return None

@st.cache_data(ttl=60)
def get_ollama_tags_map(host: str):
    """Return a mapping of model name -> tag info (size, modified_at, digest, etc.) from the specified server."""
    try:
        response = requests.get(f"{host}/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return {(m.get('name') or m.get('model')): m for m in models}
    except Exception:
        pass
    return {}

def _human_size(num_bytes: int) -> str:
    try:
        num = int(num_bytes)
    except Exception:
        return "N/A"
    units = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while num >= 1024 and i < len(units) - 1:
        num /= 1024.0
        i += 1
    return f"{num:.1f} {units[i]}"

@st.cache_data(ttl=30)  # Cache for 30 seconds
def test_ollama_connection(host: str):
    """Test connection to the specified Ollama server."""
    try:
        response = requests.get(f"{host}/api/tags", timeout=3)
        return response.status_code == 200
    except Exception:
        return False

@st.cache_data(ttl=30)  # Cache for 30 seconds
def test_openai_connection():
    """Test connection to OpenAI API."""
    try:
        headers = {
            "Authorization": f"Bearer {CONFIG['llm']['openai_api_key']}",
            "Content-Type": "application/json"
        }
        response = requests.get(
            f"{CONFIG['llm']['openai_base_url']}/models",
            headers=headers,
            timeout=5
        )
        return response.status_code == 200
    except Exception:
        return False

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_openai_models():
    """Get available OpenAI models."""
    try:
        headers = {
            "Authorization": f"Bearer {CONFIG['llm']['openai_api_key']}",
            "Content-Type": "application/json"
        }
        response = requests.get(
            f"{CONFIG['llm']['openai_base_url']}/models",
            headers=headers,
            timeout=5
        )
        if response.status_code == 200:
            models = response.json().get('data', [])
            # Filter for chat completion models
            chat_models = [model['id'] for model in models if 'gpt' in model['id'].lower()]
            return chat_models
        else:
            return []
    except Exception as e:
        st.warning(f"æ— æ³•è¿æ¥åˆ°OpenAI API: {e}")
        return []

def render_settings_tab(session_id):
    """Render the settings tab using native Streamlit components."""
    
    # Handle None session_id (user not logged in)
    if session_id is None:
        st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        return
    
    # Load user settings from file (persistent across restarts)
    user_settings = load_user_settings(session_id)
    
    # Initialize session state from file settings
    if f'llm_backend_{session_id}' not in st.session_state:
        st.session_state[f'llm_backend_{session_id}'] = user_settings['llm_backend']
    if f'ollama_model_{session_id}' not in st.session_state:
        st.session_state[f'ollama_model_{session_id}'] = user_settings['ollama_model']
    if f'openai_model_{session_id}' not in st.session_state:
        st.session_state[f'openai_model_{session_id}'] = user_settings['openai_model']
    
    # Initialize other parameters from file settings
    for param, value in user_settings.items():
        if param not in ['llm_backend', 'ollama_model', 'openai_model']:
            session_key = f'{param}_{session_id}'
            if session_key not in st.session_state:
                st.session_state[session_key] = value
    
    # Create a container with constrained width for the settings page
    with st.container(key="settings-container"):
        # Add CSS to constrain the width of this specific container
        st.markdown("""
        <style>
        .st-key-settings-container {
            max-width: 800px !important;
            margin: 0 auto !important;
        }
        </style>
        """, unsafe_allow_html=True)
        
        # User section: current user, active users, logout, and clear saved username
        st.header("ğŸ‘¤ ç”¨æˆ·ä¸ä¼šè¯")
        st.write(f"**å½“å‰ç”¨æˆ·:** {session_id}")

        cols = st.columns(1)
        with cols[0]:
            if st.button("ğŸšª é€€å‡ºç™»å½•", key=f"logout_btn_{session_id}"):
                try:
                    from util import deactivate_user_session
                    deactivate_user_session(session_id)
                except Exception:
                    pass
                # Clear session state
                for k in list(st.session_state.keys()):
                    del st.session_state[k]
                # Keep 'user' to remember username on this PC; remove only auth flag
                try:
                    if "auth" in st.query_params:
                        del st.query_params["auth"]
                except Exception:
                    pass
                st.rerun()

        st.divider()
        
        # LLM Backend Selection
        st.header("ğŸ¤– å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©")
        
        llm_options = {
            "Ollama (10.31.60.127:11434)": "ollama_127",
            "Ollama (10.31.60.9:11434)": "ollama_9",
            "OpenAI (sg.uiuiapi.com)": "openai"
        }
        
        # Get current LLM choice from session state
        current_llm_backend = st.session_state.get(f'llm_backend_{session_id}', 'ollama_127')
        
        # Find the display name for current backend
        current_display_name = None
        for display_name, backend in llm_options.items():
            if backend == current_llm_backend:
                current_display_name = display_name
                break
        
        # Create the selectbox with current selection
        selected_display_name = st.selectbox(
            "é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹", 
            list(llm_options.keys()), 
            index=list(llm_options.keys()).index(current_display_name) if current_display_name else 0,
            key=f"settings_llm_select_{session_id}"
        )
        
        # Get the backend value from the selection
        selected_backend = llm_options[selected_display_name]
        
        # Update session state when selection changes (this is appropriate since it's not a widget value)
        if selected_backend != current_llm_backend:
            st.session_state[f'llm_backend_{session_id}'] = selected_backend
            st.success(f"å·²åˆ‡æ¢åˆ°: {selected_display_name}")
            
            # Save settings to file for persistence
            save_current_settings(session_id)
        
        st.divider()
        
        # Connection Status
        st.header("ğŸ”— è¿æ¥çŠ¶æ€")

        # Resolve host for selected backend
        host = CONFIG['llm']['ollama_host']
        if selected_backend == "ollama_9":
            host = host.replace("10.31.60.127", "10.31.60.9")
        
        col1, col2 = st.columns(2)
        with col1:
            if selected_backend in ("ollama_127", "ollama_9"):
                if test_ollama_connection(host):
                    st.success("âœ… OllamaæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
                else:
                    st.error("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨")
            elif selected_backend == "openai":
                if test_openai_connection():
                    st.success("âœ… OpenAI APIè¿æ¥æ­£å¸¸")
                else:
                    st.error("âŒ æ— æ³•è¿æ¥åˆ°OpenAI API")
        
        with col2:
            st.info(f"""
            **å½“å‰åç«¯:** {selected_display_name}  
            **çŠ¶æ€:** {'åœ¨çº¿' if (selected_backend in ("ollama_127", "ollama_9") and test_ollama_connection(host)) or (selected_backend == "openai" and test_openai_connection()) else 'ç¦»çº¿'}
            """)

        st.divider()
        
        # Model Configuration
        st.header("âš™ï¸ æ¨¡å‹é…ç½®")
        
        if selected_backend in ("ollama_127", "ollama_9"):
            # Initialize selected_model variable from session state
            selected_model = st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model'])
            
            # Ollama Model Selection
            with st.spinner("æ­£åœ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨..."):
                available_models = get_ollama_models(host)
            
            if available_models:
                current_model = st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model'])

                # Ensure current model is in the list, otherwise use first available
                if current_model not in available_models:
                    current_model = available_models[0]
                    st.session_state[f'ollama_model_{session_id}'] = current_model
                    selected_model = current_model
                
                selected_model = st.selectbox(
                    "é€‰æ‹©Ollamaæ¨¡å‹",
                    available_models,
                    index=available_models.index(current_model),
                    key=f"ollama_model_select_{session_id}"
                )
                
                # Update the model session state when selection changes
                if selected_model != current_model:
                    st.session_state[f'ollama_model_{session_id}'] = selected_model
                    st.success(f"âœ… å·²åˆ‡æ¢åˆ°: {selected_model} (å°†åœ¨ä¸‹æ¬¡è¿è¡Œæ—¶ç”Ÿæ•ˆ)")
                    
                    # Save settings to file for persistence
                    save_current_settings(session_id)
                    
                    # No st.rerun() needed - settings are saved and will apply to future runs
                    # This prevents interrupting any currently running analysis
            
                # Model Information - display always, enrich from /api/tags when /api/show lacks fields
                try:
                    show_info = get_ollama_model_info(selected_model, host) or {}
                    tags_map = get_ollama_tags_map(host) or {}
                    tag_info = tags_map.get(selected_model, {})

                    name = show_info.get('name') or tag_info.get('name') or selected_model
                    size_val = show_info.get('size') or tag_info.get('size')
                    size_h = _human_size(size_val) if size_val is not None else 'N/A'
                    modified = show_info.get('modified_at') or tag_info.get('modified_at') or 'N/A'
                    param_sz = (show_info.get('parameter_size')
                                or (show_info.get('model_info') or {}).get('parameter_size')
                                or 'N/A')
                    quant_lvl = (show_info.get('quantization_level')
                                 or (show_info.get('model_info') or {}).get('quantization_level')
                                 or 'N/A')

                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**æ¨¡å‹åç§°:** {name}")
                        st.write(f"**æ¨¡å‹å¤§å°:** {size_h}")
                        st.write(f"**ä¿®æ”¹æ—¶é—´:** {modified}")
                    with col2:
                        st.write(f"**å‚æ•°æ•°é‡:** {param_sz}")
                        st.write(f"**é‡åŒ–çº§åˆ«:** {quant_lvl}")
                except Exception as e:
                    st.warning(f"æ— æ³•è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯: {e}")

            # Ollama Parameters
            st.subheader("Ollamaå‚æ•°è®¾ç½®")
            
            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider(
                    "Temperature (æ¸©åº¦)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                    step=0.1,
                    help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼äº§ç”Ÿæ›´ç¡®å®šæ€§çš„è¾“å‡ºï¼Œè¾ƒé«˜çš„å€¼äº§ç”Ÿæ›´åˆ›é€ æ€§çš„è¾“å‡ºã€‚",
                    key=f"ollama_temperature_{session_id}"
                )
                
                top_p = st.slider(
                    "Top-p (æ ¸é‡‡æ ·)",
                    min_value=0.0,
                    max_value=1.0,
                    value=st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                    step=0.1,
                    help="æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§ã€‚",
                    key=f"ollama_top_p_{session_id}"
                )
            
            with col2:
                top_k = st.slider(
                    "Top-k",
                    min_value=1,
                    max_value=100,
                    value=st.session_state.get(f'ollama_top_k_{session_id}', 40),
                    step=1,
                    help="é™åˆ¶æ¯æ¬¡é€‰æ‹©æ—¶è€ƒè™‘çš„è¯æ±‡æ•°é‡ã€‚",
                    key=f"ollama_top_k_{session_id}"
                )
                
                repeat_penalty = st.slider(
                    "Repeat Penalty (é‡å¤æƒ©ç½š)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                    step=0.1,
                    help="å‡å°‘é‡å¤å†…å®¹çš„ç”Ÿæˆã€‚",
                    key=f"ollama_repeat_penalty_{session_id}"
                )
            
            # Advanced Ollama Settings (always visible)
            col1, col2 = st.columns(2)
            with col1:
                # Determine dynamic max context length from model info
                dynamic_max_ctx = 8192
                try:
                    info = get_ollama_model_info(selected_model) or {}
                    mi = info.get('model_info', {}) or {}
                    # Try common keys first
                    for key in [
                        'gptoss.context_length',
                        'qwen3.context_length',
                        'llama.context_length',
                        'general.context_length'
                    ]:
                        if key in mi and isinstance(mi[key], int):
                            dynamic_max_ctx = int(mi[key])
                            break
                    else:
                        # Fallback: search for any *.context_length field
                        for k, v in mi.items():
                            if isinstance(k, str) and k.endswith('context_length') and isinstance(v, int):
                                dynamic_max_ctx = int(v)
                                break
                except Exception:
                    dynamic_max_ctx = 8192
                # Default to 65536; allow values beyond model-reported max (for RoPE scaling / custom builds)
                _default_ctx = st.session_state.get(f'ollama_num_ctx_{session_id}', 65536)
                num_ctx = st.number_input(
                    "ä¸Šä¸‹æ–‡çª—å£å¤§å°",
                    min_value=512,
                    max_value=131072,
                    value=_default_ctx,
                    step=512,
                    help="æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚",
                    key=f"ollama_num_ctx_{session_id}"
                )
                # Hint if chosen value exceeds the model-reported max
                try:
                    if num_ctx > int(dynamic_max_ctx):
                        st.caption(f"æç¤º: å½“å‰æ¨¡å‹å»ºè®®æœ€å¤§ä¸º {dynamic_max_ctx}ã€‚è¾ƒå¤§çš„ä¸Šä¸‹æ–‡å¯èƒ½ä¾èµ–RoPEç¼©æ”¾/è‡ªå®šä¹‰æ¨¡å‹æˆ–å¯¼è‡´å†…å­˜å‹åŠ›ã€‚")
                except Exception:
                    pass
            
            with col2:
                num_thread = st.number_input(
                    "çº¿ç¨‹æ•°",
                    min_value=1,
                    max_value=16,
                    value=st.session_state.get(f'ollama_num_thread_{session_id}', 4),
                    step=1,
                    help="ç”¨äºæ¨ç†çš„CPUçº¿ç¨‹æ•°ã€‚",
                    key=f"ollama_num_thread_{session_id}"
                )
        
        elif selected_backend == "openai":
            # OpenAI Model Selection
            available_models = get_openai_models()
            if available_models:
                current_model = st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model'])
                
                selected_model = st.selectbox(
                    "é€‰æ‹©OpenAIæ¨¡å‹",
                    available_models,
                    index=available_models.index(current_model) if current_model in available_models else 0,
                    key=f"openai_model_select_{session_id}"
                )
                
                # Update the model session state when selection changes
                if selected_model != current_model:
                    st.session_state[f'openai_model_{session_id}'] = selected_model
                    st.success(f"å·²åˆ‡æ¢åˆ°: {selected_model}")
            else:
                st.warning("æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨")
                st.session_state[f'openai_model_{session_id}'] = CONFIG['llm']['openai_model']
            
            # OpenAI Parameters
            st.subheader("OpenAIå‚æ•°è®¾ç½®")
            
            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider(
                    "Temperature (æ¸©åº¦)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                    step=0.1,
                    help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚0è¡¨ç¤ºå®Œå…¨ç¡®å®šæ€§ï¼Œ2è¡¨ç¤ºæœ€å¤§éšæœºæ€§ã€‚",
                    key=f"openai_temperature_{session_id}"
                )
                
                top_p = st.slider(
                    "Top-p (æ ¸é‡‡æ ·)",
                    min_value=0.0,
                    max_value=1.0,
                    value=st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                    step=0.1,
                    help="æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§ã€‚",
                    key=f"openai_top_p_{session_id}"
                )
            
            with col2:
                max_tokens = st.number_input(
                    "æœ€å¤§è¾“å‡ºé•¿åº¦",
                    min_value=1,
                    max_value=4096,
                    value=st.session_state.get(f'openai_max_tokens_{session_id}', 65536),
                    step=1,
                    help="ç”Ÿæˆå“åº”çš„æœ€å¤§tokenæ•°é‡ã€‚",
                    key=f"openai_max_tokens_{session_id}"
                )
                
                presence_penalty = st.slider(
                    "Presence Penalty (å­˜åœ¨æƒ©ç½š)",
                    min_value=-2.0,
                    max_value=2.0,
                    value=st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                    step=0.1,
                    help="å‡å°‘æ¨¡å‹é‡å¤ç›¸åŒä¸»é¢˜çš„å€¾å‘ã€‚",
                    key=f"openai_presence_penalty_{session_id}"
                )
            
            # Advanced OpenAI Settings
            with st.expander("é«˜çº§è®¾ç½®", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    frequency_penalty = st.slider(
                        "Frequency Penalty (é¢‘ç‡æƒ©ç½š)",
                        min_value=-2.0,
                        max_value=2.0,
                        value=st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0),
                        step=0.1,
                        help="å‡å°‘æ¨¡å‹é‡å¤ç›¸åŒè¯æ±‡çš„å€¾å‘ã€‚",
                        key=f"openai_frequency_penalty_{session_id}"
                    )
                
                with col2:
                    logit_bias = st.text_input(
                        "Logit Bias (è¯æ±‡åå¥½)",
                        value=st.session_state.get(f'openai_logit_bias_{session_id}', '{}'),
                        help="JSONæ ¼å¼çš„è¯æ±‡åå¥½è®¾ç½®ï¼Œä¾‹å¦‚: {\"word\": 0.5}",
                        key=f"openai_logit_bias_{session_id}"
                    )
        
        st.divider()
        
        # Current Configuration Overview
        st.header("ğŸ“‹ å½“å‰é…ç½®æ¦‚è§ˆ")

        if selected_backend in ("ollama_127", "ollama_9"):
            # Two-column compact overview (no JSON toggle)
            col_left, col_right = st.columns(2)
            with col_left:
                st.write("**åç«¯:**", selected_display_name)
                st.write("**ä¸»æœº:**", host)
                st.write("**å½“å‰æ¨¡å‹:**", selected_model)
            with col_right:
                st.write("**Temperature:**", st.session_state.get(f'ollama_temperature_{session_id}', 0.7))
                st.write("**Top-p:**", st.session_state.get(f'ollama_top_p_{session_id}', 0.9))
                st.write("**Top-k:**", st.session_state.get(f'ollama_top_k_{session_id}', 40))
                st.write("**Repeat Penalty:**", st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1))
                st.write("**num_ctx:**", st.session_state.get(f'ollama_num_ctx_{session_id}', 65536))
                st.write("**num_thread:**", st.session_state.get(f'ollama_num_thread_{session_id}', 4))

        elif selected_backend == "openai":
            # Two-column compact overview (no JSON toggle)
            col_left, col_right = st.columns(2)
            with col_left:
                st.write("**åç«¯:**", selected_display_name)
                st.write("**APIåœ°å€:**", CONFIG['llm']['openai_base_url'])
                st.write("**å½“å‰æ¨¡å‹:**", st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model']))
            with col_right:
                st.write("**Temperature:**", st.session_state.get(f'openai_temperature_{session_id}', 0.7))
                st.write("**Top-p:**", st.session_state.get(f'openai_top_p_{session_id}', 1.0))
                st.write("**Max Tokens:**", st.session_state.get(f'openai_max_tokens_{session_id}', 65536))
                st.write("**Presence Penalty:**", st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0))
                st.write("**Frequency Penalty:**", st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0))
                st.write("**Logit Bias:**", st.session_state.get(f'openai_logit_bias_{session_id}', '{}'))
        
        st.divider()
        
        # Documentation Links
        st.header("ğŸ“š ç›¸å…³æ–‡æ¡£")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Ollamaæ–‡æ¡£")
            st.link_button("GitHub Repository", "https://github.com/ollama/ollama")
            st.link_button("APIæ–‡æ¡£", "https://github.com/ollama/ollama/blob/main/docs/api.md")
            st.link_button("æ¨¡å‹å‚æ•°", "https://github.com/ollama/ollama/blob/main/docs/modelfile.md")
        
        with col2:
            st.subheader("OpenAIæ–‡æ¡£")
            st.link_button("APIæ–‡æ¡£", "https://platform.openai.com/docs/api-reference")
            st.link_button("æ¨¡å‹å‚æ•°", "https://platform.openai.com/docs/api-reference/chat/create")
            st.link_button("UIUIApi", "https://sg.uiuiapi.com/") 

        # Persist any changes made during this run (sliders, inputs, etc.)
        # Model selection already saves explicitly above; this ensures other parameters are saved too.
        save_current_settings(session_id)