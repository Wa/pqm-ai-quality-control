import streamlit as st
import requests
import json
import os
from pathlib import Path
from config import CONFIG

def get_user_settings_file(session_id):
    """Get the path to the user's settings file."""
    settings_dir = Path("user_settings")
    settings_dir.mkdir(exist_ok=True)
    return settings_dir / f"user_{session_id}_settings.json"

def save_user_settings(session_id, settings):
    """Save user settings to a JSON file."""
    try:
        settings_file = get_user_settings_file(session_id)
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜è®¾ç½®å¤±è´¥: {e}")
        return False

def load_user_settings(session_id):
    """Load user settings from JSON file."""
    try:
        settings_file = get_user_settings_file(session_id)
        if settings_file.exists():
            with open(settings_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # Return default settings if file doesn't exist
            return {
                'llm_backend': 'ollama',
                'ollama_model': CONFIG['llm']['ollama_model'],
                'openai_model': CONFIG['llm']['openai_model'],
                'ollama_temperature': 0.7,
                'ollama_top_p': 0.9,
                'ollama_top_k': 40,
                'ollama_repeat_penalty': 1.1,
                'openai_temperature': 0.7,
                'openai_top_p': 1.0,
                'openai_max_tokens': 2048,
                'openai_presence_penalty': 0.0,
                'openai_frequency_penalty': 0.0
            }
    except Exception as e:
        st.error(f"åŠ è½½è®¾ç½®å¤±è´¥: {e}")
        # Return default settings on error
        return {
            'llm_backend': 'ollama',
            'ollama_model': CONFIG['llm']['ollama_model'],
            'openai_model': CONFIG['llm']['openai_model'],
            'ollama_temperature': 0.7,
            'ollama_top_p': 0.9,
            'ollama_top_k': 40,
            'ollama_repeat_penalty': 1.1,
            'openai_temperature': 0.7,
            'openai_top_p': 1.0,
            'openai_max_tokens': 2048,
            'openai_presence_penalty': 0.0,
            'openai_frequency_penalty': 0.0
        }

def save_current_settings(session_id):
    """Save all current session state settings to file."""
    current_settings = {
        'llm_backend': st.session_state.get(f'llm_backend_{session_id}', 'ollama'),
        'ollama_model': st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model']),
        'openai_model': st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model']),
        'ollama_temperature': st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
        'ollama_top_p': st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
        'ollama_top_k': st.session_state.get(f'ollama_top_k_{session_id}', 40),
        'ollama_repeat_penalty': st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
        'openai_temperature': st.session_state.get(f'openai_temperature_{session_id}', 0.7),
        'openai_top_p': st.session_state.get(f'openai_top_p_{session_id}', 1.0),
        'openai_max_tokens': st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
        'openai_presence_penalty': st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
        'openai_frequency_penalty': st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0)
    }
    return save_user_settings(session_id, current_settings)

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_ollama_models():
    """Get available Ollama models from the local server."""
    try:
        response = requests.get(f"{CONFIG['llm']['ollama_host']}/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return [model['name'] for model in models]
        else:
            return []
    except Exception as e:
        st.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨: {e}")
        return []

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_ollama_model_info(model_name):
    """Get detailed information about a specific Ollama model."""
    try:
        response = requests.post(
            f"{CONFIG['llm']['ollama_host']}/api/show",
            json={"name": model_name},
            timeout=3
        )
        if response.status_code == 200:
            return response.json()
        else:
            return None
    except Exception:
        return None

@st.cache_data(ttl=30)  # Cache for 30 seconds
def test_ollama_connection():
    """Test connection to Ollama server."""
    try:
        response = requests.get(f"{CONFIG['llm']['ollama_host']}/api/tags", timeout=3)
        return response.status_code == 200
    except Exception:
        return False

@st.cache_data(ttl=30)  # Cache for 30 seconds
def test_openai_connection():
    """Test connection to OpenAI API."""
    try:
        headers = {
            "Authorization": f"Bearer {CONFIG['llm']['openai_api_key']}",
            "Content-Type": "application/json"
        }
        response = requests.get(
            f"{CONFIG['llm']['openai_base_url']}/models",
            headers=headers,
            timeout=5
        )
        return response.status_code == 200
    except Exception:
        return False

@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_openai_models():
    """Get available OpenAI models."""
    try:
        headers = {
            "Authorization": f"Bearer {CONFIG['llm']['openai_api_key']}",
            "Content-Type": "application/json"
        }
        response = requests.get(
            f"{CONFIG['llm']['openai_base_url']}/models",
            headers=headers,
            timeout=5
        )
        if response.status_code == 200:
            models = response.json().get('data', [])
            # Filter for chat completion models
            chat_models = [model['id'] for model in models if 'gpt' in model['id'].lower()]
            return chat_models
        else:
            return []
    except Exception as e:
        st.warning(f"æ— æ³•è¿æ¥åˆ°OpenAI API: {e}")
        return []

def render_settings_tab(session_id):
    """Render the settings tab using native Streamlit components."""
    
    # Handle None session_id (user not logged in)
    if session_id is None:
        st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        return
    
    # Load user settings from file (persistent across restarts)
    user_settings = load_user_settings(session_id)
    
    # Initialize session state from file settings
    if f'llm_backend_{session_id}' not in st.session_state:
        st.session_state[f'llm_backend_{session_id}'] = user_settings['llm_backend']
    if f'ollama_model_{session_id}' not in st.session_state:
        st.session_state[f'ollama_model_{session_id}'] = user_settings['ollama_model']
    if f'openai_model_{session_id}' not in st.session_state:
        st.session_state[f'openai_model_{session_id}'] = user_settings['openai_model']
    
    # Initialize other parameters from file settings
    for param, value in user_settings.items():
        if param not in ['llm_backend', 'ollama_model', 'openai_model']:
            session_key = f'{param}_{session_id}'
            if session_key not in st.session_state:
                st.session_state[session_key] = value
    
    # Create a container with constrained width for the settings page
    with st.container(key="settings-container"):
        # Add CSS to constrain the width of this specific container
        st.markdown("""
        <style>
        .st-key-settings-container {
            max-width: 800px !important;
            margin: 0 auto !important;
        }
        </style>
        """, unsafe_allow_html=True)
        
        # Page header
        st.title("è®¾ç½®")
        st.caption("é…ç½®å¤§è¯­è¨€æ¨¡å‹å‚æ•°å’Œè¿æ¥è®¾ç½®")
        st.divider()
        
        # LLM Backend Selection
        st.header("ğŸ¤– å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©")
        
        llm_options = {
            "Ollama (local)": "ollama",
            "OpenAI (sg.uiuiapi.com)": "openai"
        }
        
        # Get current LLM choice from session state
        current_llm_backend = st.session_state.get(f'llm_backend_{session_id}', 'ollama')
        
        # Find the display name for current backend
        current_display_name = None
        for display_name, backend in llm_options.items():
            if backend == current_llm_backend:
                current_display_name = display_name
                break
        
        # Create the selectbox with current selection
        selected_display_name = st.selectbox(
            "é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹", 
            list(llm_options.keys()), 
            index=list(llm_options.keys()).index(current_display_name) if current_display_name else 0,
            key=f"settings_llm_select_{session_id}"
        )
        
        # Get the backend value from the selection
        selected_backend = llm_options[selected_display_name]
        
        # Update session state when selection changes (this is appropriate since it's not a widget value)
        if selected_backend != current_llm_backend:
            st.session_state[f'llm_backend_{session_id}'] = selected_backend
            st.success(f"å·²åˆ‡æ¢åˆ°: {selected_display_name}")
            
            # Save settings to file for persistence
            save_current_settings(session_id)
        
        st.divider()
        
        # Connection Status
        st.header("ğŸ”— è¿æ¥çŠ¶æ€")
        
        col1, col2 = st.columns(2)
        with col1:
            if selected_backend == "ollama":
                if test_ollama_connection():
                    st.success("âœ… OllamaæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
                else:
                    st.error("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨")
            elif selected_backend == "openai":
                if test_openai_connection():
                    st.success("âœ… OpenAI APIè¿æ¥æ­£å¸¸")
                else:
                    st.error("âŒ æ— æ³•è¿æ¥åˆ°OpenAI API")
        
        with col2:
            st.info(f"""
            **å½“å‰åç«¯:** {selected_display_name}  
            **çŠ¶æ€:** {'åœ¨çº¿' if (selected_backend == "ollama" and test_ollama_connection()) or (selected_backend == "openai" and test_openai_connection()) else 'ç¦»çº¿'}
            """)
        
        st.divider()
        
        # Model Configuration
        st.header("âš™ï¸ æ¨¡å‹é…ç½®")
        
        if selected_backend == "ollama":
            # Initialize selected_model variable from session state
            selected_model = st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model'])
            
            # Ollama Model Selection
            with st.spinner("æ­£åœ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨..."):
                available_models = get_ollama_models()
            
            if available_models:
                current_model = st.session_state.get(f'ollama_model_{session_id}', CONFIG['llm']['ollama_model'])
                
                # Ensure current model is in the list, otherwise use first available
                if current_model not in available_models:
                    current_model = available_models[0]
                    st.session_state[f'ollama_model_{session_id}'] = current_model
                    selected_model = current_model
                
                selected_model = st.selectbox(
                    "é€‰æ‹©Ollamaæ¨¡å‹",
                    available_models,
                    index=available_models.index(current_model),
                    key=f"ollama_model_select_{session_id}"
                )
                
                # Update the model session state when selection changes
                if selected_model != current_model:
                    st.session_state[f'ollama_model_{session_id}'] = selected_model
                    st.success(f"âœ… å·²åˆ‡æ¢åˆ°: {selected_model} (å°†åœ¨ä¸‹æ¬¡è¿è¡Œæ—¶ç”Ÿæ•ˆ)")
                    
                    # Save settings to file for persistence
                    save_current_settings(session_id)
                    
                    # No st.rerun() needed - settings are saved and will apply to future runs
                    # This prevents interrupting any currently running analysis
            
            # Model Information - with error handling
            try:
                model_info = get_ollama_model_info(selected_model)
                if model_info:
                    with st.expander("æ¨¡å‹è¯¦ç»†ä¿¡æ¯", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**æ¨¡å‹åç§°:** {model_info.get('name', 'N/A')}")
                            st.write(f"**æ¨¡å‹å¤§å°:** {model_info.get('size', 'N/A')} bytes")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {model_info.get('modified_at', 'N/A')}")
                        with col2:
                            st.write(f"**å‚æ•°æ•°é‡:** {model_info.get('parameter_size', 'N/A')}")
                            st.write(f"**é‡åŒ–çº§åˆ«:** {model_info.get('quantization_level', 'N/A')}")
            except Exception as e:
                st.warning(f"æ— æ³•è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯: {e}")
            else:
                st.warning("æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡å™¨è¿æ¥")
                # Don't override the user's selection when model list can't be fetched
                # Keep the current selected_model value
            
            # Ollama Parameters
            st.subheader("Ollamaå‚æ•°è®¾ç½®")
            
            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider(
                    "Temperature (æ¸©åº¦)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                    step=0.1,
                    help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼äº§ç”Ÿæ›´ç¡®å®šæ€§çš„è¾“å‡ºï¼Œè¾ƒé«˜çš„å€¼äº§ç”Ÿæ›´åˆ›é€ æ€§çš„è¾“å‡ºã€‚",
                    key=f"ollama_temperature_{session_id}"
                )
                
                top_p = st.slider(
                    "Top-p (æ ¸é‡‡æ ·)",
                    min_value=0.0,
                    max_value=1.0,
                    value=st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                    step=0.1,
                    help="æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§ã€‚",
                    key=f"ollama_top_p_{session_id}"
                )
            
            with col2:
                top_k = st.slider(
                    "Top-k",
                    min_value=1,
                    max_value=100,
                    value=st.session_state.get(f'ollama_top_k_{session_id}', 40),
                    step=1,
                    help="é™åˆ¶æ¯æ¬¡é€‰æ‹©æ—¶è€ƒè™‘çš„è¯æ±‡æ•°é‡ã€‚",
                    key=f"ollama_top_k_{session_id}"
                )
                
                repeat_penalty = st.slider(
                    "Repeat Penalty (é‡å¤æƒ©ç½š)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                    step=0.1,
                    help="å‡å°‘é‡å¤å†…å®¹çš„ç”Ÿæˆã€‚",
                    key=f"ollama_repeat_penalty_{session_id}"
                )
            
            # Advanced Ollama Settings
            with st.expander("é«˜çº§è®¾ç½®", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    num_ctx = st.number_input(
                        "ä¸Šä¸‹æ–‡çª—å£å¤§å°",
                        min_value=512,
                        max_value=8192,
                        value=st.session_state.get(f'ollama_num_ctx_{session_id}', 4096),
                        step=512,
                        help="æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚",
                        key=f"ollama_num_ctx_{session_id}"
                    )
                
                with col2:
                    num_thread = st.number_input(
                        "çº¿ç¨‹æ•°",
                        min_value=1,
                        max_value=16,
                        value=st.session_state.get(f'ollama_num_thread_{session_id}', 4),
                        step=1,
                        help="ç”¨äºæ¨ç†çš„CPUçº¿ç¨‹æ•°ã€‚",
                        key=f"ollama_num_thread_{session_id}"
                    )
        
        elif selected_backend == "openai":
            # OpenAI Model Selection
            available_models = get_openai_models()
            if available_models:
                current_model = st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model'])
                
                selected_model = st.selectbox(
                    "é€‰æ‹©OpenAIæ¨¡å‹",
                    available_models,
                    index=available_models.index(current_model) if current_model in available_models else 0,
                    key=f"openai_model_select_{session_id}"
                )
                
                # Update the model session state when selection changes
                if selected_model != current_model:
                    st.session_state[f'openai_model_{session_id}'] = selected_model
                    st.success(f"å·²åˆ‡æ¢åˆ°: {selected_model}")
            else:
                st.warning("æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨")
                st.session_state[f'openai_model_{session_id}'] = CONFIG['llm']['openai_model']
            
            # OpenAI Parameters
            st.subheader("OpenAIå‚æ•°è®¾ç½®")
            
            col1, col2 = st.columns(2)
            with col1:
                temperature = st.slider(
                    "Temperature (æ¸©åº¦)",
                    min_value=0.0,
                    max_value=2.0,
                    value=st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                    step=0.1,
                    help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚0è¡¨ç¤ºå®Œå…¨ç¡®å®šæ€§ï¼Œ2è¡¨ç¤ºæœ€å¤§éšæœºæ€§ã€‚",
                    key=f"openai_temperature_{session_id}"
                )
                
                top_p = st.slider(
                    "Top-p (æ ¸é‡‡æ ·)",
                    min_value=0.0,
                    max_value=1.0,
                    value=st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                    step=0.1,
                    help="æ§åˆ¶è¯æ±‡é€‰æ‹©çš„å¤šæ ·æ€§ã€‚",
                    key=f"openai_top_p_{session_id}"
                )
            
            with col2:
                max_tokens = st.number_input(
                    "æœ€å¤§è¾“å‡ºé•¿åº¦",
                    min_value=1,
                    max_value=4096,
                    value=st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
                    step=1,
                    help="ç”Ÿæˆå“åº”çš„æœ€å¤§tokenæ•°é‡ã€‚",
                    key=f"openai_max_tokens_{session_id}"
                )
                
                presence_penalty = st.slider(
                    "Presence Penalty (å­˜åœ¨æƒ©ç½š)",
                    min_value=-2.0,
                    max_value=2.0,
                    value=st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                    step=0.1,
                    help="å‡å°‘æ¨¡å‹é‡å¤ç›¸åŒä¸»é¢˜çš„å€¾å‘ã€‚",
                    key=f"openai_presence_penalty_{session_id}"
                )
            
            # Advanced OpenAI Settings
            with st.expander("é«˜çº§è®¾ç½®", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    frequency_penalty = st.slider(
                        "Frequency Penalty (é¢‘ç‡æƒ©ç½š)",
                        min_value=-2.0,
                        max_value=2.0,
                        value=st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0),
                        step=0.1,
                        help="å‡å°‘æ¨¡å‹é‡å¤ç›¸åŒè¯æ±‡çš„å€¾å‘ã€‚",
                        key=f"openai_frequency_penalty_{session_id}"
                    )
                
                with col2:
                    logit_bias = st.text_input(
                        "Logit Bias (è¯æ±‡åå¥½)",
                        value=st.session_state.get(f'openai_logit_bias_{session_id}', '{}'),
                        help="JSONæ ¼å¼çš„è¯æ±‡åå¥½è®¾ç½®ï¼Œä¾‹å¦‚: {\"word\": 0.5}",
                        key=f"openai_logit_bias_{session_id}"
                    )
        
        st.divider()
        
        # Current Configuration Overview
        st.header("ğŸ“‹ å½“å‰é…ç½®æ¦‚è§ˆ")
        
        if selected_backend == "ollama":
            # Display configuration in a more compact format
            st.write("**åç«¯:**", selected_display_name)
            st.write("**ä¸»æœº:**", CONFIG['llm']['ollama_host'])
            st.write("**å½“å‰æ¨¡å‹:**", selected_model)
            st.write("**Temperature:**", st.session_state.get(f'ollama_temperature_{session_id}', 0.7))
            st.write("**Top-p:**", st.session_state.get(f'ollama_top_p_{session_id}', 0.9))
            st.write("**Top-k:**", st.session_state.get(f'ollama_top_k_{session_id}', 40))
            
            with st.expander("å®Œæ•´é…ç½®JSON", expanded=False):
                config_data = {
                    "backend": "ollama",
                    "host": CONFIG['llm']['ollama_host'],
                    "model": selected_model,
                    "temperature": st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                    "top_p": st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                    "top_k": st.session_state.get(f'ollama_top_k_{session_id}', 40),
                    "repeat_penalty": st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                    "num_ctx": st.session_state.get(f'ollama_num_ctx_{session_id}', 4096),
                    "num_thread": st.session_state.get(f'ollama_num_thread_{session_id}', 4)
                }
                st.json(config_data)
        
        elif selected_backend == "openai":
            # Display configuration in a more compact format
            st.write("**åç«¯:**", selected_display_name)
            st.write("**APIåœ°å€:**", CONFIG['llm']['openai_base_url'])
            st.write("**å½“å‰æ¨¡å‹:**", st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model']))
            st.write("**Temperature:**", st.session_state.get(f'openai_temperature_{session_id}', 0.7))
            st.write("**Top-p:**", st.session_state.get(f'openai_top_p_{session_id}', 1.0))
            st.write("**Max Tokens:**", st.session_state.get(f'openai_max_tokens_{session_id}', 2048))
            
            with st.expander("å®Œæ•´é…ç½®JSON", expanded=False):
                config_data = {
                    "backend": "openai",
                    "base_url": CONFIG['llm']['openai_base_url'],
                    "model": st.session_state.get(f'openai_model_{session_id}', CONFIG['llm']['openai_model']),
                    "temperature": st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                    "top_p": st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                    "max_tokens": st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
                    "presence_penalty": st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                    "frequency_penalty": st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0),
                    "logit_bias": st.session_state.get(f'openai_logit_bias_{session_id}', '{}')
                }
                st.json(config_data)
        
        st.divider()
        
        # User Account Management
        st.header("ğŸ‘¤ ç”¨æˆ·è´¦æˆ·")
        
        # Get current username from session state
        current_username = st.session_state.get('username', 'Unknown')
        
        st.write(f"**å½“å‰ç”¨æˆ·:** {current_username}")
        st.write("**ä¼šè¯ID:**", session_id)
        
        # Logout button
        if st.button("ğŸšª é€€å‡ºç™»å½•", key=f"logout_button_{session_id}", type="secondary"):
            st.session_state['logged_in'] = False
            st.session_state['username'] = None
            st.session_state['user_session_id'] = None
            st.success("âœ… å·²é€€å‡ºç™»å½•ï¼Œæ­£åœ¨è¿”å›ç™»å½•é¡µé¢...")
            st.rerun()  # Necessary to return to login screen
        
        st.divider()
        
        # Action Buttons
        st.header("âš¡ å¿«é€Ÿæ“ä½œ")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("é‡ç½®ä¸ºé»˜è®¤è®¾ç½®", key=f"reset_settings_{session_id}"):
                # Reset Ollama settings
                st.session_state[f'ollama_model_{session_id}'] = CONFIG['llm']['ollama_model']
                st.session_state[f'ollama_temperature_{session_id}'] = 0.7
                st.session_state[f'ollama_top_p_{session_id}'] = 0.9
                st.session_state[f'ollama_top_k_{session_id}'] = 40
                st.session_state[f'ollama_repeat_penalty_{session_id}'] = 1.1
                st.session_state[f'ollama_num_ctx_{session_id}'] = 4096
                st.session_state[f'ollama_num_thread_{session_id}'] = 4
                
                # Reset OpenAI settings
                st.session_state[f'openai_model_{session_id}'] = CONFIG['llm']['openai_model']
                st.session_state[f'openai_temperature_{session_id}'] = 0.7
                st.session_state[f'openai_top_p_{session_id}'] = 1.0
                st.session_state[f'openai_max_tokens_{session_id}'] = 2048
                st.session_state[f'openai_presence_penalty_{session_id}'] = 0.0
                st.session_state[f'openai_frequency_penalty_{session_id}'] = 0.0
                st.session_state[f'openai_logit_bias_{session_id}'] = '{}'
                
                st.success("å·²é‡ç½®ä¸ºé»˜è®¤è®¾ç½®")
                # No st.rerun() needed - Streamlit will update automatically
        
        with col2:
            if st.button("åˆ·æ–°è¿æ¥çŠ¶æ€", key=f"refresh_connection_{session_id}"):
                # Clear the cache to force fresh API calls
                get_ollama_models.clear()
                get_ollama_model_info.clear()
                test_ollama_connection.clear()
                test_openai_connection.clear()
                get_openai_models.clear()
                
                st.success("ç¼“å­˜å·²æ¸…é™¤ï¼Œæ­£åœ¨åˆ·æ–°è¿æ¥çŠ¶æ€...")
                # No st.rerun() needed - Streamlit will update automatically
        
        st.divider()
        
        # Documentation Links
        st.header("ğŸ“š ç›¸å…³æ–‡æ¡£")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Ollamaæ–‡æ¡£")
            st.link_button("GitHub Repository", "https://github.com/ollama/ollama")
            st.link_button("APIæ–‡æ¡£", "https://github.com/ollama/ollama/blob/main/docs/api.md")
            st.link_button("æ¨¡å‹å‚æ•°", "https://github.com/ollama/ollama/blob/main/docs/modelfile.md")
        
        with col2:
            st.subheader("OpenAIæ–‡æ¡£")
            st.link_button("APIæ–‡æ¡£", "https://platform.openai.com/docs/api-reference")
            st.link_button("æ¨¡å‹å‚æ•°", "https://platform.openai.com/docs/api-reference/chat/create")
            st.link_button("UIUIApi", "https://sg.uiuiapi.com/") 