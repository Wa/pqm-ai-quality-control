import streamlit as st
import os
import json
import time
from datetime import datetime
from pathlib import Path
from util import get_user_session, reset_user_session, start_analysis, complete_analysis, get_user_session_id, PromptGenerator, ensure_session_dirs, handle_file_upload
from config import CONFIG
from backend_client import get_backend_client, is_backend_available
from ollama import Client as OllamaClient
import openai
import re

def render_file_upload_section(session_dirs, session_id):
    """Render the file upload section with proper keys to prevent duplication."""
    col_cp, col_target, col_graph = st.columns([1, 1, 1])

    with col_cp:
        cp_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ æ§åˆ¶è®¡åˆ’æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"cp_uploader_{session_id}")
        if cp_files:
            handle_file_upload(cp_files, session_dirs["cp"])

    with col_target:
        target_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"target_uploader_{session_id}")
        if target_files:
            handle_file_upload(target_files, session_dirs["target"])

    with col_graph:
        graph_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ å›¾çº¸æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"graph_uploader_{session_id}")
        if graph_files:
            handle_file_upload(graph_files, session_dirs["graph"])

def parse_prompts(output_file):
    """Parse prompt_output.txt into individual prompts."""
    if not os.path.exists(output_file):
        return []
    with open(output_file, 'r', encoding='utf-8') as f:
        content = f.read()
    prompts = [p.strip() for p in content.split('='*80) if p.strip()]
    return prompts

def remove_think_blocks(text):
    """Remove all <think>...</think> blocks, including the tags."""
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)

def run_analysis_workflow(session_id, session_dirs, prompt_generator):
    """Run the complete analysis workflow."""
    session = get_user_session(session_id)
    cp_session_dir = session_dirs["cp"]
    target_session_dir = session_dirs["target"]
    generated_session_dir = session_dirs["generated"]
    st.info("ğŸ” å¼€å§‹ç‰¹æ®Šç‰¹æ€§ç¬¦å·æ£€æŸ¥åˆ†æï¼Œè¯·ç¨å€™...")
    
    # Get target files
    target_files_list = [f for f in os.listdir(target_session_dir) if os.path.isfile(os.path.join(target_session_dir, f))]
    if not target_files_list:
        st.warning("è¯·å…ˆä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶")
        return
    
    target_file_path = os.path.join(target_session_dir, target_files_list[0])
    output_file = os.path.join(generated_session_dir, "prompt_output.txt")
    
    # Check if prompt_output.txt already exists (from demo or previous run)
    if not os.path.exists(output_file):
        # Generate prompt only if it doesn't exist
        prompt_generator.generate_prompt(cp_session_dir, target_file_path, output_file)
    
    prompts = parse_prompts(output_file)
    result_file = os.path.join(generated_session_dir, "2_symbol_check_result.txt")
    
    # Get LLM backend from session state (default to ollama)
    llm_backend = st.session_state.get(f'llm_backend_{session_id}', 'ollama')
    
    # Initialize LLM clients based on selected backend
    if llm_backend == "ollama":
        ollama_client = OllamaClient(host=CONFIG["llm"]["ollama_host"])
        
        # Streaming generator for Ollama
        def llm_stream_chat(prompt):
            session['ollama_history'].append({"role": "user", "content": prompt})
            response_text = ""
            
            # Get Ollama parameters from session state
            model = st.session_state.get(f'ollama_model_{session_id}', CONFIG["llm"]["ollama_model"])
            temperature = st.session_state.get(f'ollama_temperature_{session_id}', 0.7)
            top_p = st.session_state.get(f'ollama_top_p_{session_id}', 0.9)
            top_k = st.session_state.get(f'ollama_top_k_{session_id}', 40)
            repeat_penalty = st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1)
            num_ctx = st.session_state.get(f'ollama_num_ctx_{session_id}', 4096)
            num_thread = st.session_state.get(f'ollama_num_thread_{session_id}', 4)
            
            for chunk in ollama_client.chat(
                model=model,
                messages=session['ollama_history'],
                stream=True,
                options={
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repeat_penalty": repeat_penalty,
                    "num_ctx": num_ctx,
                    "num_thread": num_thread
                }
            ):
                new_text = chunk['message']['content']
                response_text += new_text
                yield new_text
            session['ollama_history'].append({"role": "assistant", "content": response_text})

    elif llm_backend == "openai":
        openai.base_url = CONFIG["llm"]["openai_base_url"]
        openai.api_key = CONFIG["llm"]["openai_api_key"]
        
        # Streaming generator for OpenAI
        def llm_stream_chat(prompt):
            session['openai_history'].append({"role": "user", "content": prompt})
            response_text = ""
            
            # Get OpenAI parameters from session state
            model = st.session_state.get(f'openai_model_{session_id}', CONFIG["llm"]["openai_model"])
            temperature = st.session_state.get(f'openai_temperature_{session_id}', 0.7)
            top_p = st.session_state.get(f'openai_top_p_{session_id}', 1.0)
            max_tokens = st.session_state.get(f'openai_max_tokens_{session_id}', 2048)
            presence_penalty = st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0)
            frequency_penalty = st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0)
            logit_bias_str = st.session_state.get(f'openai_logit_bias_{session_id}', '{}')
            
            # Parse logit_bias if provided
            logit_bias = {}
            try:
                if logit_bias_str and logit_bias_str != '{}':
                    logit_bias = json.loads(logit_bias_str)
            except json.JSONDecodeError:
                pass
            
            stream = openai.chat.completions.create(
                model=model,
                messages=session['openai_history'],
                stream=True,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                logit_bias=logit_bias if logit_bias else None
            )
            for chunk in stream:
                delta = chunk.choices[0].delta.content or ""
                response_text += delta
                yield delta
            session['openai_history'].append({"role": "assistant", "content": response_text})

    # Add timestamp to make keys even more unique
    import time
    timestamp = int(time.time())
    
    # Display analysis results
    for prompt_idx, prompt in enumerate(prompts, 1):
        col_prompt, col_response = st.columns([1, 1])
        with col_prompt:
            # Create bounded container for auto-scrolling prompt
            prompt_container = st.container(height=400)
            
            with prompt_container:
                # Use chat message structure for prompt display
                with st.chat_message("user"):
                    prompt_placeholder = st.empty()
                    
                    # Simulated streaming for prompt (10 words at a time)
                    words = prompt.split()
                    streamed_prompt = ""
                    for chunk_idx in range(0, len(words), 10):
                        chunk_words = words[chunk_idx:chunk_idx + 10]
                        streamed_prompt += " ".join(chunk_words) + " "
                        prompt_placeholder.text(streamed_prompt.strip())
                
                # Add disabled chat input for auto-scroll with unique key including timestamp
                st.chat_input(placeholder="", disabled=True, key=f"workflow_prompt_{timestamp}_{prompt_idx}_{session_id}")
        
        with col_response:
            # Create bounded container for auto-scrolling response
            response_container = st.container(height=400)
            
            with response_container:
                # Use chat message structure for response display
                with st.chat_message("assistant"):
                    response_placeholder = st.empty()
                    
                    # Stream the response using selected LLM
                    response_text = ""
                    for chunk in llm_stream_chat(prompt):
                        response_text += chunk
                        response_placeholder.write(response_text)
                    
                    # Write response_text to file (overwrite for first, append for others), filtering <think>...</think>
                    cleaned_response = remove_think_blocks(response_text)
                    if prompt_idx == 1:
                        with open(result_file, "w", encoding="utf-8") as f:
                            f.write('æå–ä»¥ä¸‹æ–‡å­—ä¸­æœ‰å…³"ç‰¹æ®Šç‰¹æ€§ç¬¦å·ï¼ˆ/ã€ â˜…å’Œâ˜†ï¼‰ä¸ä¸€è‡´"çš„åœ°æ–¹ï¼Œå¦‚æœä¸‹æ–‡å­—æœªæåŠä»»ä½•"ç‰¹æ®Šç‰¹æ€§ç¬¦å·ï¼ˆ/ã€ â˜…å’Œâ˜†ï¼‰ä¸ä¸€è‡´"çš„åœ°æ–¹ï¼Œåˆ™è¾“å‡º"æ¯”å¯¹ç»“æœä¸ºå…¨éƒ¨ä¸€è‡´ï¼Œä½†æ˜¯æœ‰äº›å‚æ•°çš„ç‰¹æ®Šæ€§ä½œç¬¦å·åœ¨æ§åˆ¶è®¡åˆ’ä¸­æ²¡æœ‰å®šä¹‰"\n')
                            f.write(cleaned_response)
                    else:
                        with open(result_file, "a", encoding="utf-8") as f:
                            f.write(cleaned_response)
                
                # Add disabled chat input for auto-scroll with unique key including timestamp
                st.chat_input(placeholder="", disabled=True, key=f"workflow_response_{timestamp}_{prompt_idx}_{session_id}")
    
    # --- ç‰¹æ®Šç‰¹æ€§ç¬¦å·ä¸ä¸€è‡´ç»“è®º (symbol_check_final) ---
    # Read the content of 2_symbol_check_result.txt as the new prompt
    symbol_check_final_file = os.path.join(generated_session_dir, "2_symbol_check_result.txt")
    if os.path.exists(symbol_check_final_file):
        with open(symbol_check_final_file, "r", encoding="utf-8") as f:
            symbol_check_final_prompt = f.read()
        
        # Display the prompt and response side by side
        col_final_prompt, col_final_response = st.columns([1, 1])
        with col_final_prompt:
            st.subheader("ç‰¹æ®Šç‰¹æ€§ç¬¦å·ä¸ä¸€è‡´ç»“è®º - æç¤ºè¯:")
            prompt_container = st.container(height=400)
            with prompt_container:
                with st.chat_message("user"):
                    prompt_placeholder = st.empty()
                    prompt_placeholder.text(symbol_check_final_prompt)
                
                st.chat_input(placeholder="", disabled=True, key=f"workflow_final_prompt_{timestamp}_{session_id}")
        
        with col_final_response:
            st.subheader("ç‰¹æ®Šç‰¹æ€§ç¬¦å·ä¸ä¸€è‡´ç»“è®º - AIå›å¤:")
            response_container = st.container(height=400)
            with response_container:
                with st.chat_message("assistant"):
                    response_placeholder = st.empty()
                    
                    # Stream the final response using selected LLM
                    symbol_check_final_response = ""
                    if llm_backend == "ollama":
                        for chunk in ollama_client.chat(
                            model=st.session_state.get(f'ollama_model_{session_id}', CONFIG["llm"]["ollama_model"]),
                            messages=[{"role": "user", "content": symbol_check_final_prompt}],
                            stream=True,
                            options={
                                "temperature": st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                                "top_p": st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                                "top_k": st.session_state.get(f'ollama_top_k_{session_id}', 40),
                                "repeat_penalty": st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                                "num_ctx": st.session_state.get(f'ollama_num_ctx_{session_id}', 4096),
                                "num_thread": st.session_state.get(f'ollama_num_thread_{session_id}', 4)
                            }
                        ):
                            new_text = chunk['message']['content']
                            symbol_check_final_response += new_text
                            response_placeholder.write(symbol_check_final_response)
                    elif llm_backend == "openai":
                        stream = openai.chat.completions.create(
                            model=st.session_state.get(f'openai_model_{session_id}', CONFIG["llm"]["openai_model"]),
                            messages=[{"role": "user", "content": symbol_check_final_prompt}],
                            stream=True,
                            temperature=st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                            top_p=st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                            max_tokens=st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
                            presence_penalty=st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                            frequency_penalty=st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0)
                        )
                        for chunk in stream:
                            delta = chunk.choices[0].delta.content or ""
                            symbol_check_final_response += delta
                            response_placeholder.write(symbol_check_final_response)
                
                st.chat_input(placeholder="", disabled=True, key=f"workflow_final_response_{timestamp}_{session_id}")
    st.info("âœ… åˆ†æå®Œæˆ")

def render_special_symbols_check_tab(session_id):
    # Handle None session_id (user not logged in)
    if session_id is None:
        st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        return
    
    # Add CSS to hide chat input (required for auto-scroll to work)
    st.markdown("""
    <style>
    [data-testid="stChatInput"] { display: none; }
    </style>
    """, unsafe_allow_html=True)
    
    # Base directories for each upload box - using centralized config
    BASE_DIRS = {
        "cp": str(CONFIG["directories"]["cp_files"]),
        "target": str(CONFIG["directories"]["target_files"]),
        "graph": str(CONFIG["directories"]["graph_files"]),
        "generated": str(CONFIG["directories"]["generated_files"])
    }
    session_dirs = ensure_session_dirs(BASE_DIRS, session_id)
    cp_session_dir = session_dirs["cp"]
    target_session_dir = session_dirs["target"]
    graph_session_dir = session_dirs["graph"]
    generated_session_dir = session_dirs["generated"]

    # Initialize PromptGenerator
    prompt_generator = PromptGenerator()

    # Layout: right column for info, left for main content
    col_main, col_info = st.columns([2, 1])
    
    with col_main:
        # Get structured user session
        session = get_user_session(session_id)

        # Always show file upload section
        render_file_upload_section(session_dirs, session_id)
        
        # Start button - only show if process hasn't started
        if not session['process_started']:
            col_buttons = st.columns([1, 1])
            with col_buttons[0]:
                if st.button("å¼€å§‹", key=f"start_button_{session_id}"):
                    # Clear any existing generated files to ensure fresh generation
                    output_file = os.path.join(generated_session_dir, "prompt_output.txt")
                    result_file = os.path.join(generated_session_dir, "2_symbol_check_result.txt")
                    
                    if os.path.exists(output_file):
                        os.remove(output_file)
                    if os.path.exists(result_file):
                        os.remove(result_file)
                    
                    # Clear chat history for fresh analysis
                    session['ollama_history'] = []
                    session['openai_history'] = []
                    session['analysis_completed'] = False
                    
                    # Start the analysis process
                    start_analysis(session_id)
                    st.rerun()
            with col_buttons[1]:
                if st.button("æ¼”ç¤º", key=f"demo_button_{session_id}"):
                    # Workflow-based approach: Start â†’ Run â†’ Finish
                    
                    # 2. Run demo workflow
                    demo_base_dir = CONFIG["directories"]["cp_files"].parent / "demonstration"
                    
                    # Copy files from demonstration folders to session folders
                    demo_folder_mapping = {
                        "CP_files": "cp",
                        "graph_files": "graph", 
                        "target_files": "target"
                    }
                    
                    files_copied = False
                    for demo_folder, session_key in demo_folder_mapping.items():
                        demo_folder_path = os.path.join(demo_base_dir, demo_folder)
                        session_folder_path = session_dirs[session_key]
                        
                        if os.path.exists(demo_folder_path):
                            # Copy all files from demo folder to session folder
                            for file_name in os.listdir(demo_folder_path):
                                demo_file_path = os.path.join(demo_folder_path, file_name)
                                session_file_path = os.path.join(session_folder_path, file_name)
                                
                                if os.path.isfile(demo_file_path):
                                    import shutil
                                    shutil.copy2(demo_file_path, session_file_path)
                                    files_copied = True
                    
                    # Copy pre-generated prompt file (but not result file)
                    demo_prompt_file = os.path.join(demo_base_dir, "generated_files", "prompt_output.txt")
                    session_prompt_file = os.path.join(generated_session_dir, "prompt_output.txt")
                    
                    if os.path.exists(demo_prompt_file):
                        import shutil
                        shutil.copy2(demo_prompt_file, session_prompt_file)
                    
                    if files_copied:
                        # Set up session for analysis
                        session['analysis_completed'] = False
                        session['process_started'] = True
                        session['ollama_history'] = []
                        session['openai_history'] = []
                        
                        # 3. Run the analysis workflow - REMOVED: This was causing duplicate execution
                        # run_analysis_workflow(session_id, session_dirs, prompt_generator)
                        
                        # 4. Display finish message - REMOVED: This was premature
                        # st.success("âœ… åˆ†æå®Œæˆ")
                        
                        # Force page refresh to hide buttons and show analysis
                        st.rerun()
                    else:
                        st.error("æ¼”ç¤ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¼”ç¤ºæ–‡ä»¶å¤¹")
        
        # Show status and reset button if process has started
        if session['process_started']:
            # Add a button to reset and clear history
            if st.button("é‡æ–°å¼€å§‹", key=f"reset_button_{session_id}"):
                reset_user_session(session_id)
                st.rerun()
            
            # Check if we need to run analysis
            target_files_list = [f for f in os.listdir(target_session_dir) if os.path.isfile(os.path.join(target_session_dir, f))]
            if target_files_list:
                if session['process_started'] and not session['analysis_completed']:
                    # Run the analysis workflow in full width within the main column
                    run_analysis_workflow(session_id, session_dirs, prompt_generator)
                    
                    # Mark as completed
                    session['analysis_completed'] = True
                else:
                    # Files exist but process wasn't explicitly started
                    st.info("æ£€æµ‹åˆ°å¾…æ£€æŸ¥æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»\"å¼€å§‹\"æŒ‰é’®å¼€å§‹åˆ†æï¼Œæˆ–ç‚¹å‡»\"æ¼”ç¤º\"æŒ‰é’®ä½¿ç”¨æ¼”ç¤ºæ–‡ä»¶ã€‚")
            else:
                st.warning("è¯·å…ˆä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶")

    with col_info:
        # --- File Manager Module ---
        def get_file_list(folder):
            # Always use FastAPI backend
            try:
                client = get_backend_client()
                # Determine file type from folder path
                if "CP_files" in folder:
                    file_type = "cp"
                elif "target_files" in folder:
                    file_type = "target"
                elif "graph_files" in folder:
                    file_type = "graph"
                else:
                    # For other directories, return empty list
                    return []
                
                # Get files from backend
                result = client.list_files(session_id, file_type)
                if file_type in result:
                    files = []
                    for file_info in result[file_type]:
                        files.append({
                            'name': file_info['name'],
                            'size': file_info['size'],
                            'modified': float(file_info['modified_time']),
                            'path': os.path.join(folder, file_info['name'])
                        })
                    # Use stable sorting by name first, then by modification time
                    return sorted(files, key=lambda x: (x['name'].lower(), x['modified']), reverse=False)
                else:
                    return []
            except Exception as e:
                st.error(f"åç«¯è¿æ¥å¤±è´¥: {e}")
                return []
        
        def get_file_list_direct(folder):
            """Direct file system access as fallback"""
            if not os.path.exists(folder):
                return []
            files = []
            for f in os.listdir(folder):
                file_path = os.path.join(folder, f)
                if os.path.isfile(file_path):
                    stat = os.stat(file_path)
                    files.append({
                        'name': f,
                        'size': stat.st_size,
                        'modified': stat.st_mtime,
                        'path': file_path
                    })
            # Use stable sorting by name first, then by modification time
            return sorted(files, key=lambda x: (x['name'].lower(), x['modified']), reverse=False)

        def format_file_size(size_bytes):
            """Convert bytes to human readable format."""
            if size_bytes == 0:
                return "0 B"
            size_names = ["B", "KB", "MB", "GB"]
            i = 0
            while size_bytes >= 1024 and i < len(size_names) - 1:
                size_bytes /= 1024.0
                i += 1
            return f"{size_bytes:.1f} {size_names[i]}"

        def format_timestamp(timestamp):
            """Convert timestamp to readable date."""
            from datetime import datetime
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M')

        def truncate_filename(filename, max_length=40):
            """Truncate filename if too long, preserving extension."""
            if len(filename) <= max_length:
                return filename
            
            # Split filename and extension
            name, ext = os.path.splitext(filename)
            # Calculate how much space we have for the name part
            available_length = max_length - len(ext) - 3  # 3 for "..."
            
            if available_length <= 0:
                # If extension is too long, just truncate the whole thing
                return filename[:max_length-3] + "..."
            
            # Truncate name part and add ellipsis
            truncated_name = name[:available_length] + "..."
            return truncated_name + ext

        # File Manager Tabs
        tab_cp, tab_target, tab_graph = st.tabs(["æ§åˆ¶è®¡åˆ’æ–‡ä»¶", "å¾…æ£€æŸ¥æ–‡ä»¶", "å›¾çº¸æ–‡ä»¶"])
        
        with tab_cp:
            cp_files_list = get_file_list(cp_session_dir)
            
            if cp_files_list:
                for i, file_info in enumerate(cp_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")  # Show full name inside
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            # Use a more stable key for delete button
                            delete_key = f"delete_cp_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    if is_backend_available():
                                        # Use FastAPI backend
                                        client = get_backend_client()
                                        result = client.delete_file(session_id, file_info['path'])
                                        if result.get("status") == "success":
                                            st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                        else:
                                            st.error(f"åˆ é™¤å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                                    else:
                                        # Fallback to direct file system access
                                        os.remove(file_info['path'])
                                        st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
                
            # Upload new files directly in this tab
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_cp_files = st.file_uploader("é€‰æ‹©æ§åˆ¶è®¡åˆ’æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"cp_uploader_tab_{session_id}")
            if new_cp_files:
                handle_file_upload(new_cp_files, cp_session_dir)

        with tab_target:
            target_files_list = get_file_list(target_session_dir)
            
            if target_files_list:
                for i, file_info in enumerate(target_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")  # Show full name inside
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            # Use a more stable key for delete button
                            delete_key = f"delete_target_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    if is_backend_available():
                                        # Use FastAPI backend
                                        client = get_backend_client()
                                        result = client.delete_file(session_id, file_info['path'])
                                        if result.get("status") == "success":
                                            st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                        else:
                                            st.error(f"åˆ é™¤å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                                    else:
                                        # Fallback to direct file system access
                                        os.remove(file_info['path'])
                                        st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
                
            # Upload new files directly in this tab
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_target_files = st.file_uploader("é€‰æ‹©å¾…æ£€æŸ¥æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"target_uploader_tab_{session_id}")
            if new_target_files:
                handle_file_upload(new_target_files, target_session_dir)

        with tab_graph:
            graph_files_list = get_file_list(graph_session_dir)
            
            if graph_files_list:
                for i, file_info in enumerate(graph_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")  # Show full name inside
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            # Use a more stable key for delete button
                            delete_key = f"delete_graph_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    if is_backend_available():
                                        # Use FastAPI backend
                                        client = get_backend_client()
                                        result = client.delete_file(session_id, file_info['path'])
                                        if result.get("status") == "success":
                                            st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                        else:
                                            st.error(f"åˆ é™¤å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                                    else:
                                        # Fallback to direct file system access
                                        os.remove(file_info['path'])
                                        st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
                
            # Upload new files directly in this tab
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_graph_files = st.file_uploader("é€‰æ‹©å›¾çº¸æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"graph_uploader_tab_{session_id}")
            if new_graph_files:
                handle_file_upload(new_graph_files, graph_session_dir)

        # Bulk operations
        st.markdown("---")
        st.markdown("### æ‰¹é‡æ“ä½œ")
        
        # Check if backend is available
        backend_available = is_backend_available()
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶", key=f"clear_all_files_{session_id}"):
            if backend_available:
                # Use FastAPI backend
                client = get_backend_client()
                result = client.clear_files(session_id)
                
                if result.get("status") == "success":
                    st.success(f"âœ… {result.get('message', 'å·²æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶')}")
                else:
                    st.error(f"âŒ æ¸…ç©ºå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            else:
                # Fallback to direct file operations
                try:
                    # Clear all session directories
                    for dir_path in [cp_session_dir, target_session_dir, graph_session_dir]:
                        for file in os.listdir(dir_path):
                            file_path = os.path.join(dir_path, file)
                            if os.path.isfile(file_path):
                                os.remove(file_path)
                    st.success("å·²æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶")
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
        
        # Show backend status
        if backend_available:
            st.info("ğŸ”§ ä½¿ç”¨ FastAPI åç«¯å¤„ç†æ–‡ä»¶æ“ä½œ")
        else:
            st.warning("âš ï¸ FastAPI åç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ“ä½œ") 