import streamlit as st
import os
import json
import pandas as pd
from util import ensure_session_dirs, handle_file_upload, get_user_session, start_analysis, reset_user_session, resolve_ollama_host
from config import CONFIG
from util import extract_parameters_to_json
from ollama import Client as OllamaClient
import openai

def render_parameters_file_upload_section(session_dirs, session_id):
    """Render the file upload section for parameters check with unique keys."""
    col_cp, col_target, col_graph = st.columns([1, 1, 1])

    with col_cp:
        cp_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ æ§åˆ¶è®¡åˆ’æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_cp_uploader_{session_id}")
        if cp_files:
            handle_file_upload(cp_files, session_dirs["cp"])

    with col_target:
        target_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_target_uploader_{session_id}")
        if target_files:
            handle_file_upload(target_files, session_dirs["target"])

    with col_graph:
        graph_files = st.file_uploader("ç‚¹å‡»ä¸Šä¼ å›¾çº¸æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_graph_uploader_{session_id}")
        if graph_files:
            handle_file_upload(graph_files, session_dirs["graph"])

def run_parameters_analysis_workflow(session_id, session_dirs):
    """Run the complete parameters analysis workflow."""
    # Get tab-specific session state
    session = get_user_session(session_id, 'parameters')
    cp_session_dir = session_dirs["cp"]
    target_session_dir = session_dirs["target"]
    generated_session_dir = session_dirs["generated"]
    parameters_dir = session_dirs.get("generated_parameters_check", os.path.join(generated_session_dir, "parameters_check"))
    
    st.info("ğŸ” æ­£åœ¨è¿›è¡Œè®¾è®¡åˆ¶ç¨‹å‚æ•°ä¸€è‡´æ€§åˆ†æâ€¦")
    
    # Get target files
    target_files_list = [f for f in os.listdir(target_session_dir) if os.path.isfile(os.path.join(target_session_dir, f))]
    if not target_files_list:
        st.warning("è¯·å…ˆä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶")
        return
    
    # Initialize LLM backend (default to ollama)
    llm_backend = st.session_state.get(f'llm_backend_{session_id}', 'ollama_127')
    if llm_backend in ("ollama_127", "ollama_9"):
        host = resolve_ollama_host(llm_backend)
        ollama_client = OllamaClient(host=host)
    elif llm_backend == "openai":
        openai.base_url = CONFIG["llm"]["openai_base_url"]
        openai.api_key = CONFIG["llm"]["openai_api_key"]

    # Load prompt from parameters_llm_prompt.txt
    prompt_path = os.path.join(parameters_dir, "parameters_llm_prompt.txt")
    if not os.path.exists(prompt_path):
        st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ–‡ä»¶ï¼Œè¯·å…ˆç‚¹å‡»â€œå¼€å§‹â€ç”Ÿæˆ JSON ä¸æç¤ºè¯åå†è¯•ã€‚")
        return
    with open(prompt_path, 'r', encoding='utf-8') as f:
        full_prompt_text = f.read()

    # Display prompt and stream LLM response side by side
    col_prompt, col_response = st.columns([1, 1])
    with col_prompt:
        st.subheader("ç›®æ ‡å‚æ•°è¯„å®¡ - æç¤ºè¯")
        prompt_container = st.container(height=400)
        with prompt_container:
            with st.chat_message("user"):
                prompt_placeholder = st.empty()
                streamed = ""
                for line in full_prompt_text.splitlines(True):
                    streamed += line
                    prompt_placeholder.text(streamed)
        st.chat_input(placeholder="", disabled=True, key=f"parameters_prompt_input_{session_id}")

    with col_response:
        st.subheader("ç›®æ ‡å‚æ•°è¯„å®¡ - AIå›å¤")
        response_container = st.container(height=400)
        with response_container:
            with st.chat_message("assistant"):
                response_placeholder = st.empty()
                response_text = ""

                if llm_backend in ("ollama_127", "ollama_9"):
                    for chunk in ollama_client.chat(
                        model=st.session_state.get(f'ollama_model_{session_id}', CONFIG["llm"]["ollama_model"]),
                        messages=[{"role": "user", "content": full_prompt_text}],
                        stream=True,
                        options={
                            "temperature": st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                            "top_p": st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                            "top_k": st.session_state.get(f'ollama_top_k_{session_id}', 40),
                            "repeat_penalty": st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                            "num_ctx": st.session_state.get(f'ollama_num_ctx_{session_id}', 40001),
                            "num_thread": st.session_state.get(f'ollama_num_thread_{session_id}', 4),
                        }
                    ):
                        new_text = chunk['message']['content']
                        response_text += new_text
                        response_placeholder.write(response_text)
                elif llm_backend == "openai":
                    stream = openai.chat.completions.create(
                        model=st.session_state.get(f'openai_model_{session_id}', CONFIG["llm"]["openai_model"]),
                        messages=[{"role": "user", "content": full_prompt_text}],
                        stream=True,
                        temperature=st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                        top_p=st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                        max_tokens=st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
                        presence_penalty=st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                        frequency_penalty=st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0),
                    )
                    for chunk in stream:
                        delta = chunk.choices[0].delta.content or ""
                        response_text += delta
                        response_placeholder.write(response_text)

                # Save LLM response to file in parameters_check subfolder
                try:
                    result_path = os.path.join(parameters_dir, "parameters_check_result.txt")
                    with open(result_path, 'w', encoding='utf-8') as f:
                        f.write(response_text)
                    st.success(f"å·²ä¿å­˜è¯„å®¡ç»“æœ: {result_path}")
                except Exception as e:
                    st.warning(f"è¯„å®¡ç»“æœä¿å­˜å¤±è´¥: {e}")

        st.chat_input(placeholder="", disabled=True, key=f"parameters_response_input_{session_id}")

    # --- ç¬¬äºŒé˜¶æ®µï¼šä»…æå–â€œä¸ä¸€è‡´é¡¹â€å¹¶è¾“å‡ºä¸ºJSON ---
    st.divider()
    st.subheader("ä¸ä¸€è‡´é¡¹æå–ï¼ˆJSONï¼‰")

    # Load prior free-form results (may exist from one or multiple runs)
    result_paths = [
        os.path.join(parameters_dir, "parameters_check_result.txt"),
        os.path.join(parameters_dir, "parameters_check_result2.txt"),
    ]
    prior_texts = []
    for p in result_paths:
        if os.path.exists(p):
            try:
                prior_texts.append(open(p, 'r', encoding='utf-8').read())
            except Exception:
                pass
    prior_merged = "\n\n---\n\n".join(prior_texts) if prior_texts else ""

    if not prior_merged:
        st.info("æœªæ‰¾åˆ°å…ˆå‰çš„è¯„å®¡ç»“æœæ–‡æœ¬ï¼ˆparameters_check_result*.txtï¼‰ï¼Œè¯·å…ˆå®Œæˆä¸Šä¸€é˜¶æ®µè¯„å®¡ã€‚")
        return

    # Build JSON-only extraction prompt
    extraction_prompt = (
        "ä½ æ˜¯ä¸€å APQP ä¸“å®¶ã€‚ç°åœ¨è¯·ä»ä»¥ä¸‹è¯„å®¡æ–‡æœ¬ä¸­â€œåªæå–ä¸ä¸€è‡´é¡¹â€ï¼Œå¿½ç•¥æ‰€æœ‰â€œä¸€è‡´é¡¹â€å’Œâ€œæ§åˆ¶è®¡åˆ’ä¸­æ— å¯¹åº”é¡¹/ç¼ºå¤±é¡¹â€çš„æè¿°ã€‚\n"
        "è¯·å°†ä¸ä¸€è‡´é¡¹æ•´ç†ä¸ºç»Ÿä¸€çš„ JSON å¯¹è±¡å¹¶ä¸¥æ ¼åªè¾“å‡º JSONï¼ˆä¸è¦è¾“å‡ºè§£é‡Š/Markdownï¼‰ã€‚\n"
        "åœ¨æ¯æ¡ä¸ä¸€è‡´é¡¹ä¸­ï¼Œlocation å­—æ®µå¿…é¡»æä¾›æ˜ç¡®å®šä½ï¼šâ€˜ç›®æ ‡æ–‡ä»¶æ–‡ä»¶å + ç›®æ ‡æ–‡ä»¶Sheetåç§°ï¼›æ§åˆ¶è®¡åˆ’æ–‡ä»¶æ–‡ä»¶å + æ§åˆ¶è®¡åˆ’Sheetåç§°â€™ï¼Œ"
        "è‹¥æ— æ³•ç¡®å®šå…¶ä¸­ä»»ä¸€é¡¹ï¼Œè¯·ä»¥ç©ºå­—ç¬¦ä¸²ä»£æ›¿ã€‚ä¸è¦æä¾›è¡Œå·ã€‚\n\n"
        "JSON ç»“æ„è¦æ±‚å¦‚ä¸‹ï¼š\n"
        "{\n"
        "  \"items\": [\n"
        "    {\n"
        "      \"parameter\": \"å‚æ•°åç§°\",\n"
        "      \"target_value\": \"ç›®æ ‡æ–‡ä»¶ä¸­çš„å–å€¼/èŒƒå›´ï¼ˆè‹¥æ— æ³•ç¡®å®šåˆ™ç©ºå­—ç¬¦ä¸²ï¼‰\",\n"
        "      \"cp_value\": \"æ§åˆ¶è®¡åˆ’ä¸­çš„å–å€¼/èŒƒå›´ï¼ˆè‹¥æ— æ³•ç¡®å®šåˆ™ç©ºå­—ç¬¦ä¸²ï¼‰\",\n"
        "      \"location\": \"ç›®æ ‡æ–‡ä»¶ï¼š<æ–‡ä»¶å>/<Sheetå>ï¼›æ§åˆ¶è®¡åˆ’ï¼š<æ–‡ä»¶å>/<Sheetå>\",\n"
        "      \"issue\": \"ä¸€å¥è¯è¯´æ˜ä¸ä¸€è‡´ç‚¹\",\n"
        "      \"suggestion\": \"ç®€çŸ­çš„ä¿®è®¢å»ºè®®\"\n"
        "    }\n"
        "  ]\n"
        "}\n\n"
        "è¯„å®¡æ–‡æœ¬å¦‚ä¸‹ï¼š\n"
        f"{prior_merged}"
    )

    col_json_prompt, col_json_response = st.columns([1, 1])
    with col_json_prompt:
        st.markdown("**JSON æå– - æç¤ºè¯**")
        prompt_container2 = st.container(height=400)
        with prompt_container2:
            with st.chat_message("user"):
                ph = st.empty()
                streamed = ""
                for line in extraction_prompt.splitlines(True):
                    streamed += line
                    ph.text(streamed)
        st.chat_input(placeholder="", disabled=True, key=f"parameters_json_prompt_input_{session_id}")

    with col_json_response:
        st.markdown("**JSON æå– - AIå›å¤**")
        response_container2 = st.container(height=400)
        with response_container2:
            with st.chat_message("assistant"):
                ph2 = st.empty()
                json_response_text = ""

                if llm_backend in ("ollama_127", "ollama_9"):
                    for chunk in ollama_client.chat(
                        model=st.session_state.get(f'ollama_model_{session_id}', CONFIG["llm"]["ollama_model"]),
                        messages=[{"role": "user", "content": extraction_prompt}],
                        stream=True,
                        options={
                            "temperature": st.session_state.get(f'ollama_temperature_{session_id}', 0.7),
                            "top_p": st.session_state.get(f'ollama_top_p_{session_id}', 0.9),
                            "top_k": st.session_state.get(f'ollama_top_k_{session_id}', 40),
                            "repeat_penalty": st.session_state.get(f'ollama_repeat_penalty_{session_id}', 1.1),
                            "num_ctx": st.session_state.get(f'ollama_num_ctx_{session_id}', 40001),
                            "num_thread": st.session_state.get(f'ollama_num_thread_{session_id}', 4),
                            "format": "json",
                        }
                    ):
                        new_text = chunk['message']['content']
                        json_response_text += new_text
                        ph2.write(json_response_text)
                elif llm_backend == "openai":
                    stream = openai.chat.completions.create(
                        model=st.session_state.get(f'openai_model_{session_id}', CONFIG["llm"]["openai_model"]),
                        messages=[{"role": "user", "content": extraction_prompt}],
                        stream=True,
                        temperature=st.session_state.get(f'openai_temperature_{session_id}', 0.7),
                        top_p=st.session_state.get(f'openai_top_p_{session_id}', 1.0),
                        max_tokens=st.session_state.get(f'openai_max_tokens_{session_id}', 2048),
                        presence_penalty=st.session_state.get(f'openai_presence_penalty_{session_id}', 0.0),
                        frequency_penalty=st.session_state.get(f'openai_frequency_penalty_{session_id}', 0.0),
                        response_format={"type": "json_object"},
                    )
                    for chunk in stream:
                        delta = chunk.choices[0].delta.content or ""
                        json_response_text += delta
                        ph2.write(json_response_text)

        st.chat_input(placeholder="", disabled=True, key=f"parameters_json_response_input_{session_id}")

    # Parse and persist JSON
    parsed = None
    try:
        parsed = json.loads(json_response_text)
    except Exception:
        try:
            cleaned = json_response_text.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.strip('`')
                idx = cleaned.find("{")
                if idx >= 0:
                    cleaned = cleaned[idx:]
            start = cleaned.find('{')
            end = cleaned.rfind('}')
            if start >= 0 and end > start:
                cleaned = cleaned[start:end+1]
            parsed = json.loads(cleaned)
        except Exception:
            parsed = None

    if parsed and isinstance(parsed, dict):
        save_path = os.path.join(parameters_dir, "parameters_check_result.json")
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(parsed, f, ensure_ascii=False, indent=2)
            st.success(f"å·²ä¿å­˜ä¸ä¸€è‡´é¡¹JSON: {save_path}")
        except Exception as e:
            st.warning(f"ä¿å­˜JSONå¤±è´¥: {e}")

        # Show as table
        items = parsed.get('items') if isinstance(parsed.get('items'), list) else []
        # Ensure column order, fill missing keys
        norm_rows = []
        for it in items:
            norm_rows.append({
                'parameter': str(it.get('parameter', '')),
                'target_value': str(it.get('target_value', '')),
                'cp_value': str(it.get('cp_value', '')),
                'location': str(it.get('location', '')),
                'issue': str(it.get('issue', '')),
                'suggestion': str(it.get('suggestion', '')),
            })
        if norm_rows:
            df = pd.DataFrame(norm_rows, columns=['parameter', 'target_value', 'cp_value', 'location', 'issue', 'suggestion'])
            st.dataframe(df, use_container_width=True)
            # Save CSV to parameters_check folder
            try:
                csv_path = os.path.join(parameters_dir, 'parameters_check_result.csv')
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                st.success(f"å·²ä¿å­˜ä¸ä¸€è‡´é¡¹è¡¨æ ¼: {csv_path}")
            except Exception as e:
                st.warning(f"ä¿å­˜CSVå¤±è´¥: {e}")
        else:
            st.info("æœªè§£æåˆ°ä»»ä½•ä¸ä¸€è‡´é¡¹ã€‚")
    else:
        st.warning("æœªèƒ½è§£æä¸ºæœ‰æ•ˆçš„JSONï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹AIå›å¤å†…å®¹ã€‚")

def render_parameters_check_tab(session_id):
    """Render the design process parameters check tab."""
    # Handle None session_id (user not logged in)
    if session_id is None:
        st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        return
    
    # Add CSS to hide chat input (required for auto-scroll to work)
    st.markdown("""
    <style>
    [data-testid="stChatInput"] { display: none; }
    </style>
    """, unsafe_allow_html=True)
    # Page subheader
    st.subheader("ğŸ“Š è®¾è®¡åˆ¶ç¨‹æ£€æŸ¥")
    
    
    # Base directories for each upload box - using centralized config
    BASE_DIRS = {
        "cp": str(CONFIG["directories"]["cp_files"]),
        "target": str(CONFIG["directories"]["target_files"]),
        "graph": str(CONFIG["directories"]["graph_files"]),
        "generated": str(CONFIG["directories"]["generated_files"])
    }
    session_dirs = ensure_session_dirs(BASE_DIRS, session_id)
    cp_session_dir = session_dirs["cp"]
    target_session_dir = session_dirs["target"]
    graph_session_dir = session_dirs["graph"]
    generated_session_dir = session_dirs["generated"]
    parameters_dir = session_dirs.get("generated_parameters_check", os.path.join(generated_session_dir, "parameters_check"))
    os.makedirs(parameters_dir, exist_ok=True)

    # Layout: right column for info, left for main content
    col_main, col_info = st.columns([2, 1])

    # Render the info/file column FIRST so lists appear immediately when demo starts
    with col_info:
        # Early scoped clear operations: three per-bucket buttons
        col_clear_cp, col_clear_target, col_clear_graph = st.columns(3)
        with col_clear_cp:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ§åˆ¶è®¡åˆ’æ–‡ä»¶", key=f"parameters_clear_cp_files_{session_id}"):
                try:
                    for file in os.listdir(cp_session_dir):
                        file_path = os.path.join(cp_session_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    st.success("å·²æ¸…ç©ºæ§åˆ¶è®¡åˆ’æ–‡ä»¶")
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
        with col_clear_target:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¾…æ£€æŸ¥æ–‡ä»¶", key=f"parameters_clear_target_files_{session_id}"):
                try:
                    for file in os.listdir(target_session_dir):
                        file_path = os.path.join(target_session_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    st.success("å·²æ¸…ç©ºå¾…æ£€æŸ¥æ–‡ä»¶")
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
        with col_clear_graph:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå›¾çº¸æ–‡ä»¶", key=f"parameters_clear_graph_files_{session_id}"):
                try:
                    for file in os.listdir(graph_session_dir):
                        file_path = os.path.join(graph_session_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    st.success("å·²æ¸…ç©ºå›¾çº¸æ–‡ä»¶")
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
        # --- File Manager Module ---
        def get_file_list(folder):
            if not os.path.exists(folder):
                return []
            files = []
            for f in os.listdir(folder):
                file_path = os.path.join(folder, f)
                if os.path.isfile(file_path):
                    stat = os.stat(file_path)
                    files.append({
                        'name': f,
                        'size': stat.st_size,
                        'modified': stat.st_mtime,
                        'path': file_path
                    })
            # Use stable sorting by name first, then by modification time
            return sorted(files, key=lambda x: (x['name'].lower(), x['modified']), reverse=False)

        def format_file_size(size_bytes):
            """Convert bytes to human readable format."""
            if size_bytes == 0:
                return "0 B"
            size_names = ["B", "KB", "MB", "GB"]
            i = 0
            while size_bytes >= 1024 and i < len(size_names) - 1:
                size_bytes /= 1024.0
                i += 1
            return f"{size_bytes:.1f} {size_names[i]}"

        def format_timestamp(timestamp):
            """Convert timestamp to readable date."""
            from datetime import datetime
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M')

        def truncate_filename(filename, max_length=40):
            """Truncate filename if too long, preserving extension."""
            if len(filename) <= max_length:
                return filename
            name, ext = os.path.splitext(filename)
            available_length = max_length - len(ext) - 3
            if available_length <= 0:
                return filename[:max_length-3] + "..."
            truncated_name = name[:available_length] + "..."
            return truncated_name + ext

        # File Manager Tabs
        tab_cp, tab_target, tab_graph = st.tabs(["æ§åˆ¶è®¡åˆ’æ–‡ä»¶", "å¾…æ£€æŸ¥æ–‡ä»¶", "å›¾çº¸æ–‡ä»¶"])
        
        with tab_cp:
            cp_files_list = get_file_list(cp_session_dir)
            if cp_files_list:
                for i, file_info in enumerate(cp_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            delete_key = f"parameters_delete_cp_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    os.remove(file_info['path'])
                                    st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_cp_files = st.file_uploader("é€‰æ‹©æ§åˆ¶è®¡åˆ’æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_cp_uploader_tab_{session_id}")
            if new_cp_files:
                handle_file_upload(new_cp_files, cp_session_dir)

        with tab_target:
            target_files_list = get_file_list(target_session_dir)
            if target_files_list:
                for i, file_info in enumerate(target_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            delete_key = f"parameters_delete_target_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    os.remove(file_info['path'])
                                    st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_target_files = st.file_uploader("é€‰æ‹©å¾…æ£€æŸ¥æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_target_uploader_tab_{session_id}")
            if new_target_files:
                handle_file_upload(new_target_files, target_session_dir)

        with tab_graph:
            graph_files_list = get_file_list(graph_session_dir)
            if graph_files_list:
                for i, file_info in enumerate(graph_files_list):
                    display_name = truncate_filename(file_info['name'])
                    with st.expander(f"ğŸ“„ {display_name}", expanded=False):
                        col_info, col_action = st.columns([3, 1])
                        with col_info:
                            st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
                            st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
                        with col_action:
                            delete_key = f"parameters_delete_graph_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    os.remove(file_info['path'])
                                    st.success(f"å·²åˆ é™¤: {file_info['name']}")
                                except Exception as e:
                                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
            else:
                st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
            st.markdown("---")
            st.markdown("**ä¸Šä¼ æ–°æ–‡ä»¶:**")
            new_graph_files = st.file_uploader("é€‰æ‹©å›¾çº¸æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"parameters_graph_uploader_tab_{session_id}")
            if new_graph_files:
                handle_file_upload(new_graph_files, graph_session_dir)
    # Render MAIN column content: uploaders and controls
    with col_main:
        # Get structured user session
        session = get_user_session(session_id, 'parameters')

        # Always show file upload section
        render_parameters_file_upload_section(session_dirs, session_id)
        
        # Start button - only show if process hasn't started
        if not session['process_started']:
            col_buttons = st.columns([1, 1])
            with col_buttons[0]:
                if st.button("å¼€å§‹", key=f"parameters_start_button_{session_id}"):
                    # Clear any existing generated files to ensure fresh generation
                    output_file = os.path.join(parameters_dir, "parameters_prompt_output.txt")
                    result_file = os.path.join(parameters_dir, "parameters_check_result.txt")
                    
                    if os.path.exists(output_file):
                        os.remove(output_file)
                    if os.path.exists(result_file):
                        os.remove(result_file)
                    
                    # Extract parameters into JSON for this tab
                    try:
                        # Extract CP parameters JSON
                        json_output_path = os.path.join(parameters_dir, "extracted_data.json")
                        summary = extract_parameters_to_json(
                            cp_session_dir=cp_session_dir,
                            output_json_path=json_output_path,
                            # Read config from parameters subfolder instead of CP_files/<user>
                            config_csv_path=os.path.join(parameters_dir, "excel_sheets.csv"),
                        )
                        st.success(f"å·²ç”Ÿæˆå‚æ•°JSON: {summary['output']} (è¡¨: {summary['sheets']}, è¡Œ: {summary['rows']})")

                        # Extract Target parameters JSON
                        json_output_path_t = os.path.join(parameters_dir, "extracted_target_data.json")
                        summary_t = extract_parameters_to_json(
                            cp_session_dir=target_session_dir,
                            output_json_path=json_output_path_t,
                            config_csv_path=os.path.join(parameters_dir, "excel_sheets.csv"),
                        )
                        st.success(f"å·²ç”Ÿæˆç›®æ ‡å‚æ•°JSON: {summary_t['output']} (è¡¨: {summary_t['sheets']}, è¡Œ: {summary_t['rows']})")

                        # Build LLM prompt that embeds both JSON payloads and save to file
                        try:
                            with open(json_output_path_t, 'r', encoding='utf-8') as f:
                                target_json_obj = json.load(f)
                            with open(json_output_path, 'r', encoding='utf-8') as f:
                                cp_json_obj = json.load(f)
                            target_files = sorted({str(item.get('File')) for item in target_json_obj if item.get('File')}) or ["ç›®æ ‡æ–‡ä»¶"]
                            target_json_str = json.dumps(target_json_obj, ensure_ascii=False, indent=2)
                            cp_json_str = json.dumps(cp_json_obj, ensure_ascii=False, indent=2)
                            prompt_text = (
                                "ä½ æ˜¯ä¸€å APQP ä¸“å®¶ï¼Œéœ€è¦å¯¹åº”ç”¨äº¤ä»˜ç‰©è¿›è¡Œè®¾è®¡ä¸åˆ¶ç¨‹å‚æ•°ä¸€è‡´æ€§è¯„å®¡ã€‚\n"
                                "è¯·å¯¹æ¯”ç›®æ ‡æ–‡ä»¶ä¸æ§åˆ¶è®¡åˆ’ä¸­çš„å‚æ•°åç§°ã€å•ä½ã€å–å€¼/å…¬å·®èŒƒå›´ç­‰æ˜¯å¦ä¸€è‡´ï¼Œé€é¡¹ç»™å‡ºï¼šæ˜¯å¦ä¸€è‡´ã€ä¸ä¸€è‡´é¡¹ã€ç¼ºå¤±é¡¹ã€ç–‘ä¼¼é—®é¢˜ï¼Œ"
                                "å¹¶æä¾›å¼•ç”¨ä¾æ®ï¼ˆæ®µè½/æ•°æ®ï¼‰ï¼Œä¸”åœ¨å¼•ç”¨ä¸­åŠ¡å¿…æ ‡æ˜â€œæ–‡ä»¶å + Sheet åç§°â€ï¼ˆä¸è¦æä¾›è¡Œå·ï¼‰ï¼›ç»™å‡ºç®€æ˜çš„æ”¹è¿›å»ºè®®ï¼›æœ€åç»™å‡ºæ€»ä½“ç»“è®ºã€‚\n\n"
                                f"ä»¥ä¸‹ä¸ºç›®æ ‡æ–‡ä»¶ï¼ˆæå–è‡ªï¼š{', '.join(target_files)}ï¼‰çš„å‚æ•°æ•°æ®ï¼ˆJSONï¼‰ï¼š\n"
                                f"{target_json_str}\n\n"
                                "ä»¥ä¸‹ä¸ºç›¸å…³æ§åˆ¶è®¡åˆ’æ–‡ä»¶æ±‡æ€»å¾—åˆ°çš„å‚æ•°æ•°æ®ï¼ˆJSONï¼‰ï¼š\n"
                                f"{cp_json_str}\n\n"
                                "è¯·åŸºäºä¸Šè¿°æ•°æ®å®Œæˆè¯„å®¡ï¼Œå¹¶æŒ‰å‚æ•°é¡¹åˆ†ç»„è¾“å‡ºã€‚"
                            )
                            prompt_path = os.path.join(parameters_dir, "parameters_llm_prompt.txt")
                            with open(prompt_path, 'w', encoding='utf-8') as f:
                                f.write(prompt_text)
                            st.success(f"å·²ç”Ÿæˆè¯„å®¡æç¤ºè¯: {prompt_path}")
                        except Exception as e:
                            st.warning(f"æç¤ºè¯ç”Ÿæˆå¤±è´¥ï¼ˆä¸ä¼šå½±å“åç»­JSONç»“æœï¼‰ï¼š{e}")

                    except Exception as e:
                        st.error(f"å‚æ•°æå–å¤±è´¥: {e}")
                        return
                    
                    # Clear chat history for fresh analysis
                    session['ollama_history'] = []
                    session['openai_history'] = []
                    session['analysis_completed'] = False
                    
                    # Start the analysis process
                    start_analysis(session_id, 'parameters')
                    st.rerun()
            with col_buttons[1]:
                if st.button("æ¼”ç¤º", key=f"parameters_demo_button_{session_id}"):
                    # Copy demo files into this tab's session directories only (isolated)
                    demo_base_dir = CONFIG["directories"]["cp_files"].parent / "demonstration"
                    # Map demonstration folders to this tab's session directories
                    demo_folder_mapping = {
                        "CP_files": "cp",
                        "graph_files": "graph",
                        "target_files": "target",
                    }
                    files_copied = False
                    for demo_folder, session_key in demo_folder_mapping.items():
                        demo_folder_path = os.path.join(demo_base_dir, demo_folder)
                        session_folder_path = session_dirs[session_key]

                        if os.path.exists(demo_folder_path):
                            for file_name in os.listdir(demo_folder_path):
                                demo_file_path = os.path.join(demo_folder_path, file_name)
                                session_file_path = os.path.join(session_folder_path, file_name)
                                if os.path.isfile(demo_file_path):
                                    import shutil
                                    shutil.copy2(demo_file_path, session_file_path)
                                    files_copied = True

                    # Also copy excel_sheets.csv config into parameters_dir for this tab
                    demo_config_file = os.path.join(demo_base_dir, "excel_sheets.csv")
                    if os.path.exists(demo_config_file):
                        import shutil
                        shutil.copy2(demo_config_file, os.path.join(parameters_dir, "excel_sheets.csv"))

                    if files_copied and os.path.exists(os.path.join(parameters_dir, "excel_sheets.csv")):
                        # Auto-generate JSONs and prompt so the analysis view works immediately
                        try:
                            # CP JSON
                            json_output_path = os.path.join(parameters_dir, "extracted_data.json")
                            extract_parameters_to_json(
                                cp_session_dir=cp_session_dir,
                                output_json_path=json_output_path,
                                config_csv_path=os.path.join(parameters_dir, "excel_sheets.csv"),
                            )
                            # Target JSON
                            json_output_path_t = os.path.join(parameters_dir, "extracted_target_data.json")
                            extract_parameters_to_json(
                                cp_session_dir=target_session_dir,
                                output_json_path=json_output_path_t,
                                config_csv_path=os.path.join(parameters_dir, "excel_sheets.csv"),
                            )
                            # Prompt
                            with open(json_output_path_t, 'r', encoding='utf-8') as f:
                                target_json_obj = json.load(f)
                            with open(json_output_path, 'r', encoding='utf-8') as f:
                                cp_json_obj = json.load(f)
                            target_files = sorted({str(item.get('File')) for item in target_json_obj if item.get('File')}) or ["ç›®æ ‡æ–‡ä»¶"]
                            target_json_str = json.dumps(target_json_obj, ensure_ascii=False, indent=2)
                            cp_json_str = json.dumps(cp_json_obj, ensure_ascii=False, indent=2)
                            prompt_text = (
                                "ä½ æ˜¯ä¸€å APQP ä¸“å®¶ï¼Œéœ€è¦å¯¹åº”ç”¨äº¤ä»˜ç‰©è¿›è¡Œè®¾è®¡ä¸åˆ¶ç¨‹å‚æ•°ä¸€è‡´æ€§è¯„å®¡ã€‚\n"
                                "è¯·å¯¹æ¯”ç›®æ ‡æ–‡ä»¶ä¸æ§åˆ¶è®¡åˆ’ä¸­çš„å‚æ•°åç§°ã€å•ä½ã€å–å€¼/å…¬å·®èŒƒå›´ç­‰æ˜¯å¦ä¸€è‡´ï¼Œé€é¡¹ç»™å‡ºï¼šæ˜¯å¦ä¸€è‡´ã€ä¸ä¸€è‡´é¡¹ã€ç¼ºå¤±é¡¹ã€ç–‘ä¼¼é—®é¢˜ï¼Œ"
                                "å¹¶æä¾›å¼•ç”¨ä¾æ®ï¼ˆæ®µè½/æ•°æ®ï¼‰ï¼Œä¸”åœ¨å¼•ç”¨ä¸­åŠ¡å¿…æ ‡æ˜â€œæ–‡ä»¶å + Sheet åç§°â€ï¼ˆä¸è¦æä¾›è¡Œå·ï¼‰ï¼›ç»™å‡ºç®€æ˜çš„æ”¹è¿›å»ºè®®ï¼›æœ€åç»™å‡ºæ€»ä½“ç»“è®ºã€‚\n\n"
                                f"ä»¥ä¸‹ä¸ºç›®æ ‡æ–‡ä»¶ï¼ˆæå–è‡ªï¼š{', '.join(target_files)}ï¼‰çš„å‚æ•°æ•°æ®ï¼ˆJSONï¼‰ï¼š\n"
                                f"{target_json_str}\n\n"
                                "ä»¥ä¸‹ä¸ºç›¸å…³æ§åˆ¶è®¡åˆ’æ–‡ä»¶æ±‡æ€»å¾—åˆ°çš„å‚æ•°æ•°æ®ï¼ˆJSONï¼‰ï¼š\n"
                                f"{cp_json_str}\n\n"
                                "è¯·åŸºäºä¸Šè¿°æ•°æ®å®Œæˆè¯„å®¡ï¼Œå¹¶æŒ‰å‚æ•°é¡¹åˆ†ç»„è¾“å‡ºã€‚"
                            )
                            prompt_path = os.path.join(parameters_dir, "parameters_llm_prompt.txt")
                            with open(prompt_path, 'w', encoding='utf-8') as f:
                                f.write(prompt_text)
                        except Exception as e:
                            st.warning(f"æ¼”ç¤ºå‡†å¤‡æç¤ºè¯å¤±è´¥ï¼š{e}")

                        # Prepare this tab's session state and start analysis lifecycle
                        session['analysis_completed'] = False
                        session['process_started'] = True
                        session['ollama_history'] = []
                        session['openai_history'] = []
                        st.rerun()
                    else:
                        st.info("æœªæ‰¾åˆ°æ¼”ç¤ºæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ demonstration ç›®å½•ã€‚")

            
        
        # Show status and reset button if process has started
        if session['process_started']:
            # Add a button to reset and clear history
            if st.button("é‡æ–°å¼€å§‹", key=f"parameters_reset_button_{session_id}"):
                reset_user_session(session_id, 'parameters')
                st.rerun()
            
            # Check if we need to run analysis
            target_files_list = [f for f in os.listdir(target_session_dir) if os.path.isfile(os.path.join(target_session_dir, f))]
            if target_files_list:
                if session['process_started'] and not session['analysis_completed']:
                    # Run the analysis workflow
                    run_parameters_analysis_workflow(session_id, session_dirs)
                    
                    # Mark as completed
                    session['analysis_completed'] = True
                else:
                    # Files exist but process wasn't explicitly started
                    st.info("æ£€æµ‹åˆ°å¾…æ£€æŸ¥æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»\"å¼€å§‹\"æŒ‰é’®å¼€å§‹åˆ†æï¼Œæˆ–ç‚¹å‡»\"æ¼”ç¤º\"æŒ‰é’®ä½¿ç”¨æ¼”ç¤ºæ–‡ä»¶ã€‚")
            else:
                st.warning("è¯·å…ˆä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶")


        # (Bulk operations moved earlier to avoid duplicate keys and to update UI promptly)