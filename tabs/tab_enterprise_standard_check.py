import streamlit as st
import os
import io
import zipfile
import json
import re
import time
import requests
from util import ensure_session_dirs, handle_file_upload, resolve_ollama_host
from config import CONFIG
from ollama import Client as OllamaClient
from bisheng_client import (
    call_workflow_invoke,
    call_flow_process,
    parse_flow_answer,
    split_to_chunks,
    stop_workflow,
    find_knowledge_id_by_name,
    create_knowledge,
    kb_sync_folder,
)


# 
# --- Bisheng fixed settings (edit here if endpoints or workflow change) ---
# Base URL of Bisheng server
BISHENG_BASE_URL = "http://10.31.60.11:3001"
# Invoke and Stop API paths (workflow kept for other tabs; Flow uses /api/v1/process)
BISHENG_INVOKE_PATH = "/api/v2/workflow/invoke"
BISHENG_STOP_PATH = "/api/v2/workflow/stop"
# Default workflow id and API key (if your server requires one)
BISHENG_WORKFLOW_ID = "31208af992c94e9fb56b759ebff2f242"
# Flow settings: set to your Flow id and node ids per exported JSON
BISHENG_FLOW_ID = "0bbd56f359454b4ea67ca85f4460e969"  # çŸ¥è¯†åº“é—®ç­”ã€å¢å¼ºç‰ˆã€‘-1dae2
FLOW_INPUT_NODE_ID = "RetrievalQA-f0f31"
FLOW_MILVUS_NODE_ID = "Milvus-cyR5W"
FLOW_ES_NODE_ID = "ElasticKeywordsSearch-1c80e"
BISHENG_API_KEY = ""
# Chunking and request timeout
BISHENG_MAX_WORDS = 2000
BISHENG_TIMEOUT_S = 90

# Knowledge base settings
# Model ID used when creating a new KB; keep consistent with server defaults
KB_MODEL_ID = 7
# Short slug for this tab; used to synthesize per-user KB names
TAB_SLUG = "enterprise"


def _estimate_tokens(text: str) -> int:
    """Rudimentary token estimate: Chinese chars + latin-word count.

    Good enough for correlation studies without server-side tokenizers.
    """
    try:
        cjk = len(re.findall(r"[\u4E00-\u9FFF]", text or ""))
        latin_words = len(re.findall(r"[A-Za-z0-9_]+", text or ""))
        return cjk + latin_words
    except Exception:
        return max(1, len(text or "") // 2)


def _get_metrics_path(base_out_dir: str, session_id: str) -> str:
	# æ¯æ¬¡è¿è¡Œç”Ÿæˆä¸€æ¬¡æ—¶é—´æˆ³æ–‡ä»¶åå¹¶ç¼“å­˜åœ¨ session_state
	metrics_dir = os.path.join(base_out_dir, "metrics")
	os.makedirs(metrics_dir, exist_ok=True)
	key = f"metrics_file_{session_id}"
	fname = st.session_state.get(key)
	if not fname:
		from datetime import datetime as _dt
		ts = _dt.now().strftime("%Y%m%d_%H%M%S")
		fname = f"llm_calls_{ts}.csv"
		st.session_state[key] = fname
	return os.path.join(metrics_dir, fname)

def _log_llm_metrics(base_out_dir: str, session_id: str, row: dict):
	try:
		import csv
		path = _get_metrics_path(base_out_dir, session_id)
		exists = os.path.exists(path)
		headers = [
			"ts","engine","model","session_id","file","part","phase",
			"prompt_chars","prompt_tokens","output_chars","output_tokens",
			"duration_ms","success","error"
		]
		with open(path, 'a', encoding='utf-8-sig', newline='') as f:
			w = csv.writer(f)
			if not exists:
				w.writerow(headers)
			w.writerow([
				row.get("ts"), row.get("engine"), row.get("model"), row.get("session_id"),
				row.get("file"), row.get("part"), row.get("phase"),
				row.get("prompt_chars"), row.get("prompt_tokens"), row.get("output_chars"),
				row.get("output_tokens"), row.get("duration_ms"), row.get("success"), row.get("error")
			])
	except Exception:
		pass

def _list_pdfs(folder: str):
	"""Return absolute paths for all PDF files in a folder (non-recursive)."""
	try:
		return [
			os.path.join(folder, f)
			for f in os.listdir(folder)
			if os.path.isfile(os.path.join(folder, f)) and f.lower().endswith('.pdf')
		]
	except Exception:
		return []


def _mineru_parse_pdf(pdf_path: str) -> bytes:
	"""Call MinerU API to parse a single PDF and return ZIP bytes on success.

	Raises an exception on failure.
	"""
	api_url = "http://10.31.60.127:8000/file_parse"
	data = {
		'backend': 'vlm-sglang-engine',
		'response_format_zip': 'true',
		# Enable richer outputs; we will primarily consume the .md text for now
		'formula_enable': 'true',
		'table_enable': 'true',
		'return_images': 'false',
		'return_middle_json': 'true',
		'return_model_output': 'false',
		'return_content_list': 'true',
	}
	with open(pdf_path, 'rb') as f:
		files = {'files': (os.path.basename(pdf_path), f, 'application/pdf')}
		resp = requests.post(api_url, data=data, files=files, timeout=300)
		if resp.status_code != 200:
			raise RuntimeError(f"MinerU API error {resp.status_code}: {resp.text[:200]}")
		return resp.content


def _zip_to_txts(zip_bytes: bytes, target_txt_path: str) -> bool:
	"""Extract first .md file from ZIP bytes and save as plain text (.txt).

	Returns True if a .txt was written, False otherwise.
	"""
	# MinerU returns a ZIP archive for each PDF containing: a Markdown file (extracted
	# plain text content), JSONs (structured intermediates), and optionally images.
	# For now we only need plain text for LLM prompts, so we take the first .md file
	# and write it out as a .txt. The images are intentionally ignored here, but they
	# are valuable for future RAG/Q&A over figures or diagrams. We will revisit image
	# handling later to index them alongside text for multimodal retrieval.
	bio = io.BytesIO(zip_bytes)
	try:
		with zipfile.ZipFile(bio) as zf:
			# Prefer top-level or nested .md
			md_members = [n for n in zf.namelist() if n.lower().endswith('.md')]
			if not md_members:
				return False
			# Use the first .md
			name = md_members[0]
			content = zf.read(name)
			# Ensure output directory exists
			os.makedirs(os.path.dirname(target_txt_path), exist_ok=True)
			with open(target_txt_path, 'wb') as out_f:
				out_f.write(content)
			return True
	except zipfile.BadZipFile:
		return False

def _insert_source_markers(text: str, source_label: str, line_interval: int = 80) -> str:
	"""Insert unobtrusive source markers so small retrieved fragments retain provenance.

	Strategy:
	- Add a file-level header at top: ã€æ¥æºæ–‡ä»¶: <name>ã€‘
	- If Markdown headings (#/##) are present, add a marker right after each H1/H2.
	- Otherwise, add a marker every N non-empty lines (default 80).

	Idempotent: if a marker containing this source_label already exists, return original text.
	"""
	marker = f"ã€æ¥æºæ–‡ä»¶: {source_label}ã€‘"
	if source_label and marker in text:
		return text
	lines = text.splitlines()
	has_md_heading = any(re.match(r'^\s{0,3}#{1,2}\s+\S', ln) for ln in lines[:500])

	annotated_lines = []
	# Always place a header at the very top
	annotated_lines.append(marker)
	annotated_lines.append("")

	if has_md_heading:
		for ln in lines:
			annotated_lines.append(ln)
			if re.match(r'^\s{0,3}#{1,2}\s+\S', ln):
				annotated_lines.append(marker)
	else:
		non_empty_count = 0
		for ln in lines:
			annotated_lines.append(ln)
			if ln.strip():
				non_empty_count += 1
				if non_empty_count % max(10, int(line_interval)) == 0:
					annotated_lines.append(marker)

	return "\n".join(annotated_lines)


def _annotate_txt_file_inplace(file_path: str, source_label: str, line_interval: int = 80) -> bool:
	"""Open a .txt file and inject source markers in-place. Returns True if updated."""
	try:
		with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
			original = f.read()
		annotated = _insert_source_markers(original, source_label, line_interval=line_interval)
		if annotated == original:
			return False
		with open(file_path, 'w', encoding='utf-8') as f:
			f.write(annotated)
		return True
	except Exception:
		return False

def _process_pdf_folder(input_dir: str, output_dir: str, progress_area, annotate_sources: bool = False):
	"""Process all PDFs in input_dir via MinerU and write .txts into output_dir."""
	pdf_paths = _list_pdfs(input_dir)
	if not pdf_paths:
		return []
	created = []
	for pdf_path in pdf_paths:
		orig_name = os.path.basename(pdf_path)
		# Preserve original extension in output filename, e.g., name.pdf.txt
		out_txt = os.path.join(output_dir, f"{orig_name}.txt")
		try:
			# Skip if parsed file already exists and is non-empty
			if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
				progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
				continue
			progress_area.write(f"è§£æ: {os.path.basename(pdf_path)} â€¦")
			zip_bytes = _mineru_parse_pdf(pdf_path)
			ok = _zip_to_txts(zip_bytes, out_txt)
			if ok:
				# Inject source markers so retrieved snippets carry provenance
				if annotate_sources:
					_annotate_txt_file_inplace(out_txt, orig_name)
				created.append(out_txt)
			else:
				progress_area.warning(f"æœªå‘ç°å¯ç”¨çš„ .md å†…å®¹ï¼Œè·³è¿‡: {os.path.basename(pdf_path)}")
		except Exception as e:
			progress_area.error(f"å¤±è´¥: {os.path.basename(pdf_path)} â†’ {e}")
	return created


def _list_word_ppt(folder: str):
	"""Return absolute paths for .doc, .docx, .ppt, .pptx in a folder (non-recursive)."""
	try:
		return [
			os.path.join(folder, f)
			for f in os.listdir(folder)
			if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in {'.doc', '.docx', '.ppt', '.pptx'}
		]
	except Exception:
		return []


def _unstructured_partition_to_txt(file_path: str, target_txt_path: str) -> bool:
	"""Send a single Word/PPT file to Unstructured API and write plain text (.txt).

	The Unstructured server is expected at 10.31.60.11 running the API. We call the
	"general" endpoint and extract text fields. Table-like data, if present, are
	flattened with tab separators for readability in plain text.

	Future plan (RAG-focused tables):
	- Keep Unstructured for narrative text/structure.
	- Extract tables directly from the original DOCX/PPTX using python-docx/python-pptx.
	- Convert those tables to TSV (one row per line, cells separated by a single tab).
	- Replace or insert TSV blocks into the final output .txt in place of flattened table text.
	This will improve recall/precision on numeric lookups. Not implemented yet; will
	be added later when schedule allows.
	"""
	# Resolve API URL: env var first, then CONFIG.services.unstructured_api_url
	api_url = os.getenv('UNSTRUCTURED_API_URL') or CONFIG.get('services', {}).get('unstructured_api_url') or 'http://10.31.60.11:8000/general/v0/general'
	try:
		with open(file_path, 'rb') as f:
			files = {'files': (os.path.basename(file_path), f)}
			# RAG-optimized defaults: structured tables, auto strategy, Chinese+English OCR support
			form = {
				"strategy": "auto",
				"ocr_languages": "chi_sim,eng",
				"infer_table_structure": "true",
			}
			resp = requests.post(api_url, files=files, data=form, timeout=300)
			if resp.status_code != 200:
				raise RuntimeError(f"Unstructured API {resp.status_code}: {resp.text[:200]}")
			data = resp.json()
			# data is expected to be a list of elements; each may have 'text' or table-like content
			lines = []
			if isinstance(data, list):
				for el in data:
					# Prefer 'text'
					text = None
					if isinstance(el, dict):
						# Common key is 'text'
						text = el.get('text')
						# Some table extractions might be under 'data' (list of rows)
						if not text and isinstance(el.get('data'), list):
							for row in el['data']:
								if isinstance(row, list):
									lines.append('\t'.join(str(c) for c in row))
							# Continue to next element after adding table rows
							continue
					if isinstance(text, str) and text.strip():
						lines.append(text.strip())
			# Write as UTF-8 plain text
			os.makedirs(os.path.dirname(target_txt_path), exist_ok=True)
			with open(target_txt_path, 'w', encoding='utf-8') as out_f:
				out_f.write('\n'.join(lines))
			return True
	except Exception as e:
		# Surface errors to caller via return False; logging via progress UI
		return False


def _process_word_ppt_folder(input_dir: str, output_dir: str, progress_area, annotate_sources: bool = False):
	"""Process .doc/.docx/.ppt/.pptx via Unstructured API and write .txts."""
	paths = _list_word_ppt(input_dir)
	if not paths:
		return []
	created = []
	for p in paths:
		orig_name = os.path.basename(p)
		# Preserve original extension in output filename, e.g., name.docx.txt / name.ppt.txt
		out_txt = os.path.join(output_dir, f"{orig_name}.txt")
		try:
			# Skip if parsed file already exists and is non-empty
			if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
				progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
				continue
			progress_area.write(f"è§£æ(Word/PPT): {os.path.basename(p)} â€¦")
			ok = _unstructured_partition_to_txt(p, out_txt)
			if ok:
				# Inject source markers
				if annotate_sources:
					_annotate_txt_file_inplace(out_txt, orig_name)
				created.append(out_txt)
			else:
				progress_area.warning(f"æœªèƒ½ä»æ–‡ä»¶ä¸­ç”Ÿæˆæ–‡æœ¬ï¼Œè·³è¿‡: {os.path.basename(p)}")
		except Exception as e:
			progress_area.error(f"å¤±è´¥: {os.path.basename(p)} â†’ {e}")
	return created


def _list_excels(folder: str):
	"""Return absolute paths for .xls/.xlsx/.xlsm in a folder (non-recursive)."""
	try:
		return [
			os.path.join(folder, f)
			for f in os.listdir(folder)
			if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in {'.xls', '.xlsx', '.xlsm'}
		]
	except Exception:
		return []


def _sanitize_sheet_name(name: str) -> str:
	"""Sanitize sheet names for filenames: keep readable, remove path-forbidden chars."""
	bad = ['\\', '/', ':', '*', '?', '"', '<', '>', '|']
	for ch in bad:
		name = name.replace(ch, '_')
	return '_'.join(name.strip().split())[:80] or 'Sheet'


def _process_excel_folder(input_dir: str, output_dir: str, progress_area, annotate_sources: bool = False):
	"""Convert each Excel sheet to CSV text and save as <file>_SHEET_<sheet>.txt.

	Note: We intentionally save CSV content with a .txt extension for uniform LLM
	consumption. This is technically fine: the content is plain text CSV and the
	file extension does not affect parsing for our use case.
	"""
	paths = _list_excels(input_dir)
	if not paths:
		return []
	created = []
	import pandas as pd
	for excel_path in paths:
		orig_name = os.path.basename(excel_path)  # keep extension in base name per spec
		try:
			xls = pd.ExcelFile(excel_path)
			for sheet in xls.sheet_names:
				safe_sheet = _sanitize_sheet_name(sheet)
				out_txt = os.path.join(output_dir, f"{orig_name}_SHEET_{safe_sheet}.txt")
				# Skip if exists and non-empty
				if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
					progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
					continue
				progress_area.write(f"è½¬æ¢(Excelâ†’CSV): {orig_name} / {sheet} â€¦")
				df = xls.parse(sheet)
				# Write CSV content into .txt
				df.to_csv(out_txt, index=False, encoding='utf-8')
				# Inject source markers including sheet context
				if annotate_sources:
					_annotate_txt_file_inplace(out_txt, f"{orig_name} / {sheet}")
				created.append(out_txt)
		except Exception as e:
			progress_area.error(f"å¤±è´¥: {orig_name} â†’ {e}")
	return created


def _process_textlike_folder(input_dir: str, output_dir: str, progress_area):
	"""Copy text-like files (csv, tsv, md, txt, json, yaml, yml, log, ini, cfg, rst)
	to output_dir with .txt extension. Skip if the target exists and is non-empty.
	"""
	try:
		if not os.path.isdir(input_dir):
			return []
		exts = {
			'.txt', '.md', '.csv', '.tsv', '.json', '.yaml', '.yml', '.log', '.ini', '.cfg', '.rst'
		}
		os.makedirs(output_dir, exist_ok=True)
		written = []
		for name in os.listdir(input_dir):
			spath = os.path.join(input_dir, name)
			if not os.path.isfile(spath):
				continue
			ext = os.path.splitext(name)[1].lower()
			if ext not in exts:
				continue
			base = os.path.splitext(name)[0]
			dst = os.path.join(output_dir, f"{base}.txt")
			try:
				if os.path.exists(dst) and os.path.getsize(dst) > 0:
					progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(dst)}")
					continue
				with open(spath, 'r', encoding='utf-8', errors='ignore') as fr:
					content = fr.read()
				with open(dst, 'w', encoding='utf-8') as fw:
					fw.write(content)
				written.append(dst)
			except Exception as e:
				progress_area.warning(f"å¤åˆ¶å¤±è´¥: {name} â†’ {e}")
		return written
	except Exception:
		return []


def _process_archives(input_dir: str, output_dir: str, progress_area) -> int:
	"""Extract archives (.zip, optionally .7z/.rar if libs available) and process their contents
	into text outputs under output_dir. Returns number of archives processed.

	Strategy:
	- For each archive in input_dir, extract into a temp folder under output_dir.
	- For each extracted subfolder, run the existing processors on that folder:
	  PDFs â†’ MinerU; Word/PPT â†’ Unstructured; Excel â†’ CSVâ†’.txt; text-like â†’ copy.
	- Clean up extracted temp if possible.
	"""
	try:
		if not os.path.isdir(input_dir):
			return 0
		import shutil
		import tempfile
		processed = 0
		# Optional handlers
		try:
			import py7zr  # type: ignore
		except Exception:
			py7zr = None  # type: ignore
		try:
			import rarfile  # type: ignore
		except Exception:
			rarfile = None  # type: ignore

		for name in os.listdir(input_dir):
			spath = os.path.join(input_dir, name)
			if not os.path.isfile(spath):
				continue
			ext = os.path.splitext(name)[1].lower()
			if ext not in {'.zip', '.7z', '.rar'}:
				continue
			# Create temp extraction root
			tmp_root = tempfile.mkdtemp(prefix="extract_", dir=output_dir)
			ok = False
			try:
				if ext == '.zip':
					try:
						with zipfile.ZipFile(spath) as zf:
							zf.extractall(tmp_root)
						ok = True
					except Exception as e:
						progress_area.warning(f"è§£å‹å¤±è´¥: {name} â†’ {e}")
				elif ext == '.7z' and py7zr is not None:
					try:
						with py7zr.SevenZipFile(spath, mode='r') as z:
							z.extractall(path=tmp_root)
						ok = True
					except Exception as e:
						progress_area.warning(f"è§£å‹7zå¤±è´¥: {name} â†’ {e}")
				elif ext == '.rar' and rarfile is not None:
					try:
						rf = rarfile.RarFile(spath)
						rf.extractall(tmp_root)
						rf.close()
						ok = True
					except Exception as e:
						progress_area.warning(f"è§£å‹rarå¤±è´¥: {name} â†’ {e}")
				else:
					# Unsupported archive (missing libs)
					progress_area.info(f"è·³è¿‡æœªæ”¯æŒçš„å‹ç¼©åŒ…: {name}")

				if ok:
					# Walk extracted tree and process each folder with existing processors
					for root, dirs, files in os.walk(tmp_root):
						# Run per-folder handlers (they skip existing outputs)
						_process_pdf_folder(root, output_dir, progress_area, annotate_sources=True)
						_process_word_ppt_folder(root, output_dir, progress_area, annotate_sources=True)
						_process_excel_folder(root, output_dir, progress_area, annotate_sources=True)
						_process_textlike_folder(root, output_dir, progress_area)
					processed += 1
			finally:
				# Cleanup temp extraction root
				try:
					shutil.rmtree(tmp_root, ignore_errors=True)
				except Exception:
					pass
		return processed
	except Exception:
		return 0

def _cleanup_orphan_txts(source_dir: str, output_dir: str, progress_area=None) -> int:
	"""Delete .txt files in output_dir that do not correspond to files currently in source_dir.

	Rules:
	- For PDF/Word/PPT sources: keep exact name match "<orig_name>.txt" where <orig_name> includes the extension
	  (e.g., foo.pdf -> foo.pdf.txt, bar.docx -> bar.docx.txt).
	- For Excel sources: keep any file starting with "<orig_name>_SHEET_" (multiple per workbook).
	"""
	try:
		if not os.path.isdir(output_dir):
			return 0
		# Build allowlists from current source files
		keep_exact = set()
		keep_prefixes = []
		try:
			for fname in os.listdir(source_dir or "."):
				spath = os.path.join(source_dir, fname)
				if not os.path.isfile(spath):
					continue
				ext = os.path.splitext(fname)[1].lower()
				if ext in {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}:
					keep_exact.add((fname + '.txt').lower())
				elif ext in {'.xls', '.xlsx', '.xlsm'}:
					keep_prefixes.append((fname + '_SHEET_').lower())
		except Exception:
			# If source_dir not accessible, do not delete anything
			return 0

		deleted_count = 0
		for oname in os.listdir(output_dir):
			opath = os.path.join(output_dir, oname)
			if not os.path.isfile(opath):
				continue
			name_lower = oname.lower()
			if not name_lower.endswith('.txt'):
				continue
			keep = name_lower in keep_exact or any(name_lower.startswith(pfx) for pfx in keep_prefixes)
			if not keep:
				try:
					os.remove(opath)
					deleted_count += 1
					if progress_area is not None:
						progress_area.info(f"æ¸…ç†æ— å…³æ–‡æœ¬: {oname}")
				except Exception:
					pass
		return deleted_count
	except Exception:
		return 0


def render_enterprise_standard_check_tab(session_id):
	# Handle None session_id (user not logged in)
	if session_id is None:
		st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
		return

	st.subheader("ğŸ¢ ä¼ä¸šæ ‡å‡†æ£€æŸ¥")

	# No CSS width overrides; rely on Streamlit columns like special symbols tab
	# Ensure enterprise directories and a generated output root exist
	base_dirs = {
		"generated": str(CONFIG["directories"]["generated_files"]),
	}
	session_dirs = ensure_session_dirs(base_dirs, session_id)
	standards_dir = session_dirs.get("enterprise_standards")
	examined_dir = session_dirs.get("enterprise_examined")
	generated_session_dir = session_dirs.get("generated")
	enterprise_out_root = os.path.join(generated_session_dir, "enterprise_standard_check")
	standards_txt_dir = os.path.join(enterprise_out_root, "standards_txt")
	examined_txt_dir = os.path.join(enterprise_out_root, "examined_txt")
	os.makedirs(standards_txt_dir, exist_ok=True)
	os.makedirs(examined_txt_dir, exist_ok=True)

	# Layout: right column for info, left for main content
	col_main, col_info = st.columns([2, 1])

	with col_info:
		# Right column intentionally limited to file manager and utilities only
		# File manager utilities (mirroring completeness tab behavior)
		def get_file_list(folder):
			if not folder or not os.path.exists(folder):
				return []
			files = []
			for f in os.listdir(folder):
				file_path = os.path.join(folder, f)
				if os.path.isfile(file_path):
					stat = os.stat(file_path)
					files.append({
						'name': f,
						'size': stat.st_size,
						'modified': stat.st_mtime,
						'path': file_path
					})
			# Sort by name then modified time for stability
			return sorted(files, key=lambda x: (x['name'].lower(), x['modified']))

		def format_file_size(size_bytes):
			if size_bytes == 0:
				return "0 B"
			size_names = ["B", "KB", "MB", "GB"]
			i = 0
			while size_bytes >= 1024 and i < len(size_names) - 1:
				size_bytes /= 1024.0
				i += 1
			return f"{size_bytes:.1f} {size_names[i]}"

		def format_timestamp(timestamp):
			from datetime import datetime
			return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M')

		def truncate_filename(filename, max_length=40):
			if len(filename) <= max_length:
				return filename
			name, ext = os.path.splitext(filename)
			available_length = max_length - len(ext) - 3
			if available_length <= 0:
				return filename[:max_length-3] + "..."
			truncated_name = name[:available_length] + "..."
			return truncated_name + ext

		# Clear buttons
		col_clear1, col_clear2, col_clear3 = st.columns(3)
		with col_clear1:
			if st.button("ğŸ—‘ï¸ æ¸…ç©ºä¼ä¸šæ ‡å‡†æ–‡ä»¶", key=f"clear_enterprise_std_{session_id}"):
				try:
					for file in os.listdir(standards_dir):
						file_path = os.path.join(standards_dir, file)
						if os.path.isfile(file_path):
							os.remove(file_path)
					st.success("å·²æ¸…ç©ºä¼ä¸šæ ‡å‡†æ–‡ä»¶")
				except Exception as e:
					st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
		with col_clear2:
			if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¾…æ£€æŸ¥æ–‡ä»¶", key=f"clear_enterprise_exam_{session_id}"):
				try:
					for file in os.listdir(examined_dir):
						file_path = os.path.join(examined_dir, file)
						if os.path.isfile(file_path):
							os.remove(file_path)
					st.success("å·²æ¸…ç©ºå¾…æ£€æŸ¥æ–‡ä»¶")
				except Exception as e:
					st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
		with col_clear3:
			if st.button("ğŸ—‘ï¸ æ¸…ç©ºåˆ†æç»“æœ", key=f"clear_enterprise_results_{session_id}"):
				try:
					final_dir_path = os.path.join(enterprise_out_root, 'final_results')
					deleted_count = 0
					if os.path.isdir(final_dir_path):
						for fname in os.listdir(final_dir_path):
							fpath = os.path.join(final_dir_path, fname)
							if os.path.isfile(fpath):
								os.remove(fpath)
								deleted_count += 1
					st.success(f"å·²æ¸…ç©ºåˆ†æç»“æœï¼ˆ{deleted_count} ä¸ªæ–‡ä»¶ï¼‰")
				except Exception as e:
					st.error(f"æ¸…ç©ºå¤±è´¥: {e}")

		# File lists in tabs (fixed order)
		tab_std, tab_exam, tab_results = st.tabs(["ä¼ä¸šæ ‡å‡†æ–‡ä»¶", "å¾…æ£€æŸ¥æ–‡ä»¶", "åˆ†æç»“æœ"])
		with tab_std:
			std_files = get_file_list(standards_dir)
			if std_files:
				for file_info in std_files:
					display_name = truncate_filename(file_info['name'])
					with st.expander(f"ğŸ“„ {display_name}", expanded=False):
						col_i, col_a = st.columns([3, 1])
						with col_i:
							st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
							st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
							st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
						with col_a:
							delete_key = f"del_std_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
							if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
								try:
									os.remove(file_info['path'])
									st.success(f"å·²åˆ é™¤: {file_info['name']}")
								except Exception as e:
									st.error(f"åˆ é™¤å¤±è´¥: {e}")
			else:
				st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")

		with tab_exam:
			exam_files = get_file_list(examined_dir)
			if exam_files:
				for file_info in exam_files:
					display_name = truncate_filename(file_info['name'])
					with st.expander(f"ğŸ“„ {display_name}", expanded=False):
						col_i, col_a = st.columns([3, 1])
						with col_i:
							st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
							st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
							st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
						with col_a:
							delete_key = f"del_exam_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
							if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
								try:
									os.remove(file_info['path'])
									st.success(f"å·²åˆ é™¤: {file_info['name']}")
								except Exception as e:
									st.error(f"åˆ é™¤å¤±è´¥: {e}")
			else:
				st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")

		with tab_results:
			# List files under generated/<session>/enterprise_standard_check/final_results
			final_dir = os.path.join(enterprise_out_root, 'final_results')
			if os.path.isdir(final_dir):
				final_files = get_file_list(final_dir)
				if final_files:
					for file_info in final_files:
						display_name = truncate_filename(file_info['name'])
						with st.expander(f"ğŸ“„ {display_name}", expanded=False):
							col_i, col_a = st.columns([4, 1])
							with col_i:
								st.write(f"**æ–‡ä»¶å:** {file_info['name']}")
								st.write(f"**å¤§å°:** {format_file_size(file_info['size'])}")
								st.write(f"**ä¿®æ”¹æ—¶é—´:** {format_timestamp(file_info['modified'])}")
							with col_a:
								try:
									with open(file_info['path'], 'rb') as _fbin:
										_data = _fbin.read()
									st.download_button(
										label="â¬‡ï¸ ä¸‹è½½",
										data=_data,
										file_name=file_info['name'],
										mime='application/octet-stream',
										key=f"dl_final_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
									)
								except Exception as e:
									st.error(f"ä¸‹è½½å¤±è´¥: {e}")
								delete_key = f"del_final_{file_info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
								if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
									try:
										os.remove(file_info['path'])
										st.success(f"å·²åˆ é™¤: {file_info['name']}")
									except Exception as e:
										st.error(f"åˆ é™¤å¤±è´¥: {e}")
				else:
					st.write("ï¼ˆæš‚æ— åˆ†æç»“æœï¼‰")
			else:
				st.write("ï¼ˆæš‚æ— åˆ†æç»“æœç›®å½•ï¼‰")




	with col_main:
		# Two uploaders side by side
		col_std, col_exam = st.columns(2)
		with col_std:
			files_std = st.file_uploader("ç‚¹å‡»ä¸Šä¼ ä¼ä¸šæ ‡å‡†æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"enterprise_std_{session_id}")
			if files_std:
				handle_file_upload(files_std, standards_dir)
				st.success(f"å·²ä¸Šä¼  {len(files_std)} ä¸ªä¼ä¸šæ ‡å‡†æ–‡ä»¶")
		with col_exam:
			files_exam = st.file_uploader("ç‚¹å‡»ä¸Šä¼ å¾…æ£€æŸ¥æ–‡ä»¶", type=None, accept_multiple_files=True, key=f"enterprise_exam_{session_id}")
			if files_exam:
				handle_file_upload(files_exam, examined_dir)
				st.success(f"å·²ä¸Šä¼  {len(files_exam)} ä¸ªå¾…æ£€æŸ¥æ–‡ä»¶")

		# Start / Stop / Demo buttons
		btn_col1, btn_col_stop, btn_col2 = st.columns([1, 1, 1])
		with btn_col1:
			if st.button("å¼€å§‹", key=f"enterprise_start_button_{session_id}"):
				# Process PDFs (MinerU) and Word/PPT (Unstructured) into plain text
				area = st.container()
				with area:
					# Step 0: Clean orphan .txt files that don't correspond to current uploads
					try:
						removed_std = _cleanup_orphan_txts(standards_dir, standards_txt_dir, st)
						removed_exam = _cleanup_orphan_txts(examined_dir, examined_txt_dir, st)
						if removed_std or removed_exam:
							st.info(f"å·²æ¸…ç†æ— å…³æ–‡æœ¬ {removed_std + removed_exam} ä¸ª")
					except Exception:
						pass
					# Step 0b: Clear previous run results so current run writes fresh outputs
					try:
						initial_dir = os.path.join(enterprise_out_root, 'initial_results')
						os.makedirs(initial_dir, exist_ok=True)
						cleared = 0
						for fname in os.listdir(initial_dir):
							fpath = os.path.join(initial_dir, fname)
							if os.path.isfile(fpath):
								try:
									os.remove(fpath)
									cleared += 1
								except Exception:
									pass
						if cleared:
							st.info(f"å·²æ¸…ç©ºä¸Šæ¬¡è¿è¡Œç»“æœ {cleared} ä¸ªæ–‡ä»¶")
					except Exception:
						pass
					st.markdown("**é˜…è¯»ä¼ä¸šæ ‡å‡†æ–‡ä»¶ä¸­ï¼Œ10åˆ†é’Ÿå·¦å³ï¼Œè¯·ç­‰å¾…...**")
					created_std_pdf = _process_pdf_folder(standards_dir, standards_txt_dir, st, annotate_sources=True)
					created_std_wp = _process_word_ppt_folder(standards_dir, standards_txt_dir, st, annotate_sources=True)
					created_std_xls = _process_excel_folder(standards_dir, standards_txt_dir, st, annotate_sources=True)
					# New: copy text-like files directly to standards_txt
					created_std_text = _process_textlike_folder(standards_dir, standards_txt_dir, st)
					# New: extract archives and process recursively
					_ = _process_archives(standards_dir, standards_txt_dir, st)
					st.markdown("**é˜…è¯»å¾…æ£€æŸ¥æ–‡ä»¶ä¸­ï¼Œ10åˆ†é’Ÿå·¦å³ï¼Œè¯·ç­‰å¾…...**")
					created_exam_pdf = _process_pdf_folder(examined_dir, examined_txt_dir, st, annotate_sources=False)
					created_exam_wp = _process_word_ppt_folder(examined_dir, examined_txt_dir, st, annotate_sources=False)
					created_exam_xls = _process_excel_folder(examined_dir, examined_txt_dir, st, annotate_sources=False)
					# New: copy text-like files directly to examined_txt
					created_exam_text = _process_textlike_folder(examined_dir, examined_txt_dir, st)
					# New: extract archives and process recursively
					_ = _process_archives(examined_dir, examined_txt_dir, st)

					# If we have any txt, switch to running phase and rerun so streaming renders in main column
					try:
						std_txt_files = [f for f in os.listdir(standards_txt_dir) if f.lower().endswith('.txt')] if os.path.isdir(standards_txt_dir) else []
						exam_txt_files = [f for f in os.listdir(examined_txt_dir) if f.lower().endswith('.txt')] if os.path.isdir(examined_txt_dir) else []
						if not exam_txt_files:
							st.warning("æœªå‘ç°å¾…æ£€æŸ¥çš„ .txt æ–‡æœ¬ï¼Œè·³è¿‡ä¼ä¸šæ ‡å‡†æ¯”å¯¹ã€‚")
						else:
							# --- Checkpoint preparation: generate prompts for all chunks and manifest ---
							try:
								checkpoint_dir = os.path.join(enterprise_out_root, 'checkpoint')
								os.makedirs(checkpoint_dir, exist_ok=True)
								# If previous manifest exists and all entries are done, clear checkpoint files
								try:
									import json as _json
									_manifest_path = os.path.join(checkpoint_dir, 'manifest.json')
									if os.path.isfile(_manifest_path):
										with open(_manifest_path, 'r', encoding='utf-8') as _mf:
											_prev_manifest = _json.load(_mf) or {}
										_entries_prev = _prev_manifest.get('entries') or []
										_all_done = bool(_entries_prev) and all((str(e.get('status')) == 'done') for e in _entries_prev)
										if _all_done:
											for _fn in os.listdir(checkpoint_dir):
												_fp = os.path.join(checkpoint_dir, _fn)
												try:
													if os.path.isfile(_fp):
														os.remove(_fp)
												except Exception:
													pass
								except Exception:
									pass
								# Build run_id based on current examined_txt files (name|size|mtime)
								try:
									import hashlib
									infos = []
									for _n in sorted(exam_txt_files, key=lambda x: x.lower()):
										_pp = os.path.join(examined_txt_dir, _n)
										try:
											_st = os.stat(_pp)
											infos.append(f"{_n}|{_st.st_size}|{int(_st.st_mtime)}")
										except Exception:
											infos.append(f"{_n}|0|0")
									run_id = hashlib.sha1("\n".join(infos).encode('utf-8', errors='ignore')).hexdigest()
								except Exception:
									run_id = ""
								manifest = {"run_id": run_id, "entries": []}
								# The same prompt prefix used later in streaming phase
								prompt_prefix = (
									"è¯·ä½œä¸ºä¼ä¸šæ ‡å‡†ç¬¦åˆæ€§æ£€æŸ¥ä¸“å®¶ï¼Œå®¡é˜…å¾…æ£€æŸ¥æ–‡ä»¶ä¸ä¼ä¸šæ ‡å‡†æ˜¯å¦ä¸€è‡´ã€‚"
									"ä»¥åˆ—è¡¨å½¢å¼åˆ—å‡ºä¸ä¸€è‡´çš„ç‚¹ï¼Œå¹¶å¼•ç”¨åŸæ–‡è¯æ®ï¼ˆç®€çŸ­æ‘˜å½•ï¼‰ã€æ ‡æ˜å‡ºå¤„ï¼ˆæä¾›ä¼ä¸šæ ‡å‡†æ–‡ä»¶çš„æ–‡ä»¶åï¼‰ã€‚\n"
									"è¾“å‡ºçš„å†…å®¹è¦è¨€ç®€æ„èµ…ï¼Œåˆ—å‡ºä¸ä¸€è‡´çš„ç‚¹å³å¯ï¼Œä¸éœ€è¦åˆ—å‡ºä¸€è‡´çš„ç‚¹ï¼Œä¹Ÿä¸éœ€è¦åˆ—å‡ºä¼ä¸šæ ‡å‡†ä¸­ç¼ºå¤±çš„ç‚¹ï¼Œæœ€åä¸éœ€è¦æ€»ç»“ã€‚\n"
									"ç”±äºå¾…æ£€æŸ¥æ–‡ä»¶è¾ƒé•¿ï¼Œæˆ‘å°†åˆ†æˆå¤šä¸ªéƒ¨åˆ†å°†å…¶ä¸Šä¼ ç»™ä½ ã€‚ä»¥ä¸‹æ˜¯å¾…æ£€æŸ¥æ–‡ä»¶çš„ä¸€éƒ¨åˆ†ã€‚\n"
								)
								entry_id = 0
								for _fname in sorted(exam_txt_files, key=lambda x: x.lower()):
									_src = os.path.join(examined_txt_dir, _fname)
									try:
										with open(_src, 'r', encoding='utf-8') as _f:
											_doc_text = _f.read()
									except Exception:
										_doc_text = ""
									_chunks = split_to_chunks(_doc_text, int(BISHENG_MAX_WORDS))
									for _ci, _piece in enumerate(_chunks):
										entry_id += 1
										_prompt_text = f"{prompt_prefix}{_piece}"
										# Use 1-based numbering per chunk within file for filenames, new convention
										_num = _ci + 1
										_prompt_name = f"checkpoint_prompt_{_fname}_pt{_num}.txt"
										_resp_name = f"checkpoint_response_{_fname}_pt{_num}.txt"
										_prompt_path = os.path.join(checkpoint_dir, _prompt_name)
										_response_path = os.path.join(checkpoint_dir, _resp_name)
										try:
											with open(_prompt_path, 'w', encoding='utf-8') as _pf:
												_pf.write(_prompt_text)
										except Exception:
											pass
											manifest["entries"].append({
											"id": entry_id,
											"file_name": _fname,
											"chunk_index": _ci,
											"prompt_file": _prompt_name,
											"response_file": _resp_name,
											"status": "not_done"
										})
								# Write manifest.json atomically (paths stored as relative filenames for portability)
								try:
									import json as _json, tempfile as _tmp, shutil as _sh
									_manifest_path = os.path.join(checkpoint_dir, 'manifest.json')
									with _tmp.NamedTemporaryFile('w', delete=False, encoding='utf-8', dir=checkpoint_dir) as _tf:
										_tf.write(_json.dumps(manifest, ensure_ascii=False, indent=2))
										_tmpname = _tf.name
									_sh.move(_tmpname, _manifest_path)
								except Exception:
									pass
							except Exception:
								pass

							st.session_state[f"enterprise_continue_running_{session_id}"] = False
							st.session_state[f"enterprise_running_{session_id}"] = True
							st.session_state[f"enterprise_std_txt_files_{session_id}"] = std_txt_files
							st.session_state[f"enterprise_exam_txt_files_{session_id}"] = exam_txt_files
							st.session_state[f"enterprise_out_root_{session_id}"] = enterprise_out_root
							st.session_state[f"enterprise_standards_txt_dir_{session_id}"] = standards_txt_dir
							st.session_state[f"enterprise_examined_txt_dir_{session_id}"] = examined_txt_dir
							st.rerun()
					except Exception as e:
						st.error(f"ä¼ä¸šæ ‡å‡†æ¯”å¯¹æµç¨‹å¼‚å¸¸ï¼š{e}")
					
		with btn_col_stop:
			if st.button("åœæ­¢", key=f"enterprise_stop_button_{session_id}"):
				st.session_state[f"enterprise_running_{session_id}"] = False
				try:
					# Load current bisheng session id if any
					bs_key = f"bisheng_session_{session_id}"
					bisheng_sid = st.session_state.get(bs_key)
					bs_cfg = CONFIG.get('bisheng', {})
					base_url = (st.session_state.get(f"bisheng_{session_id}_base_url")
								or os.getenv('BISHENG_BASE_URL') or bs_cfg.get('base_url') or 'http://10.31.60.11:3001')
					stop_path = (st.session_state.get(f"bisheng_{session_id}_stop_path")
								or os.getenv('BISHENG_STOP_PATH') or bs_cfg.get('stop_path') or '/api/v2/workflow/stop')
					api_key = (st.session_state.get(f"bisheng_{session_id}_api_key")
								or os.getenv('BISHENG_API_KEY') or bs_cfg.get('api_key') or '')
					if not bisheng_sid:
						st.info("å½“å‰æ— æ´»åŠ¨ä¼šè¯å¯åœæ­¢ã€‚")
					else:
						res = stop_workflow(base_url, stop_path, bisheng_sid, api_key or None)
						st.success(f"å·²è¯·æ±‚åœæ­¢ï¼Œå“åº”ï¼š{res}")
				except Exception as e:
					st.error(f"åœæ­¢å¤±è´¥ï¼š{e}")

			# Continue æŒ‰é’®ï¼šå§‹ç»ˆå¯è§ã€‚è‹¥æ— å¯ç»­æ¡ç›®ï¼Œæç¤ºä¿¡æ¯ï¼›å¦åˆ™åˆ‡æ¢åˆ°è¿è¡Œåˆ†æ”¯æ¶ˆè´¹ checkpointã€‚
			if st.button("ç»§ç»­", key=f"enterprise_continue_button_{session_id}"):
				if st.session_state.get(f"enterprise_running_{session_id}"):
					st.info("å½“å‰æµç¨‹æ­£åœ¨è¿è¡Œï¼Œè¯·å…ˆåœæ­¢å†ç»§ç»­ã€‚")
				else:
					checkpoint_dir = os.path.join(enterprise_out_root, 'checkpoint')
					manifest_path = os.path.join(checkpoint_dir, 'manifest.json')
					_has_entries = False
					try:
						import json as _json
						if os.path.isfile(manifest_path):
							with open(manifest_path, 'r', encoding='utf-8') as _mf:
								_m = _json.load(_mf) or {}
								_ents = _m.get('entries') or []
								_has_entries = len(_ents) > 0
					except Exception:
						_has_entries = False
					if not _has_entries:
						st.info("æœªå‘ç°è·‘åˆ°ä¸€åŠçš„é¡¹ç›®")
					else:
						st.session_state[f"enterprise_running_{session_id}"] = False
						st.session_state[f"enterprise_continue_running_{session_id}"] = True
						st.rerun()

		with btn_col2:
			if st.button("æ¼”ç¤º", key=f"enterprise_demo_button_{session_id}"):
				# Copy demonstration files into the user's enterprise folders (no processing here)
				try:
					import shutil
					# Locate demonstration root (same convention as other tabs)
					demo_base_dir = CONFIG["directories"]["cp_files"].parent / "demonstration"
					demo_enterprise = os.path.join(str(demo_base_dir), "enterprise_standard_files")
					# Subfolders to copy from â†’ to
					pairs = [
						(os.path.join(demo_enterprise, "standards"), standards_dir),
						(os.path.join(demo_enterprise, "examined_files"), examined_dir),
						# New: copy demonstration prompt/response chunks into session enterprise output
						# Entire folders copied under enterprise_out_root
						(os.path.join(demo_enterprise, "prompt_text_chunks"), os.path.join(enterprise_out_root, "prompt_text_chunks")),
						(os.path.join(demo_enterprise, "llm responses"), os.path.join(enterprise_out_root, "llm responses")),
						# New: copy final_results for demo summary
						(os.path.join(demo_enterprise, "final_results"), os.path.join(enterprise_out_root, "final_results")),
						# New: copy pre-made prompted responses and json outputs for demo
						(os.path.join(demo_enterprise, "prompted_llm responses_and_json"), os.path.join(enterprise_out_root, "prompted_llm responses_and_json")),
					]
					files_copied = 0
					for src, dst in pairs:
						if not os.path.exists(src):
							continue
					# If source is a directory that we want to mirror (prompt_text_chunks / llm responses / final_results / prompted_llm responses_and_json)
						if os.path.isdir(src) and (src.endswith("prompt_text_chunks") or src.endswith("llm responses") or src.endswith("final_results") or src.endswith("prompted_llm responses_and_json")):
							os.makedirs(os.path.dirname(dst), exist_ok=True)
							# Copy whole directory tree into enterprise_out_root subfolder
							shutil.copytree(src, dst, dirs_exist_ok=True)
							for root, _, files in os.walk(src):
								files_copied += len([f for f in files if os.path.isfile(os.path.join(root, f))])
							continue
						# Otherwise treat as file list copy (standards / examined_files)
						for name in os.listdir(src):
							src_path = os.path.join(src, name)
							dst_path = os.path.join(dst, name)
							if os.path.isfile(src_path):
								os.makedirs(dst, exist_ok=True)
								shutil.copy2(src_path, dst_path)
								files_copied += 1
					# Trigger demo streaming phase
					st.session_state[f"enterprise_demo_{session_id}"] = True
					st.success(f"å·²å¤åˆ¶æ¼”ç¤ºæ–‡ä»¶ï¼š{files_copied} ä¸ªï¼Œå¼€å§‹æ¼”ç¤ºâ€¦")
				except Exception as e:
					st.error(f"æ¼”ç¤ºæ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")
				# Immediately rerun to render the demo streaming phase in main column
				st.rerun()

		# Render streaming phase in main column after rerun (mirrors special_symbols pattern)
		if st.session_state.get(f"enterprise_running_{session_id}"):
			# Retrieve context saved before rerun
			std_txt_files = st.session_state.get(f"enterprise_std_txt_files_{session_id}") or []
			exam_txt_files = st.session_state.get(f"enterprise_exam_txt_files_{session_id}") or []
			enterprise_out = st.session_state.get(f"enterprise_out_root_{session_id}") or enterprise_out_root
			std_txt_dir = st.session_state.get(f"enterprise_standards_txt_dir_{session_id}") or standards_txt_dir
			exam_txt_dir = st.session_state.get(f"enterprise_examined_txt_dir_{session_id}") or examined_txt_dir

			# Upload standards once (optional)
			if std_txt_files:
				with st.status("Sync standards to KB...", expanded=False) as status:
					try:
						kb_name_dyn = f"{session_id}_{TAB_SLUG}"
						kid = find_knowledge_id_by_name(BISHENG_BASE_URL, BISHENG_API_KEY or None, kb_name_dyn)
						if not kid:
							kid = create_knowledge(BISHENG_BASE_URL, BISHENG_API_KEY or None, kb_name_dyn, model=str(KB_MODEL_ID))
						if kid:
							res = kb_sync_folder(
								base_url=BISHENG_BASE_URL,
								api_key=BISHENG_API_KEY or None,
								knowledge_id=int(kid),
								folder_path=std_txt_dir,
								clear_first=False,
								chunk_size=1000,
								chunk_overlap=0,
								separators=["\n\n", "\n"],
								separator_rule=["after", "after"],
							)
							status.update(label=f"KB sync: uploaded {len(res.get('uploaded', []))}, deleted {len(res.get('deleted', []))}, skipped {len(res.get('skipped', []))}", state="complete")
						else:
							status.update(label="KB create/lookup failed (check server auth)", state="error")
					except Exception as e:
						status.update(label=f"KB sync failed: {e}", state="error")

			# Iterate examined texts
			exam_txt_files.sort(key=lambda x: x.lower())
			bisheng_session_id = st.session_state.get(f"bisheng_session_{session_id}")
			initial_dir = os.path.join(enterprise_out, 'initial_results')
			os.makedirs(initial_dir, exist_ok=True)
			# é¢„çƒ­æ­¥éª¤ï¼šåœ¨å¹¶å‘å¼€å§‹å‰ï¼Œä¸²è¡Œå¯¹ Bisheng Flow å‘èµ·ä¸€æ¬¡æçŸ­è¯·æ±‚ï¼Œä¿ƒä½¿æ£€ç´¢/LLM åˆå§‹åŒ–ä¸ç¼“å­˜
			# è¯´æ˜ï¼šé¦–æ¬¡è¯·æ±‚å¸¸è§çš„å†·å¯åŠ¨ï¼ˆæ¨¡å‹åŠ è½½ã€è¿æ¥æ± ã€æ£€ç´¢ç´¢å¼•å”¤é†’ï¼‰ä¼šå¯¼è‡´é¦–æ‰¹å¹¶å‘è¯·æ±‚å¤±è´¥ç‡ä¸Šå‡ï¼›
			# é€šè¿‡ä¸€æ¬¡è½»é‡çš„é¢„çƒ­ï¼Œå¯ä»¥æ˜¾è‘—é™ä½â€œç¬¬ä¸€æ‰¹å…¨æŒ‚â€çš„æ¦‚ç‡ã€‚è¿”å›å†…å®¹æ— éœ€ä½¿ç”¨ã€‚
			try:
				if not st.session_state.get(f"enterprise_warmup_done_{session_id}"):
					warmup_prompt = "é¢„çƒ­ï¼šè¯·ç®€çŸ­å›å¤ 'gotcha' å³å¯ã€‚"
					_ = call_flow_process(
						base_url=BISHENG_BASE_URL,
						flow_id=BISHENG_FLOW_ID,
						question=warmup_prompt,
						kb_id=None,
						input_node_id=FLOW_INPUT_NODE_ID,
						api_key=BISHENG_API_KEY or None,
						session_id=None,
						history_count=0,
						extra_tweaks={"CombineDocsChain-520ca": {"token_max": 5000}},
						milvus_node_id=FLOW_MILVUS_NODE_ID,
						es_node_id=FLOW_ES_NODE_ID,
						timeout_s=60,
						max_retries=0,
						# clear_cache=True,
					)
					st.session_state[f"enterprise_warmup_done_{session_id}"] = True
			except Exception:
				pass
			# Start fresh: original per-file splitting and live prompting
			for idx_file, name in enumerate(exam_txt_files, start=1):
				src_path = os.path.join(exam_txt_dir, name)
				st.markdown(f"**ğŸ“„ æ­£åœ¨æ¯”å¯¹ç¬¬{idx_file}ä¸ªæ–‡ä»¶ï¼Œå…±{len(exam_txt_files)}ä¸ªï¼š{name}**")
				try:
					with open(src_path, 'r', encoding='utf-8') as f:
						doc_text = f.read()
				except Exception as e:
					st.error(f"è¯»å–å¤±è´¥ï¼š{e}")
					continue
				if not doc_text.strip():
					st.info("æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
					continue
				chunks = split_to_chunks(doc_text, int(BISHENG_MAX_WORDS))
				prompt_prefix = (
					"è¯·ä½œä¸ºä¼ä¸šæ ‡å‡†ç¬¦åˆæ€§æ£€æŸ¥ä¸“å®¶ï¼Œå®¡é˜…å¾…æ£€æŸ¥æ–‡ä»¶ä¸ä¼ä¸šæ ‡å‡†æ˜¯å¦ä¸€è‡´ã€‚"
					"ä»¥åˆ—è¡¨å½¢å¼åˆ—å‡ºä¸ä¸€è‡´çš„ç‚¹ï¼Œå¹¶å¼•ç”¨åŸæ–‡è¯æ®ï¼ˆç®€çŸ­æ‘˜å½•ï¼‰ã€æ ‡æ˜å‡ºå¤„ï¼ˆæä¾›ä¼ä¸šæ ‡å‡†æ–‡ä»¶çš„æ–‡ä»¶åï¼‰ã€‚\n"
					"è¾“å‡ºçš„å†…å®¹è¦è¨€ç®€æ„èµ…ï¼Œåˆ—å‡ºä¸ä¸€è‡´çš„ç‚¹å³å¯ï¼Œä¸éœ€è¦åˆ—å‡ºä¸€è‡´çš„ç‚¹ï¼Œä¹Ÿä¸éœ€è¦åˆ—å‡ºä¼ä¸šæ ‡å‡†ä¸­ç¼ºå¤±çš„ç‚¹ï¼Œæœ€åä¸éœ€è¦æ€»ç»“ã€‚\n"
					"ç”±äºå¾…æ£€æŸ¥æ–‡ä»¶è¾ƒé•¿ï¼Œæˆ‘å°†åˆ†æˆå¤šä¸ªéƒ¨åˆ†å°†å…¶ä¸Šä¼ ç»™ä½ ã€‚ä»¥ä¸‹æ˜¯å¾…æ£€æŸ¥æ–‡ä»¶çš„ä¸€éƒ¨åˆ†ã€‚\n"
				)
				full_out_text = ""
				prompt_texts = []
				for i, piece in enumerate(chunks, start=1):
					col_prompt, col_response = st.columns([1, 1])
					prompt_text = f"{prompt_prefix}{piece}"
					prompt_texts.append(prompt_text)
					with col_prompt:
						st.markdown(f"æç¤ºè¯ï¼ˆç¬¬{i}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ï¼‰")
						prompt_container = st.container(height=400)
						with prompt_container:
							with st.chat_message("user"):
								prompt_placeholder = st.empty()
								words = prompt_text.split()
								streamed = ""
								for j in range(0, len(words), 30):
									chunk_words = words[j:j+30]
									streamed += " ".join(chunk_words) + " "
									prompt_placeholder.text(streamed.strip())
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_prompt_{session_id}_{idx_file}_{i}")
					with col_response:
						st.markdown(f"AIæ¯”å¯¹ç»“æœï¼ˆç¬¬{i}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ï¼‰")
						response_container = st.container(height=400)
						with response_container:
							with st.chat_message("assistant"):
								response_placeholder = st.empty()
								try:
									# Determine per-user KB name and call Flow with tweaks for per-run KB binding
									kb_name_dyn = f"{session_id}_{TAB_SLUG}"
									kid = find_knowledge_id_by_name(BISHENG_BASE_URL, BISHENG_API_KEY or None, kb_name_dyn)
									start_ts = time.time()
									res = call_flow_process(
										base_url=BISHENG_BASE_URL,
										flow_id=BISHENG_FLOW_ID,
										question=prompt_text,
										kb_id=kid,
										input_node_id=FLOW_INPUT_NODE_ID,
										api_key=BISHENG_API_KEY or None,
										session_id=bisheng_session_id,
										history_count=0,
										extra_tweaks=None,
										milvus_node_id=FLOW_MILVUS_NODE_ID,
										es_node_id=FLOW_ES_NODE_ID,
										timeout_s=180,        # æ–°å¢
										max_retries=2,        # æ–°å¢
										# clear_cache=True,
										)
									ans_text, new_sid = parse_flow_answer(res)
									dur_ms = int((time.time() - start_ts) * 1000)
									try:
										from datetime import datetime as _dt
										_log_llm_metrics(
											enterprise_out,
											session_id,
											{
												"ts": _dt.now().isoformat(timespec="seconds"),
												"engine": "bisheng",
												"model": "qwen3",
												"session_id": bisheng_session_id or "",
												"file": name,
												"part": i,
												"phase": "compare",
												"prompt_chars": len(prompt_text or ""),
												"prompt_tokens": _estimate_tokens(prompt_text or ""),
												"output_chars": len(ans_text or ""),
												"output_tokens": _estimate_tokens(ans_text or ""),
												"duration_ms": dur_ms,
												"success": 1 if (ans_text or "").strip() else 0,
												"error": res.get("error") if isinstance(res, dict) else "",
											},
										)
									except Exception:
										pass
									if new_sid:
										bisheng_session_id = new_sid
										st.session_state[f"bisheng_session_{session_id}"] = bisheng_session_id
									response_placeholder.write(ans_text or "")
									full_out_text += ("\n\n" if full_out_text else "") + (ans_text or "")
								except Exception as e:
									response_placeholder.error(f"è°ƒç”¨å¤±è´¥ï¼š{e}")
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_response_{session_id}_{idx_file}_{i}")
				# Persist per-file combined output
				try:
					name_no_ext = os.path.splitext(name)[0]
					# Also persist all prompt parts as a single file: prompt_{name_no_ext}.txt
					try:
						total_parts = len(prompt_texts)
						prompt_out_lines = []
						for idx_p, ptxt in enumerate(prompt_texts, start=1):
							prompt_out_lines.append(f"æç¤ºè¯ï¼ˆç¬¬{idx_p}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰ï¼š")
							prompt_out_lines.append(ptxt)
						prompt_out_text = "\n".join(prompt_out_lines)
						prompt_out_path = os.path.join(initial_dir, f"prompt_{name_no_ext}.txt")
						with open(prompt_out_path, 'w', encoding='utf-8') as pf:
							pf.write(prompt_out_text)
					except Exception:
						pass
					out_path = os.path.join(initial_dir, f"response_{name_no_ext}.txt")
					with open(out_path, 'w', encoding='utf-8') as outf:
						outf.write(full_out_text)
					# Immediately produce chunked prompted_response_*_ptN.txt files
					try:
						original_name = name_no_ext
						prompt_lines = [
							"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç»“æ„åŒ–ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚ä»¥ä¸‹å†…å®¹æ¥è‡ªä¸€ä¸ªåŸºäº RAG çš„å¤§è¯­è¨€æ¨¡å‹ (RAG-LLM) ç³»ç»Ÿï¼Œ",
							"ç”¨äºå°†è‹¥å¹²æŠ€æœ¯æ–‡ä»¶ä¸ä¼ä¸šæ ‡å‡†è¿›è¡Œé€æ¡æ¯”å¯¹ï¼Œè¾“å‡ºå‘ç°çš„ä¸ç¬¦åˆé¡¹åŠå…¶ç†ç”±ã€‚æœ¬æ–‡ä»¶å¯¹åº”çš„åŸå§‹æŠ€æœ¯æ–‡ä»¶åç§°ä¸ºï¼š",
							f"{original_name}ã€‚",
							"\nè¯·å°†éšåçš„å…¨æ–‡å†…å®¹è½¬æ¢ä¸º JSON æ•°ç»„ï¼ˆlist of objectsï¼‰ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«å¦‚ä¸‹äº”ä¸ªé”®ï¼š",
							f"- æŠ€æœ¯æ–‡ä»¶åï¼š\"{original_name}\"",
							"- æŠ€æœ¯æ–‡ä»¶å†…å®¹ï¼šä¸è¯¥æ¡ä¸ä¸€è‡´ç‚¹ç›¸å…³çš„æŠ€æœ¯æ–‡ä»¶æ¡ç›®æˆ–æ®µè½åç§°/ç¼–å·/æ‘˜è¦",
							"- ä¼ä¸šæ ‡å‡†ï¼šè¢«å¯¹æ¯”çš„ä¼ä¸šæ ‡å‡†æ¡æ¬¾åç§°/ç¼–å·/æ‘˜è¦",
							"- ä¸ä¸€è‡´ä¹‹å¤„ï¼šä¸ç¬¦åˆæˆ–å­˜åœ¨å·®å¼‚çš„å…·ä½“ç‚¹ï¼Œå°½é‡å…·ä½“æ˜ç¡®",
							"- ç†ç”±ï¼šåˆ¤æ–­ä¸ä¸€è‡´çš„ä¾æ®ä¸ç®€è¦è§£é‡Šï¼ˆä¿æŒå®¢è§‚ã€å¯è¿½æº¯ï¼‰",
							"\nè¦æ±‚ï¼š",
							"1) ä»…è¾“å‡ºä¸¥æ ¼çš„ JSONï¼ˆUTF-8ï¼Œæ— æ³¨é‡Šã€æ— å¤šä½™æ–‡æœ¬ï¼‰ï¼Œé”®åä½¿ç”¨ä¸Šè¿°ä¸­æ–‡ï¼›",
							"2) è‹¥å†…å®¹åŒ…å«å¤šå¤„å¯¹æ¯”ï¼ŒæŒ‰æ¡ç›®æ‹†åˆ†ä¸ºå¤šæ¡ JSON å¯¹è±¡ï¼›",
							"3) è‹¥æŸå¤„ä¿¡æ¯ç¼ºå¤±ï¼Œè¯·ä»¥ç©ºå­—ç¬¦ä¸² \"\" å ä½ï¼Œä¸è¦ç¼–é€ ï¼›",
							"4) å°½é‡ä¿ç•™å¯ç”¨äºè¿½æº¯å®šä½çš„åŸæ–‡çº¿ç´¢ï¼ˆå¦‚ç¼–å·ã€æ ‡é¢˜ã€é¡µç ç­‰ï¼‰äºç›¸åº”å­—æ®µä¸­ã€‚",
							"\nä¸‹é¢æ˜¯éœ€è¦è½¬æ¢ä¸º JSON çš„åŸå§‹æ¯”å¯¹è¾“å‡ºï¼š\n\n",
						]
						instr = "".join(prompt_lines)
						# Split content by every 20 occurrences of "<think>" markers
						text = full_out_text or ""
						# æ›´ç¨³å¥çš„åˆ‡åˆ†ï¼šåŒæ—¶é™åˆ¶æ¯æ®µæœ€å¤šåŒ…å«çš„ <think> æ•°é‡ä¸æœ€å¤§å­—ç¬¦æ•°ï¼Œé¿å…å•æ®µè¿‡å¤§
						max_thinks_per_part = 20
						max_chars_per_part = 8000
						think_indices = []
						needle = "<think>"
						start = 0
						while True:
							pos = text.find(needle, start)
							if pos == -1:
								break
							think_indices.append(pos)
							start = pos + len(needle)
						boundaries = []
						prev = 0
						count_since_prev = 0
						for pos in think_indices:
							count_since_prev += 1
							if count_since_prev >= max_thinks_per_part or (pos - prev) >= max_chars_per_part:
								boundaries.append(pos)
								prev = pos
								count_since_prev = 0
						parts = []
						prev = 0
						for b in boundaries:
							parts.append(text[prev:b])
							prev = b
						parts.append(text[prev:])
						# Write each part to prompted_response_<name>_ptN.txt after removing <think> blocks
						for idx_part, part in enumerate(parts, start=1):
							cleaned_part = re.sub(r"<think>[\s\S]*?</think>", "", part)
							prompted_path = os.path.join(initial_dir, f"prompted_response_{name_no_ext}_pt{idx_part}.txt")
							with open(prompted_path, 'w', encoding='utf-8') as pf:
								pf.write(instr)
								pf.write(cleaned_part)
					except Exception:
						pass

					# For each prompted_response_* part, call Ollama LLM and display side-by-side
					try:
						# Discover all parts for current file
						part_files = []
						try:
							for fn in os.listdir(initial_dir):
								if fn.startswith(f"prompted_response_{name_no_ext}_pt") and fn.endswith('.txt'):
									part_files.append(fn)
						except Exception:
							part_files = []
						part_files.sort(key=lambda x: x.lower())
						total_parts = len(part_files)
						# Prepare LLM client (Ollama on 10.31.60.9 by default)
						host = resolve_ollama_host("ollama_9")
						ollama_client = OllamaClient(host=host)
						model = CONFIG["llm"].get("ollama_model")
						temperature = 0.7
						top_p = 0.9
						top_k = 40
						repeat_penalty = 1.1
						num_ctx = 40001
						num_thread = 4
						for part_idx, pfname in enumerate(part_files, start=1):
							p_path = os.path.join(initial_dir, pfname)
							try:
								with open(p_path, 'r', encoding='utf-8') as fpr:
									prompt_text_all = fpr.read()
							except Exception:
								prompt_text_all = ""
							# Two-column display for this prompt-response pair
							col_prompt2, col_resp2 = st.columns([1, 1])
							with col_prompt2:
								st.markdown(f"ç”Ÿæˆæ±‡æ€»è¡¨æ ¼æç¤ºè¯ï¼ˆç¬¬{part_idx}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰")
								prompt_container2 = st.container(height=400)
								with prompt_container2:
									with st.chat_message("user"):
										ph2 = st.empty()
										# Stream display the prompt text in chunks of words
										words = (prompt_text_all or "").split()
										streamed2 = ""
										for idxw in range(0, len(words), 30):
											chunk_words = words[idxw:idxw+30]
											streamed2 += " ".join(chunk_words) + " "
											ph2.text(streamed2.strip())
									st.chat_input(placeholder="", disabled=True, key=f"enterprise_prompted_prompt_{session_id}_{pfname}")
							with col_resp2:
								st.markdown(f"ç”Ÿæˆæ±‡æ€»è¡¨æ ¼ç»“æœï¼ˆç¬¬{part_idx}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰")
								resp_container2 = st.container(height=400)
								with resp_container2:
									with st.chat_message("assistant"):
										ph_resp2 = st.empty()
										response_text = ""
										start_ts = time.time()
										last_stats = None
										error_msg = ""
										chunks_seen = 0
										try:
											for chunk in ollama_client.chat(
												model=model,
												messages=[{"role": "user", "content": prompt_text_all}],
												stream=True,
												options={ "temperature": temperature, "top_p": top_p, "top_k": top_k,
												          "repeat_penalty": repeat_penalty, "num_ctx": num_ctx, "num_thread": num_thread }
											):
												chunks_seen += 1
												new_text = chunk.get('message', {}).get('content', '')
												response_text += new_text
												ph_resp2.write(response_text)
												# æŸäº›å¼•æ“ä¼šåœ¨æœ€åä¸€ä¸ªåˆ†å—é‡Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
												last_stats = chunk.get('eval_info') or chunk.get('stats') or last_stats
										except Exception as e:
											error_msg = str(e)[:300]
										finally:
											from datetime import datetime as _dt
											dur_ms = int((time.time() - start_ts) * 1000)
											prompt_tokens = (last_stats or {}).get('prompt_eval_count')
											output_tokens = (last_stats or {}).get('eval_count')
											if not error_msg and chunks_seen == 0:
												error_msg = "no_stream_chunks"
											_log_llm_metrics(
												enterprise_out, session_id,
												{
													"ts": _dt.now().isoformat(timespec="seconds"),
													"engine": "ollama",
													"model": model,
													"session_id": "",
													"file": name_no_ext,
													"part": part_idx,
													"phase": "summarize",
													"prompt_chars": len(prompt_text_all or ""),
													"prompt_tokens": prompt_tokens if isinstance(prompt_tokens, int) else _estimate_tokens(prompt_text_all or ""),
													"output_chars": len(response_text or ""),
													"output_tokens": output_tokens if isinstance(output_tokens, int) else _estimate_tokens(response_text or ""),
													"duration_ms": dur_ms,
													"success": 1 if (response_text or "").strip() else 0,
													"error": "error_msg"
												}
											)

										# Save LLM response to json_<original>_ptN.txt
										try:
											# Map prompted_response_X_ptN.txt -> json_X_ptN.txt
											base = pfname[len("prompted_response_"):]
											json_name = f"json_{base}"
											json_path = os.path.join(initial_dir, json_name)
											with open(json_path, 'w', encoding='utf-8') as jf:
												jf.write(response_text)
										except Exception:
											pass
					except Exception:
						pass
				except Exception as e:
					st.error(f"ä¿å­˜ç»“æœå¤±è´¥ï¼š{e}")

			# End of current run; clear running flag (no final aggregation here)
			try:
				st.session_state[f"enterprise_running_{session_id}"] = False
			except Exception as e:
				st.error(f"æµç¨‹æ”¶å°¾å¤±è´¥ï¼š{e}")

			# After LLM step: aggregate json_*_ptN.txt into CSV and XLSX in final_results
			try:
				final_dir = os.path.join(enterprise_out, 'final_results')
				os.makedirs(final_dir, exist_ok=True)
				# Collect json_*.txt under initial_results
				json_files = []
				try:
					for fn in os.listdir(initial_dir):
						if fn.startswith('json_') and fn.lower().endswith('.txt'):
							json_files.append(os.path.join(initial_dir, fn))
				except Exception:
					json_files = []
				# Build rows
				columns = ["æŠ€æœ¯æ–‡ä»¶å", "æŠ€æœ¯æ–‡ä»¶å†…å®¹", "ä¼ä¸šæ ‡å‡†", "ä¸ä¸€è‡´ä¹‹å¤„", "ç†ç”±"]
				rows = []
				import csv
				import pandas as pd
				def _strip_code_fences(s: str) -> str:
					s = (s or "").strip()
					if s.startswith("```") and s.endswith("```"):
						s = s[3:-3].strip()
						if s.lower().startswith("json"):
							s = s[4:].strip()
					return s
				def _extract_json_text(text: str) -> str:
					s = (text or "").strip().lstrip("\ufeff")
					s = _strip_code_fences(s)
					# try direct
					try:
						json.loads(s)
						return s
					except Exception:
						pass
					# try array
					try:
						start = s.find("[")
						end = s.rfind("]")
						if start != -1 and end != -1 and end > start:
							candidate = s[start:end+1]
							try:
								json.loads(candidate)
								return candidate
							except Exception:
								merged = re.sub(r"\]\s*,?\s*\[", ",", candidate)
								json.loads(merged)
								return merged
					except Exception:
						pass
					# try object
					try:
						start = s.find("{")
						end = s.rfind("}")
						if start != -1 and end != -1 and end > start:
							candidate = s[start:end+1]
							json.loads(candidate)
							return candidate
					except Exception:
						pass
					return s
				for jf in json_files:
					try:
						with open(jf, 'r', encoding='utf-8') as f:
							text = f.read()
					except Exception:
						text = ""
					js = _extract_json_text(text)
					try:
						parsed = json.loads(js)
					except Exception:
						parsed = None
					items = []
					if isinstance(parsed, list):
						items = [x for x in parsed if isinstance(x, dict)]
					elif isinstance(parsed, dict):
						vals = list(parsed.values())
						for v in vals:
							if isinstance(v, list) and all(isinstance(i, dict) for i in v):
								items = v
								break
						if not items:
							items = [parsed]
					# Derive original source filename from json_*.txt â†’ strip 'json_' and trailing _ptN.txt
					base_name = os.path.basename(jf)
					orig_name = base_name[5:] if base_name.startswith('json_') else base_name
					try:
						orig_name = re.sub(r"_pt\d+\.txt$", "", orig_name)
					except Exception:
						pass
					for obj in items:
						row = [
							orig_name,
							str(obj.get("æŠ€æœ¯æ–‡ä»¶å†…å®¹", "")),
							str(obj.get("ä¼ä¸šæ ‡å‡†", "")),
							str(obj.get("ä¸ä¸€è‡´ä¹‹å¤„", "")),
							str(obj.get("ç†ç”±", "")),
						]
						rows.append(row)
				# Write CSV
				from datetime import datetime as _dt
				ts = _dt.now().strftime('%Y%m%d_%H%M%S')
				csv_path = os.path.join(final_dir, f"ä¼æ ‡æ£€æŸ¥ç»“æœ_{ts}.csv")
				with open(csv_path, 'w', encoding='utf-8-sig', newline='') as cf:
					writer = csv.writer(cf)
					writer.writerow(columns)
					for r in rows:
						writer.writerow(r)
				# Write XLSX
				xlsx_path = os.path.join(final_dir, f"ä¼æ ‡æ£€æŸ¥ç»“æœ_{ts}.xlsx")
				try:
					df = pd.DataFrame(rows, columns=columns)
					df.to_excel(xlsx_path, index=False, engine='openpyxl')
				except Exception:
					pass
				# Offer downloads
				try:
					with open(csv_path, 'rb') as fcsv:
						st.download_button(label="ä¸‹è½½CSVç»“æœ", data=fcsv.read(), file_name=os.path.basename(csv_path), mime='text/csv', key=f"download_csv_{session_id}")
					with open(xlsx_path, 'rb') as fxlsx:
						st.download_button(label="ä¸‹è½½Excelç»“æœ", data=fxlsx.read(), file_name=os.path.basename(xlsx_path), mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', key=f"download_xlsx_{session_id}")
				except Exception:
					pass
				# Build Word document of analysis process from prompt_*.txt and response_*.txt
				try:
					try:
						from docx import Document
					except Exception:
						Document = None
					if Document is not None:
						# Pair prompt_/response_ files by base name
						pairs = []
						prompt_files = [f for f in os.listdir(initial_dir) if f.startswith('prompt_') and f.lower().endswith('.txt')]
						for pf in prompt_files:
							base = pf[len('prompt_'):-4]
							rf = os.path.join(initial_dir, f"response_{base}.txt")
							if os.path.isfile(rf):
								pairs.append((os.path.join(initial_dir, pf), rf, base))
						pairs.sort(key=lambda x: x[2].lower())
						if pairs:
							doc = Document()
							# Set default fonts to å®‹ä½“ where possible
							try:
								styles = doc.styles
								styles['Normal'].font.name = 'å®‹ä½“'
								# Heading styles
								styles['Heading 1'].font.name = 'å®‹ä½“'
								styles['Heading 2'].font.name = 'å®‹ä½“'
							except Exception:
								pass
							# Main title
							doc.add_heading("ä¼æ ‡æ£€æŸ¥å…¨éƒ¨åˆ†æè¿‡ç¨‹", level=0)
							# Simple TOC section (static list)
							doc.add_heading("ç›®å½•", level=1)
							for _, _, base in pairs:
								p = doc.add_paragraph()
								p.add_run(f"{base}")
							for p_path, r_path, base in pairs:
								doc.add_heading(f"ä»¥ä¸‹æ˜¯ã€Š{base}ã€‹æ ¹æ®ä¼ä¸šæ ‡å‡†æ£€æŸ¥çš„åˆ†æè¿‡ç¨‹ï¼š", level=1)
								# Insert prompt content (mark each æç¤ºè¯... è¡Œä¸ºæ ‡é¢˜2)
								try:
									with open(p_path, 'r', encoding='utf-8') as f:
										ptext = f.read()
								except Exception:
									ptext = ""
								def _to_plain_table(txt: str) -> str:
									# Very simple HTML table conversion: extract <tr> rows and <td>/<th> cells
									try:
										import re as _re
										rows = []
										for m in _re.finditer(r"<tr[\s\S]*?>([\s\S]*?)</tr>", txt, flags=_re.IGNORECASE):
											row_html = m.group(1)
											cells = _re.findall(r"<(?:td|th)[^>]*>([\s\S]*?)</(?:td|th)>", row_html, flags=_re.IGNORECASE)
											cells_clean = [ _re.sub(r"<[^>]+>", "", c).strip() for c in cells ]
											if cells_clean:
												rows.append("\t".join(cells_clean))
										if rows:
											return "\n".join(rows)
										# Fallback: strip tags
										return _re.sub(r"<[^>]+>", " ", txt)
									except Exception:
										return txt
								for para in (ptext or "").splitlines():
									line = para.strip()
									if not line:
										continue
									if re.match(r"^æç¤ºè¯ï¼ˆç¬¬\d+éƒ¨åˆ†ï¼Œå…±\d+éƒ¨åˆ†ï¼‰ï¼š", line):
										doc.add_heading(line, level=2)
									else:
										doc.add_paragraph(_to_plain_table(line))
								# Insert response content
								try:
									with open(r_path, 'r', encoding='utf-8') as f:
										rtext = f.read()
								except Exception:
									rtext = ""
								for para in (rtext or "").splitlines():
									line = para.strip()
									if not line:
										continue
									doc.add_paragraph(_to_plain_table(line))
							from datetime import datetime as _dt
							ts_doc = _dt.now().strftime('%Y%m%d_%H%M%S')
							doc_path = os.path.join(final_dir, f"ä¼æ ‡æ£€æŸ¥åˆ†æè¿‡ç¨‹_{ts_doc}.docx")
							doc.save(doc_path)
							# Optional download button
							try:
								with open(doc_path, 'rb') as fdoc:
									st.download_button(label="ä¸‹è½½åˆ†æè¿‡ç¨‹Word", data=fdoc.read(), file_name=os.path.basename(doc_path), mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document', key=f"download_docx_{session_id}")
							except Exception:
								pass
					else:
						st.info("æœªå®‰è£… python-docxï¼Œæš‚æ— æ³•ç”ŸæˆWordæ–‡æ¡£ã€‚è¯·å®‰è£… python-docx åé‡è¯•ã€‚")
				except Exception as e:
					st.error(f"ç”Ÿæˆåˆ†æè¿‡ç¨‹Wordå¤±è´¥ï¼š{e}")
			except Exception as e:
				st.error(f"æ±‡æ€»å¯¼å‡ºå¤±è´¥ï¼š{e}")

		# Demo streaming phase (reads from prepared prompt/response chunks; no LLM calls)
		if st.session_state.get(f"enterprise_demo_{session_id}"):
			# Directories prepared by demo button copy
			prompt_dir = os.path.join(enterprise_out_root, 'prompt_text_chunks')
			resp_dir = os.path.join(enterprise_out_root, 'llm responses')
			final_dir = os.path.join(enterprise_out_root, 'final_results')
			prompted_and_json_dir = os.path.join(enterprise_out_root, 'prompted_llm responses_and_json')
			# Collect prompt chunk files
			prompt_files = []
			try:
				if os.path.isdir(prompt_dir):
					for f in os.listdir(prompt_dir):
						if f.lower().endswith('.txt'):
							prompt_files.append(f)
			except Exception:
				prompt_files = []
			# Natural sort by base name and numeric part index
			_prompt_entries = []
			for _f in prompt_files:
				_m = re.match(r"^(?P<base>.+)_pt(?P<idx>\d+)\.txt$", _f)
				if _m:
					_prompt_entries.append((_m.group('base').lower(), int(_m.group('idx')), _f))
				else:
					_prompt_entries.append(("", 0, _f))
			_prompt_entries.sort(key=lambda t: (t[0], t[1]))
			# Render each prompt/response pair in UI (original demo)
			for _, _, fname in _prompt_entries:
				m = re.match(r"^(?P<base>.+)_pt(?P<idx>\d+)\.txt$", fname)
				if not m:
					continue
				base = m.group('base')
				idx = m.group('idx')
				prompt_path = os.path.join(prompt_dir, fname)
				resp_name = f"response_{base}_pt{idx}.txt"
				resp_path = os.path.join(resp_dir, resp_name)
				# Read prompt content
				try:
					with open(prompt_path, 'r', encoding='utf-8') as f:
						prompt_text = f.read()
				except Exception:
					prompt_text = ""
				# Read response content (optional)
				resp_text = None
				if os.path.isfile(resp_path):
					try:
						with open(resp_path, 'r', encoding='utf-8') as f:
							resp_text = f.read()
					except Exception:
						resp_text = None
				col_prompt, col_response = st.columns([1, 1])
				with col_prompt:
					st.markdown(f"æç¤ºè¯ï¼ˆ{base} - ç¬¬{idx}éƒ¨åˆ†ï¼‰")
					prompt_container = st.container(height=400)
					with prompt_container:
						with st.chat_message("user"):
							prompt_placeholder = st.empty()
							words = (prompt_text or "").split()
							streamed = ""
							for j in range(0, len(words), 30):
								chunk_words = words[j:j+30]
								streamed += " ".join(chunk_words) + " "
								prompt_placeholder.text(streamed.strip())
								time.sleep(0.1)
						st.chat_input(placeholder="", disabled=True, key=f"enterprise_demo_prompt_{session_id}_{base}_{idx}")
				with col_response:
					st.markdown(f"ç¤ºä¾‹æ¯”å¯¹ç»“æœï¼ˆ{base} - ç¬¬{idx}éƒ¨åˆ†ï¼‰")
					response_container = st.container(height=400)
					with response_container:
						with st.chat_message("assistant"):
							resp_placeholder = st.empty()
							if resp_text is None:
								resp_placeholder.info("æœªæ‰¾åˆ°å¯¹åº”ç¤ºä¾‹ç»“æœã€‚")
							else:
								words_r = resp_text.split()
								streamed_r = ""
								for j in range(0, len(words_r), 30):
									chunk_words = words_r[j:j+30]
									streamed_r += " ".join(chunk_words) + " "
									resp_placeholder.write(streamed_r.strip())
									time.sleep(0.1)
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_demo_resp_{session_id}_{base}_{idx}")
			# (Removed hardcoded final report section for demo)
			# End of demo streaming pass; reset the flag
				st.session_state[f"enterprise_demo_{session_id}"] = False

			# New demo rendering: prompted_response_* and corresponding json_* from pre-made folder
			try:
				if os.path.isdir(prompted_and_json_dir):
					# Find prompted_response_*.txt files and for each, display prompt and its json_ response
					demo_parts = [f for f in os.listdir(prompted_and_json_dir) if f.startswith('prompted_response_') and f.lower().endswith('.txt')]
					# Natural sort by extracted name and numeric part index
					_entries = []
					for _pf in demo_parts:
						_m = re.match(r"^prompted_response_(?P<name>.+)_pt(?P<idx>\d+)\.txt$", _pf)
						if _m:
							_entries.append((_m.group('name').lower(), int(_m.group('idx')), _pf))
						else:
							_entries.append(("", 0, _pf))
					_entries.sort(key=lambda t: (t[0], t[1]))
					total_parts = len(_entries)
					for idx, (_, _, pf) in enumerate(_entries, start=1):
						p_path = os.path.join(prompted_and_json_dir, pf)
						# Map to json_<name>.txt in same folder
						json_name = f"json_{pf[len('prompted_response_'):] }"
						j_path = os.path.join(prompted_and_json_dir, json_name)
						# Read prompt
						try:
							with open(p_path, 'r', encoding='utf-8') as f:
								ptext = f.read()
						except Exception:
							ptext = ""
						# Read json response (text form)
						try:
							with open(j_path, 'r', encoding='utf-8') as f:
								jtext = f.read()
						except Exception:
							jtext = ""
						# Two column display
						col_lp, col_lr = st.columns([1, 1])
						with col_lp:
							st.markdown(f"ç”Ÿæˆæ±‡æ€»è¡¨æ ¼æç¤ºè¯ï¼ˆç¬¬{idx}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰")
							pc = st.container(height=400)
							with pc:
								with st.chat_message("user"):
									ph = st.empty()
									# Stream-like rendering of prompt
									words = (ptext or "").split()
									acc = ""
									for k in range(0, len(words), 30):
										acc += " ".join(words[k:k+30]) + " "
										ph.text(acc.strip())
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_demo_prompted_prompt_{session_id}_{idx}")
						with col_lr:
							st.markdown(f"ç”Ÿæˆæ±‡æ€»è¡¨æ ¼ç»“æœï¼ˆç¬¬{idx}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰")
							rc = st.container(height=400)
							with rc:
								with st.chat_message("assistant"):
									ph2 = st.empty()
									acc2 = ""
									words2 = (jtext or "").split()
									for k in range(0, len(words2), 30):
										acc2 += " ".join(words2[k:k+30]) + " "
										ph2.write(acc2.strip())
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_demo_prompted_resp_{session_id}_{idx}")
			except Exception:
				pass

			# Add demo download buttons for CSV/XLSX in final_results
			try:
				if os.path.isdir(final_dir):
					csv_files = [f for f in os.listdir(final_dir) if f.lower().endswith('.csv')]
					xlsx_files = [f for f in os.listdir(final_dir) if f.lower().endswith('.xlsx')]
					def _latest(path_list):
						if not path_list:
							return None
						paths = [os.path.join(final_dir, f) for f in path_list]
						paths.sort(key=lambda p: os.path.getmtime(p))
						return paths[-1]
					latest_csv = _latest(csv_files)
					latest_xlsx = _latest(xlsx_files)
					if latest_csv:
						with open(latest_csv, 'rb') as fcsv:
							st.download_button(label="ä¸‹è½½CSVç»“æœ(æ¼”ç¤º)", data=fcsv.read(), file_name=os.path.basename(latest_csv), mime='text/csv', key=f"demo_download_csv_{session_id}")
					if latest_xlsx:
						with open(latest_xlsx, 'rb') as fxlsx:
							st.download_button(label="ä¸‹è½½Excelç»“æœ(æ¼”ç¤º)", data=fxlsx.read(), file_name=os.path.basename(latest_xlsx), mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', key=f"demo_download_xlsx_{session_id}")
			except Exception:
				pass

		# Continue streaming/processing section (consume checkpoint: done â†’ playback; not_done â†’ run)
		if st.session_state.get(f"enterprise_continue_running_{session_id}"):
			# Retrieve context and init dirs
			std_txt_files = st.session_state.get(f"enterprise_std_txt_files_{session_id}") or []
			exam_txt_files = st.session_state.get(f"enterprise_exam_txt_files_{session_id}") or []
			enterprise_out = st.session_state.get(f"enterprise_out_root_{session_id}") or enterprise_out_root
			initial_dir = os.path.join(enterprise_out, 'initial_results')
			os.makedirs(initial_dir, exist_ok=True)
			checkpoint_dir = os.path.join(enterprise_out, 'checkpoint')
			manifest_path = os.path.join(checkpoint_dir, 'manifest.json')
			_manifest = None
			try:
				import json as _json
				with open(manifest_path, 'r', encoding='utf-8') as _mf:
					_manifest = _json.load(_mf) or {}
			except Exception:
				_manifest = None
			if not _manifest or not isinstance(_manifest.get('entries'), list) or not _manifest['entries']:
				st.info("æœªå‘ç°è·‘åˆ°ä¸€åŠçš„é¡¹ç›®")
				st.session_state[f"enterprise_continue_running_{session_id}"] = False
			else:
				# group by file in entry order
				from collections import OrderedDict as _OD
				_entries_sorted = sorted((_manifest.get('entries') or []), key=lambda e: int(e.get('id', 0)))
				_groups = _OD()
				for _e in _entries_sorted:
					fname = str(_e.get('file_name', ''))
					_groups.setdefault(fname, []).append(_e)
				bisheng_session_id = st.session_state.get(f"bisheng_session_{session_id}")
				for name, _elist in _groups.items():
					st.markdown(f"**ğŸ“„ ç»§ç»­æ¯”å¯¹ï¼š{name}**")
					full_out_text = ""
					prompt_texts = []
					_total_parts = len(_elist)
					try:
						_elist.sort(key=lambda e: (int(e.get('chunk_index', 0)), int(e.get('id', 0))))
					except Exception:
						pass
					for _i, _entry in enumerate(_elist, start=1):
						_prompt_file = str(_entry.get('prompt_file', ''))
						_response_file = str(_entry.get('response_file', ''))
						_status = str(_entry.get('status', 'not_done'))
						_prompt_path = os.path.join(checkpoint_dir, _prompt_file)
						_response_path = os.path.join(checkpoint_dir, _response_file)
						# load prompt
						try:
							with open(_prompt_path, 'r', encoding='utf-8') as _pf:
								prompt_text = _pf.read()
						except Exception:
							prompt_text = ""
						prompt_texts.append(prompt_text)
						col_prompt, col_response = st.columns([1, 1])
						with col_prompt:
							st.markdown(f"æç¤ºè¯ï¼ˆç¬¬{_i}éƒ¨åˆ†ï¼Œå…±{_total_parts}éƒ¨åˆ†ï¼‰")
							prompt_container = st.container(height=400)
							with prompt_container:
								with st.chat_message("user"):
									prompt_placeholder = st.empty()
									words = (prompt_text or "").split()
									streamed = ""
									for j in range(0, len(words), 30):
										chunk_words = words[j:j+30]
										streamed += " ".join(chunk_words) + " "
										prompt_placeholder.text(streamed.strip())
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_continue_prompt_{session_id}_{name}_{_i}")
						with col_response:
							st.markdown(f"AIæ¯”å¯¹ç»“æœï¼ˆç¬¬{_i}éƒ¨åˆ†ï¼Œå…±{_total_parts}éƒ¨åˆ†ï¼‰")
							response_container = st.container(height=400)
							with response_container:
								with st.chat_message("assistant"):
									ph = st.empty()
									if _status == 'done':
										# playback response
										try:
											with open(_response_path, 'r', encoding='utf-8') as _rf:
												_resp_text = _rf.read()
										except Exception:
											_resp_text = ""
										acc = ""
										words_r = (_resp_text or "").split()
										for k in range(0, len(words_r), 30):
											chunk_words = words_r[k:k+30]
											acc += " ".join(chunk_words) + " "
											ph.write(acc.strip())
											import time as _t
											_t.sleep(0.1)
										full_out_text += ("\n\n" if full_out_text else "") + (_resp_text or "")
									else:
										# run LLM and update manifest
										try:
											kb_name_dyn = f"{session_id}_{TAB_SLUG}"
											kid = find_knowledge_id_by_name(BISHENG_BASE_URL, BISHENG_API_KEY or None, kb_name_dyn)
											response_text = ""
											res = call_flow_process(
												base_url=BISHENG_BASE_URL,
												flow_id=BISHENG_FLOW_ID,
												question=prompt_text,
												kb_id=kid,
												input_node_id=FLOW_INPUT_NODE_ID,
												api_key=BISHENG_API_KEY or None,
												session_id=bisheng_session_id,
												history_count=0,
												extra_tweaks=None,
												milvus_node_id=FLOW_MILVUS_NODE_ID,
												es_node_id=FLOW_ES_NODE_ID,
												timeout_s=180,
												max_retries=2,
											)
											ans_text, new_sid = parse_flow_answer(res)
											if new_sid:
												bisheng_session_id = new_sid
												st.session_state[f"bisheng_session_{session_id}"] = bisheng_session_id
											ph.write(ans_text or "")
											full_out_text += ("\n\n" if full_out_text else "") + (ans_text or "")
											# save resp and mark done
											try:
												with open(_response_path, 'w', encoding='utf-8') as _rf:
													_rf.write(ans_text or "")
											except Exception:
												pass
											try:
												import json as _json, tempfile as _tmp, shutil as _sh
												for __e in _manifest.get('entries', []):
													if int(__e.get('id', 0)) == int(_entry.get('id', 0)):
														__e['status'] = 'done'
														break
												with _tmp.NamedTemporaryFile('w', delete=False, encoding='utf-8', dir=checkpoint_dir) as _tf:
													_tf.write(_json.dumps(_manifest, ensure_ascii=False, indent=2))
													_tmpname = _tf.name
												_sh.move(_tmpname, manifest_path)
											except Exception:
												pass
										except Exception as e:
											ph.error(f"è°ƒç”¨å¤±è´¥ï¼š{e}")
							st.chat_input(placeholder="", disabled=True, key=f"enterprise_continue_response_{session_id}_{name}_{_i}")
					# write combined prompt/response for this file
					name_no_ext = os.path.splitext(name)[0]
					try:
						# prompts
						total_parts = len(prompt_texts)
						prompt_out_lines = []
						for idx_p, ptxt in enumerate(prompt_texts, start=1):
							prompt_out_lines.append(f"æç¤ºè¯ï¼ˆç¬¬{idx_p}éƒ¨åˆ†ï¼Œå…±{total_parts}éƒ¨åˆ†ï¼‰ï¼š")
							prompt_out_lines.append(ptxt)
						prompt_out_text = "\n".join(prompt_out_lines)
						with open(os.path.join(initial_dir, f"prompt_{name_no_ext}.txt"), 'w', encoding='utf-8') as pf:
							pf.write(prompt_out_text)
					except Exception:
						pass
					try:
						with open(os.path.join(initial_dir, f"response_{name_no_ext}.txt"), 'w', encoding='utf-8') as outf:
							outf.write(full_out_text)
					except Exception:
						pass
			# End continue branch
# The end