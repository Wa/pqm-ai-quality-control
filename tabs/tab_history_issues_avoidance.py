from __future__ import annotations

import hashlib
import io
import json
import os
import re
import zipfile
from dataclasses import dataclass
from datetime import datetime
from typing import Iterable, Optional, Sequence

import pandas as pd
import requests
import streamlit as st
from ollama import Client as OllamaClient
from openpyxl import load_workbook
from pydantic import BaseModel, Field

from config import CONFIG
from util import ensure_session_dirs, handle_file_upload, resolve_ollama_host
from bisheng_client import (
    create_knowledge,
    find_knowledge_id_by_name,
    kb_sync_folder,
    parse_flow_answer,
)


PDF_EXTENSIONS = {".pdf"}
WORD_PPT_EXTENSIONS = {".doc", ".docx", ".ppt", ".pptx"}
SPREADSHEET_EXTENSIONS = {".xls", ".xlsx", ".xlsm", ".csv"}
HEADER_SCAN_LIMIT = 10
HEADER_SCORE_THRESHOLD = 4
LLM_CHUNK_LIMIT = 9500
HISTORY_KB_MODEL_ID = 7
HISTORY_FLOW_ID = "191af6f3565e415ca9670f1bc2b9117e"
HISTORY_FLOW_BASE = (
    os.getenv("HISTORY_FLOW_BASE")
    or f"{CONFIG.get('bisheng', {}).get('base_url', 'http://10.31.60.11:3001').rstrip('/')}/api/v1/process"
)
HISTORY_FLOW_TWEAKS = {
    "MixEsVectorRetriever-J35CZ": {},
    "Milvus-cyR5W": {},
    "PromptTemplate-bs0vj": {},
    "BishengLLM-768ac": {},
    "ElasticKeywordsSearch-1c80e": {},
    "RetrievalQA-f0f31": {},
    "CombineDocsChain-2f68e": {},
}

HEADER_ALIASES = {
    "failure_mode": [
        "å¤±æ•ˆæ¨¡å¼",
        "é—®é¢˜",
        "é—®é¢˜æè¿°",
        "å†å²é—®é¢˜",
        "ä¸è‰¯ç°è±¡",
        "æ•…éšœæ¨¡å¼",
        "Failure Mode",
    ],
    "root_cause": [
        "æ ¹å› ",
        "åŸå› ",
        "åŸå› åˆ†æ",
        "é—®é¢˜åŸå› ",
        "Root Cause",
        "å‘ç”ŸåŸå› ",
    ],
    "prevention_action": [
        "é¢„é˜²æªæ–½",
        "é¢„é˜²è¡ŒåŠ¨",
        "æ”¹è¿›æªæ–½",
        "æ§åˆ¶æªæ–½",
        "æ°¸ä¹…æªæ–½",
        "Permanent Action",
    ],
    "detection_action": [
        "æ£€æµ‹æªæ–½",
        "æ£€æµ‹è®¡åˆ’",
        "æ£€éªŒæªæ–½",
        "éªŒè¯æªæ–½",
        "Detection",
    ],
    "severity": ["ä¸¥é‡åº¦", "ä¸¥é‡æ€§", "S"],
    "occurrence": ["å‘ç”Ÿåº¦", "å‘ç”Ÿé¢‘åº¦", "O"],
    "detection": ["æ¢æµ‹åº¦", "æ£€æµ‹åº¦", "D"],
    "risk_priority": ["RPN", "é£é™©ä¼˜å…ˆæ•°", "é£é™©è¯„ä¼°"],
    "responsible": ["è´£ä»»äºº", "è´Ÿè´£äºº", "è´£ä»»éƒ¨é—¨", "Owner"],
    "due_date": ["å®Œæˆæ—¶é—´", "è®¡åˆ’å®Œæˆæ—¶é—´", "æˆªæ­¢æ—¥æœŸ", "å®Œæˆæ—¥æœŸ", "Due Date"],
    "status": ["çŠ¶æ€", "è¿›åº¦", "è½å®æƒ…å†µ"],
    "remarks": ["å¤‡æ³¨", "è¯´æ˜", "å¤‡æ³¨ä¿¡æ¯", "Remarks"],
}

REQUIRED_FIELDS = ["failure_mode", "root_cause", "prevention_action", "detection_action"]
OPTIONAL_FIELDS = [
    "severity",
    "occurrence",
    "detection",
    "risk_priority",
    "responsible",
    "due_date",
    "status",
    "remarks",
]

FULLWIDTH_TRANSLATION = str.maketrans({
    "ï¼Œ": ",",
    "ã€‚": ".",
    "ï¼›": ";",
    "ï¼š": ":",
    "ï¼Ÿ": "?",
    "ï¼": "!",
    "ï¼ˆ": "(",
    "ï¼‰": ")",
    "ã€": "[",
    "ã€‘": "]",
    "â€œ": '"',
    "â€": '"',
    "â€˜": "'",
    "â€™": "'",
})

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€åèµ„æ·±è´¨é‡å·¥ç¨‹å¸ˆï¼Œéœ€è¦ä»æ–‡æœ¬ä¸­æå–å†å²é—®é¢˜è®°å½•ã€‚"
    "è¯·æŠŠæ¯æ®µå†…å®¹è½¬æ¢æˆJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è‡³å°‘åŒ…å«"
    "failure_modeã€root_causeã€prevention_actionã€detection_actionå­—æ®µã€‚"
    "å¦‚æœæ— æ³•æ‰¾åˆ°æŸä¸ªå­—æ®µï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ã€‚ä»…è¿”å›JSONï¼Œå‹¿æ·»åŠ é¢å¤–è¯´æ˜ã€‚"
)


class IssueRecord(BaseModel):
    failure_mode: str = Field(..., description="å†å²é—®é¢˜æˆ–å¤±æ•ˆæ¨¡å¼")
    root_cause: str = ""
    prevention_action: str = ""
    detection_action: str = ""
    severity: str | None = None
    occurrence: str | None = None
    detection: str | None = None
    risk_priority: str | None = None
    responsible: str | None = None
    due_date: str | None = None
    status: str | None = None
    remarks: str | None = None
    hash_key: str = Field(..., description="åŒæ–‡ä»¶å†…ç”¨äºå»é‡çš„å“ˆå¸Œ")


@dataclass
class IssueSummary:
    total_rows: int = 0
    parsed_rows: int = 0
    skipped_rows: int = 0
    duplicates: int = 0

    def to_dict(self) -> dict[str, int]:
        return {
            "total_rows": self.total_rows,
            "parsed_rows": self.parsed_rows,
            "skipped_rows": self.skipped_rows,
            "duplicates": self.duplicates,
        }


def _list_files(folder: str, extensions: Iterable[str] | None = None) -> list[str]:
    if not folder or not os.path.isdir(folder):
        return []
    results: list[str] = []
    for name in os.listdir(folder):
        path = os.path.join(folder, name)
        if not os.path.isfile(path):
            continue
        if extensions:
            ext = os.path.splitext(name)[1].lower()
            if ext not in extensions:
                continue
        results.append(path)
    return results


def _list_pdfs(folder: str) -> list[str]:
    return _list_files(folder, PDF_EXTENSIONS)


def _mineru_parse_pdf(pdf_path: str) -> bytes:
    api_url = "http://10.31.60.127:8000/file_parse"
    data = {
        "backend": "vlm-sglang-engine",
        "response_format_zip": "true",
        "formula_enable": "true",
        "table_enable": "true",
        "return_images": "false",
        "return_middle_json": "true",
        "return_model_output": "false",
        "return_content_list": "true",
    }
    with open(pdf_path, "rb") as f:
        files = {"files": (os.path.basename(pdf_path), f, "application/pdf")}
        resp = requests.post(api_url, data=data, files=files, timeout=300)
        if resp.status_code != 200:
            raise RuntimeError(f"MinerU API error {resp.status_code}: {resp.text[:200]}")
        return resp.content


def _zip_to_txts(zip_bytes: bytes, target_txt_path: str) -> bool:
    bio = io.BytesIO(zip_bytes)
    try:
        with zipfile.ZipFile(bio) as zf:
            md_members = [n for n in zf.namelist() if n.lower().endswith(".md")]
            if not md_members:
                return False
            name = md_members[0]
            content = zf.read(name)
            os.makedirs(os.path.dirname(target_txt_path), exist_ok=True)
            with open(target_txt_path, "wb") as out_f:
                out_f.write(content)
            return True
    except zipfile.BadZipFile:
        return False


def _list_word_ppt(folder: str) -> list[str]:
    return _list_files(folder, WORD_PPT_EXTENSIONS)


def _unstructured_partition_to_txt(file_path: str, target_txt_path: str) -> bool:
    api_url = (
        os.getenv("UNSTRUCTURED_API_URL")
        or CONFIG.get("services", {}).get("unstructured_api_url")
        or "http://10.31.60.11:8000/general/v0/general"
    )
    try:
        with open(file_path, "rb") as f:
            files = {"files": (os.path.basename(file_path), f)}
            form = {
                "strategy": "auto",
                "ocr_languages": "chi_sim,eng",
                "infer_table_structure": "true",
            }
            resp = requests.post(api_url, files=files, data=form, timeout=300)
            if resp.status_code != 200:
                raise RuntimeError(f"Unstructured API {resp.status_code}: {resp.text[:200]}")
            data = resp.json()
            lines: list[str] = []
            if isinstance(data, list):
                for el in data:
                    text = None
                    if isinstance(el, dict):
                        text = el.get("text")
                        if not text and isinstance(el.get("data"), list):
                            for row in el["data"]:
                                if isinstance(row, list):
                                    lines.append("\t".join(str(c) for c in row))
                            continue
                    if isinstance(text, str) and text.strip():
                        lines.append(text.strip())
            os.makedirs(os.path.dirname(target_txt_path), exist_ok=True)
            with open(target_txt_path, "w", encoding="utf-8") as out_f:
                out_f.write("\n".join(lines))
            return True
    except Exception:
        return False
    return False


def _list_excels(folder: str) -> list[str]:
    return _list_files(folder, SPREADSHEET_EXTENSIONS - {".csv"})


def _collect_files(folder: str) -> list[dict[str, object]]:
    if not folder or not os.path.isdir(folder):
        return []
    items = []
    for name in os.listdir(folder):
        path = os.path.join(folder, name)
        if not os.path.isfile(path):
            continue
        stat = os.stat(path)
        items.append(
            {
                "name": name,
                "path": path,
                "size": stat.st_size,
                "modified": stat.st_mtime,
            }
        )
    items.sort(key=lambda info: info["modified"], reverse=True)
    return items


def _format_file_size(size_bytes: int) -> str:
    if size_bytes == 0:
        return "0 B"
    units = ["B", "KB", "MB", "GB"]
    value = float(size_bytes)
    index = 0
    while value >= 1024 and index < len(units) - 1:
        value /= 1024.0
        index += 1
    return f"{value:.1f} {units[index]}"


def _format_timestamp(timestamp: float) -> str:
    return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M")


def _truncate_filename(filename: str, max_length: int = 40) -> str:
    if len(filename) <= max_length:
        return filename
    name, ext = os.path.splitext(filename)
    available = max_length - len(ext) - 3
    if available <= 0:
        return filename[: max_length - 3] + "..."
    return name[:available] + "..." + ext


def _ensure_generated_dirs(root: str) -> dict[str, str]:
    mapping: dict[str, str] = {}
    os.makedirs(root, exist_ok=True)
    for key in ("issue_lists", "dfmea", "pfmea", "cp"):
        path = os.path.join(root, f"{key}_txt")
        os.makedirs(path, exist_ok=True)
        mapping[key] = path
    initial_dir = os.path.join(root, "initial_results")
    final_dir = os.path.join(root, "final_results")
    os.makedirs(initial_dir, exist_ok=True)
    os.makedirs(final_dir, exist_ok=True)
    mapping["initial_results"] = initial_dir
    mapping["final_results"] = final_dir
    return mapping


def _sanitize_sheet_name(name: str) -> str:
    bad = ["\\", "/", ":", "*", "?", '"', "<", ">", "|"]
    for ch in bad:
        name = name.replace(ch, "_")
    return "_".join(name.strip().split())[:80] or "Sheet"


def _process_pdf_folder(input_dir: str, output_dir: str, progress_area) -> list[str]:
    pdf_paths = _list_pdfs(input_dir)
    created: list[str] = []
    if not pdf_paths:
        progress_area.info("ï¼ˆæ— PDFæ–‡ä»¶å¯å¤„ç†ï¼‰")
        return created
    for pdf_path in pdf_paths:
        orig_name = os.path.basename(pdf_path)
        out_txt = os.path.join(output_dir, f"{orig_name}.txt")
        try:
            if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
                progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
                continue
            progress_area.write(f"è§£æPDF: {orig_name} â€¦")
            zip_bytes = _mineru_parse_pdf(pdf_path)
            ok = _zip_to_txts(zip_bytes, out_txt)
            if ok:
                created.append(out_txt)
                progress_area.success(f"å·²ç”Ÿæˆ: {os.path.basename(out_txt)}")
            else:
                progress_area.warning(f"æœªç”Ÿæˆæ–‡æœ¬ï¼Œè·³è¿‡: {orig_name}")
        except Exception as error:
            progress_area.error(f"å¤±è´¥: {orig_name} â†’ {error}")
    return created


def _process_word_ppt_folder(input_dir: str, output_dir: str, progress_area) -> list[str]:
    doc_paths = _list_word_ppt(input_dir)
    created: list[str] = []
    if not doc_paths:
        progress_area.info("ï¼ˆæ— Word/PPTæ–‡ä»¶å¯å¤„ç†ï¼‰")
        return created
    for file_path in doc_paths:
        orig_name = os.path.basename(file_path)
        out_txt = os.path.join(output_dir, f"{orig_name}.txt")
        try:
            if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
                progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
                continue
            progress_area.write(f"è§£ææ–‡æ¡£: {orig_name} â€¦")
            ok = _unstructured_partition_to_txt(file_path, out_txt)
            if ok:
                created.append(out_txt)
                progress_area.success(f"å·²ç”Ÿæˆ: {os.path.basename(out_txt)}")
            else:
                progress_area.warning(f"æœªç”Ÿæˆæ–‡æœ¬ï¼Œè·³è¿‡: {orig_name}")
        except Exception as error:
            progress_area.error(f"å¤±è´¥: {orig_name} â†’ {error}")
    return created


def _process_excel_folder(input_dir: str, output_dir: str, progress_area) -> list[str]:
    excel_paths = _list_excels(input_dir)
    created: list[str] = []
    if not excel_paths:
        progress_area.info("ï¼ˆæ— Excelæ–‡ä»¶å¯å¤„ç†ï¼‰")
        return created
    for excel_path in excel_paths:
        orig_name = os.path.basename(excel_path)
        try:
            xls = pd.ExcelFile(excel_path)
            for sheet in xls.sheet_names:
                safe_sheet = _sanitize_sheet_name(sheet)
                out_txt = os.path.join(output_dir, f"{orig_name}_SHEET_{safe_sheet}.txt")
                if os.path.exists(out_txt) and os.path.getsize(out_txt) > 0:
                    progress_area.info(f"å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {os.path.basename(out_txt)}")
                    continue
                progress_area.write(f"è½¬æ¢Excel: {orig_name} / {sheet} â€¦")
                df = xls.parse(sheet)
                df.to_csv(out_txt, index=False, encoding="utf-8")
                created.append(out_txt)
                progress_area.success(f"å·²ç”Ÿæˆ: {os.path.basename(out_txt)}")
        except Exception as error:
            progress_area.error(f"å¤±è´¥: {orig_name} â†’ {error}")
    return created


def _process_category(label: str, source_dir: str | None, output_dir: str, progress_area) -> list[str]:
    os.makedirs(output_dir, exist_ok=True)
    if not source_dir or not os.path.isdir(source_dir):
        progress_area.warning(f"æœªæ‰¾åˆ° {label} ä¸Šä¼ ç›®å½•ï¼Œå·²è·³è¿‡ã€‚")
        return []
    progress_area.markdown(f"**{label} â†’ æ–‡æœ¬è½¬æ¢**")
    created: list[str] = []
    created.extend(_process_pdf_folder(source_dir, output_dir, progress_area))
    created.extend(_process_word_ppt_folder(source_dir, output_dir, progress_area))
    created.extend(_process_excel_folder(source_dir, output_dir, progress_area))
    if not created:
        progress_area.info(f"{label} æœªç”Ÿæˆä»»ä½•æ–‡æœ¬æ–‡ä»¶ï¼Œè¯·ç¡®è®¤å·²ä¸Šä¼  PDF/Word/Excelã€‚")
    return created


def _safe_result_name(base: str) -> str:
    name = re.sub(r"[^0-9A-Za-z\u4e00-\u9fff._-]", "_", base)
    name = name.strip("._")
    return name[:120] or "result"


def _sanitize_bisheng_base_url(raw: str | None) -> str:
    if not raw:
        return ""
    trimmed = raw.strip()
    if not trimmed:
        return ""
    trimmed = trimmed.rstrip("/")
    suffixes = [
        "/api/v1/process",
        "/api/v2/workflow/invoke",
        "/api/v2/workflow",
        "/api/v2",
    ]
    lowered = trimmed.lower()
    for suffix in suffixes:
        if lowered.endswith(suffix):
            trimmed = trimmed[: -len(suffix)].rstrip("/")
            lowered = trimmed.lower()
    return trimmed


def _resolve_bisheng_credentials() -> tuple[str, Optional[str]]:
    settings = CONFIG.get("bisheng", {})
    base_url = (
        os.getenv("HISTORY_BISHENG_BASE_URL")
        or os.getenv("BISHENG_BASE_URL")
        or settings.get("base_url")
        or ""
    )
    api_key = (
        os.getenv("HISTORY_BISHENG_API_KEY")
        or os.getenv("BISHENG_API_KEY")
        or settings.get("api_key")
        or ""
    )
    return _sanitize_bisheng_base_url(base_url), (api_key or None)


def _build_history_kb_name(session_id: str) -> str:
    base = f"{session_id}_history"
    return _safe_result_name(base)


def _sync_history_kb(
    session_id: str,
    text_dirs: dict[str, str],
    progress_area,
) -> Optional[int]:
    base_url, api_key = _resolve_bisheng_credentials()
    if not base_url:
        progress_area.warning("æœªé…ç½®æ¯•æ˜‡æœåŠ¡åœ°å€ï¼Œè·³è¿‡çŸ¥è¯†åº“åŒæ­¥ã€‚")
        return None
    kb_name = _build_history_kb_name(session_id)
    try:
        knowledge_id = find_knowledge_id_by_name(base_url, api_key, kb_name)
        if knowledge_id:
            progress_area.info(f"å·²æ‰¾åˆ°çŸ¥è¯†åº“ï¼š{kb_name} (ID: {knowledge_id})")
        else:
            progress_area.write(f"æ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“ï¼š{kb_name} â€¦")
            knowledge_id = create_knowledge(
                base_url,
                api_key,
                kb_name,
                model=str(HISTORY_KB_MODEL_ID),
                description="å†å²é—®é¢˜è§„é¿-é¡¹ç›®æ–‡æ¡£",
            )
            if knowledge_id:
                progress_area.success(f"å·²åˆ›å»ºçŸ¥è¯†åº“ï¼š{kb_name} (ID: {knowledge_id})")
        if not knowledge_id:
            progress_area.warning(
                "æ— æ³•åˆ›å»ºæˆ–è·å–çŸ¥è¯†åº“ï¼Œè·³è¿‡åŒæ­¥ã€‚è¯·æ£€æŸ¥æ¯•æ˜‡æœåŠ¡é…ç½®ä¸æƒé™ã€‚"
            )
            return None
        total_uploaded = 0
        total_skipped = 0
        for label, folder in text_dirs.items():
            if not folder or not os.path.isdir(folder):
                continue
            progress_area.write(f"åŒæ­¥è‡³çŸ¥è¯†åº“ ({label}) â€¦")
            try:
                result = kb_sync_folder(
                    base_url=base_url,
                    api_key=api_key,
                    knowledge_id=int(knowledge_id),
                    folder_path=folder,
                    clear_first=False,
                    chunk_size=800,
                    chunk_overlap=80,
                    separators=["\n\n", "\n"],
                    separator_rule=["after", "after"],
                )
            except Exception as error:
                progress_area.error(f"åŒæ­¥ {label} å¤±è´¥: {error}")
                continue
            uploaded = len(result.get("uploaded", [])) if isinstance(result, dict) else 0
            skipped = len(result.get("skipped", [])) if isinstance(result, dict) else 0
            total_uploaded += uploaded
            total_skipped += skipped
            progress_area.success(
                f"{label} å·²åŒæ­¥ï¼šä¸Šä¼  {uploaded} ä¸ªï¼Œè·³è¿‡ {skipped} ä¸ªã€‚"
            )
        if total_uploaded or total_skipped:
            progress_area.info(
                f"çŸ¥è¯†åº“åŒæ­¥å®Œæˆï¼ˆä¸Šä¼  {total_uploaded}ï¼Œè·³è¿‡ {total_skipped}ï¼‰ã€‚"
            )
        else:
            progress_area.info("çŸ¥è¯†åº“åŒæ­¥å®Œæˆï¼ˆæ— å¯åŒæ­¥æ–‡ä»¶ï¼‰ã€‚")
        return int(knowledge_id)
    except Exception as error:
        progress_area.error(f"çŸ¥è¯†åº“åŒæ­¥å¤±è´¥: {error}")
        return None


def _apply_kb_to_tweaks(kb_id: Optional[int]) -> dict:
    tweaks = {key: value.copy() for key, value in HISTORY_FLOW_TWEAKS.items()}
    if kb_id is None:
        return tweaks
    for node_id in ("Milvus-cyR5W", "ElasticKeywordsSearch-1c80e"):
        node_tw = tweaks.get(node_id, {}).copy()
        node_tw["collection_id"] = str(kb_id)
        tweaks[node_id] = node_tw
    return tweaks


def _invoke_history_flow(prompt: str, kb_id: Optional[int], progress_area) -> dict:
    payload = {"inputs": {"input": prompt}}
    tweaks = _apply_kb_to_tweaks(kb_id)
    if tweaks:
        payload["tweaks"] = tweaks
    url = f"{HISTORY_FLOW_BASE.rstrip('/')}/{HISTORY_FLOW_ID}"
    try:
        response = requests.post(url, json=payload, timeout=180)
        response.raise_for_status()
        return response.json()
    except Exception as error:
        progress_area.error(f"è°ƒç”¨æ¯•æ˜‡æ¯”å¯¹å¤±è´¥: {error}")
        return {"error": str(error)}


def _build_record_prompt(record: IssueRecord) -> str:
    details = [
        f"å¤±æ•ˆæ¨¡å¼: {record.failure_mode}",
        f"æ ¹å› : {record.root_cause or 'ï¼ˆæœªæä¾›ï¼‰'}",
        f"é¢„é˜²æªæ–½: {record.prevention_action or 'ï¼ˆæœªæä¾›ï¼‰'}",
        f"æ£€æµ‹æªæ–½: {record.detection_action or 'ï¼ˆæœªæä¾›ï¼‰'}",
    ]
    for label, value in (
        ("ä¸¥é‡åº¦", record.severity),
        ("å‘ç”Ÿåº¦", record.occurrence),
        ("æ£€æµ‹åº¦", record.detection),
        ("è´£ä»»äºº", record.responsible),
        ("è®¡åˆ’å®Œæˆæ—¶é—´", record.due_date),
        ("çŠ¶æ€", record.status),
        ("å¤‡æ³¨", record.remarks),
    ):
        if value:
            details.append(f"{label}: {value}")
    instructions = """
ä½ æ˜¯ä¸€åèµ„æ·±APQPè´¨é‡å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ£€æŸ¥å†å²é—®é¢˜æ˜¯å¦å·²åœ¨å½“å‰é¡¹ç›®çš„DFMEAã€PFMEAã€æ§åˆ¶è®¡åˆ’ä¸­å¾—åˆ°é¢„é˜²ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. é˜…è¯»â€œå†å²é—®é¢˜â€çš„æè¿°ï¼Œç†è§£å…¶å¤±æ•ˆæ¨¡å¼ã€æ ¹æœ¬åŸå› ã€é¢„é˜²æªæ–½ã€æ£€æµ‹æªæ–½ã€‚
2. é˜…è¯»æä¾›çš„å½“å‰é¡¹ç›®æ–‡æ¡£ç‰‡æ®µï¼ˆDFMEA/PFMEA/æ§åˆ¶è®¡åˆ’ï¼‰ã€‚
3. åˆ¤æ–­è¯¥å†å²é—®é¢˜çš„é¢„é˜²ä¸æ£€æµ‹æªæ–½æ˜¯å¦å·²ç»è¢«è¦†ç›–ã€‚
4. è‹¥æœªè¦†ç›–ï¼Œè¯·å»ºè®®åœ¨å“ªä¸ªæ–‡æ¡£ï¼ˆDFMEA/PFMEA/æ§åˆ¶è®¡åˆ’ï¼‰ä¸­å¢åŠ ä½•ç§æ§åˆ¶ã€‚

è¯·ä»…è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼š
{
  "status": "å·²è¦†ç›– | éƒ¨åˆ†è¦†ç›– | æœªè¦†ç›–",
  "where_covered": [
    {"doc_type": "PFMEA", "row_ref": "PFMEA-R12", "è¯´æ˜": "å·²æœ‰UVå›ºåŒ–æ—¶é—´5såŠæ‹‰åŠ›æµ‹è¯•æ§åˆ¶"}
  ],
  "å»ºè®®æ›´æ–°": [
    {"ç›®æ ‡æ–‡ä»¶": "æ§åˆ¶è®¡åˆ’", "å»ºè®®å†…å®¹": "å¢åŠ 100%æ‹‰åŠ›æµ‹è¯•â‰¥5N", "ç†ç”±": "å¯¹åº”å†å²é—®é¢˜NTCæ¢å¤´æ¾è„±"}
  ]
}
""".strip()
    return f"{instructions}\n\nå†å²é—®é¢˜è¯¦æƒ…ï¼š\n" + "\n".join(f"- {line}" for line in details if line)


def _load_issue_payloads(issue_dir: str) -> list[tuple[str, list[IssueRecord]]]:
    payloads: list[tuple[str, list[IssueRecord]]] = []
    if not issue_dir or not os.path.isdir(issue_dir):
        return payloads
    for name in sorted(os.listdir(issue_dir)):
        path = os.path.join(issue_dir, name)
        if not os.path.isfile(path) or not name.lower().endswith(".txt"):
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            continue
        records_raw = data.get("records") if isinstance(data, dict) else None
        if not isinstance(records_raw, list):
            continue
        records: list[IssueRecord] = []
        for item in records_raw:
            if not isinstance(item, dict):
                continue
            try:
                records.append(IssueRecord(**item))
            except Exception:
                continue
        if records:
            payloads.append((name, records))
    return payloads


def _evaluate_history_problems(
    issue_dir: str,
    initial_dir: str,
    final_dir: str,
    kb_id: Optional[int],
    session_id: str,
    progress_area,
) -> None:
    os.makedirs(initial_dir, exist_ok=True)
    os.makedirs(final_dir, exist_ok=True)
    payloads = _load_issue_payloads(issue_dir)
    if not payloads:
        progress_area.info("æœªå‘ç°å†å²é—®é¢˜è§£æç»“æœï¼Œè·³è¿‡æ¯”å¯¹ã€‚")
        return
    total_records = sum(len(records) for _, records in payloads)
    if total_records == 0:
        progress_area.info("å†å²é—®é¢˜è®°å½•ä¸ºç©ºã€‚")
        return
    progress_area.markdown("**å¼€å§‹å†å²é—®é¢˜è¦†ç›–æ€§æ¯”å¯¹**")
    progress_bar = progress_area.progress(0.0)
    collected_rows: list[dict[str, object]] = []
    processed = 0
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    for file_name, records in payloads:
        safe_file = _safe_result_name(os.path.splitext(file_name)[0])
        for index, record in enumerate(records, start=1):
            prompt = _build_record_prompt(record)
            response = _invoke_history_flow(prompt, kb_id, progress_area)
            answer_text, _ = parse_flow_answer(response)
            parsed_answer: dict[str, object] | None = None
            if isinstance(answer_text, str) and answer_text.strip():
                try:
                    parsed_answer = json.loads(answer_text)
                except json.JSONDecodeError:
                    parsed_answer = None
            initial_payload = {
                "source_file": file_name,
                "record_index": index,
                "prompt": prompt,
                "record": record.model_dump(),
                "response": response,
                "answer": answer_text,
            }
            out_name = f"{safe_file}_record_{index:04d}.json"
            try:
                with open(os.path.join(initial_dir, out_name), "w", encoding="utf-8") as f:
                    json.dump(initial_payload, f, ensure_ascii=False, indent=2)
            except Exception as error:
                progress_area.warning(f"å†™å…¥åˆæ­¥ç»“æœå¤±è´¥ ({out_name}): {error}")
            row = {
                "source_file": file_name,
                "record_index": index,
                "session_id": session_id,
                "failure_mode": record.failure_mode,
                "root_cause": record.root_cause,
                "prevention_action": record.prevention_action,
                "detection_action": record.detection_action,
                "status": "",
                "where_covered": "",
                "å»ºè®®æ›´æ–°": "",
                "åŸå§‹å›ç­”": answer_text or "",
            }
            if parsed_answer:
                row["status"] = str(parsed_answer.get("status", ""))
                row["where_covered"] = json.dumps(
                    parsed_answer.get("where_covered", []), ensure_ascii=False
                )
                row["å»ºè®®æ›´æ–°"] = json.dumps(
                    parsed_answer.get("å»ºè®®æ›´æ–°", []), ensure_ascii=False
                )
            collected_rows.append(row)
            processed += 1
            progress_bar.progress(min(processed / total_records, 1.0))
    progress_bar.empty()
    if not collected_rows:
        progress_area.info("æœªè·å–åˆ°æœ‰æ•ˆçš„æ¯”å¯¹ç»“æœã€‚")
        return
    df = pd.DataFrame(collected_rows)
    csv_path = os.path.join(final_dir, f"history_issues_results_{timestamp}.csv")
    xlsx_path = os.path.join(final_dir, f"history_issues_results_{timestamp}.xlsx")
    try:
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        progress_area.success(f"å·²ç”ŸæˆCSVç»“æœï¼š{os.path.basename(csv_path)}")
    except Exception as error:
        progress_area.error(f"å¯¼å‡ºCSVå¤±è´¥: {error}")
    try:
        df.to_excel(xlsx_path, index=False, engine="openpyxl")
        progress_area.success(f"å·²ç”ŸæˆExcelç»“æœï¼š{os.path.basename(xlsx_path)}")
    except Exception as error:
        progress_area.error(f"å¯¼å‡ºExcelå¤±è´¥: {error}")


def _strip_html(value: str) -> str:
    return re.sub(r"<[^>]+>", "", value)


def _normalize_text(value: str) -> str:
    text = value.replace("\u00a0", " ")
    text = re.sub(r"\s+", " ", text)
    return text.translate(FULLWIDTH_TRANSLATION).strip()


def _clean_cell_value(value) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return _normalize_text(_strip_html(value))
    if isinstance(value, (int, float)):
        if isinstance(value, float) and value.is_integer():
            return str(int(value))
        return str(value)
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.strftime("%Y-%m-%d")
    return _normalize_text(str(value))


def _horizontal_fill(row: Sequence[str]) -> list[str]:
    filled: list[str] = []
    last = ""
    for cell in row:
        text = cell or ""
        if not text and last:
            filled.append(last)
        else:
            filled.append(text)
            if text:
                last = text
    return filled


def _build_header_candidate(rows: list[list[str]], indices: Sequence[int]) -> list[str]:
    selected = []
    for idx in indices:
        if idx < len(rows):
            selected.append(_horizontal_fill(rows[idx]))
    if not selected:
        return []
    width = max(len(row) for row in selected)
    headers: list[str] = []
    for col in range(width):
        parts = []
        for row in selected:
            value = row[col] if col < len(row) else ""
            if value:
                parts.append(value)
        headers.append("/".join(parts).strip("/"))
    return headers


def _match_header(header: str) -> tuple[str | None, int]:
    if not header:
        return None, 0
    normalized = header.lower()
    best_key = None
    best_score = 0
    for canonical, aliases in HEADER_ALIASES.items():
        for alias in aliases:
            alias_norm = alias.lower()
            if normalized == alias_norm:
                score = 100 + len(alias_norm)
            elif alias_norm in normalized or normalized in alias_norm:
                score = len(alias_norm)
            else:
                continue
            if score > best_score:
                best_score = score
                best_key = canonical
    return best_key, best_score


def map_headers(headers: Sequence[str]) -> tuple[dict[int, str], int]:
    mapping: dict[int, str] = {}
    total_score = 0
    used: set[str] = set()
    for index, header in enumerate(headers):
        canonical, score = _match_header(header)
        if canonical and (canonical not in used or score >= 100):
            mapping[index] = canonical
            total_score += score
            used.add(canonical)
    return mapping, total_score


def _detect_header(rows: list[list[str]]) -> tuple[list[int], dict[int, str]]:
    candidates: list[tuple[int, list[int], dict[int, str]]] = []
    scan_limit = min(HEADER_SCAN_LIMIT, len(rows))
    for row_index in range(scan_limit):
        headers = _build_header_candidate(rows, [row_index])
        mapping, score = map_headers(headers)
        if score >= HEADER_SCORE_THRESHOLD and "failure_mode" in mapping.values():
            candidates.append((score, [row_index], mapping))
    for row_index in range(scan_limit - 1):
        combo = [row_index, row_index + 1]
        headers = _build_header_candidate(rows, combo)
        mapping, score = map_headers(headers)
        if score >= HEADER_SCORE_THRESHOLD and "failure_mode" in mapping.values():
            candidates.append((score, combo, mapping))
    if not candidates:
        raise ValueError("æœªèƒ½è¯†åˆ«è¡¨å¤´")
    candidates.sort(key=lambda item: (item[0], -len(item[1])), reverse=True)
    _, indices, mapping = candidates[0]
    return indices, mapping


def _prepare_rows(raw_rows: list[list]) -> list[list[str]]:
    processed: list[list[str]] = []
    for raw_row in raw_rows:
        processed.append([_clean_cell_value(cell) for cell in raw_row])
    return processed


def _iter_excel_rows(workbook_path: str) -> Iterable[tuple[str, list[list]]]:
    wb = load_workbook(workbook_path, data_only=True, read_only=False)
    for sheet in wb.worksheets:
        if sheet.sheet_state != "visible":
            continue
        rows: list[list] = []
        for row in sheet.iter_rows(values_only=True):
            if row is None:
                continue
            rows.append(list(row))
        if all(not any(cell is not None and str(cell).strip() for cell in row) for row in rows):
            continue
        yield sheet.title, rows


def _load_csv_rows(path: str) -> list[list]:
    encodings = ["utf-8-sig", "utf-8", "gbk", "gb2312"]
    for encoding in encodings:
        try:
            df = pd.read_csv(
                path,
                header=None,
                dtype=str,
                keep_default_na=False,
                encoding=encoding,
            )
            return df.fillna("").values.tolist()
        except Exception:
            continue
    raise ValueError("æ— æ³•è§£æCSVç¼–ç ")


def _build_issue_record(row_map: dict[str, str]) -> IssueRecord | None:
    failure_mode = row_map.get("failure_mode", "").strip()
    if not failure_mode:
        return None
    hash_input = "|".join(
        row_map.get(field, "").strip().lower() for field in REQUIRED_FIELDS
    )
    hash_key = hashlib.sha1(hash_input.encode("utf-8")).hexdigest()
    record_data: dict[str, str | None] = {
        "failure_mode": failure_mode,
        "root_cause": row_map.get("root_cause", ""),
        "prevention_action": row_map.get("prevention_action", ""),
        "detection_action": row_map.get("detection_action", ""),
        "hash_key": hash_key,
    }
    for field in OPTIONAL_FIELDS:
        record_data[field] = row_map.get(field) or None
    return IssueRecord(**record_data)


def _parse_issue_sheet(rows: list[list[str]]) -> tuple[list[IssueRecord], IssueSummary]:
    rows_prepared = [_horizontal_fill(row) for row in rows]
    header_indices, mapping = _detect_header(rows_prepared)
    data_start = max(header_indices) + 1
    width = max(len(row) for row in rows_prepared) if rows_prepared else 0
    summary = IssueSummary()
    records: list[IssueRecord] = []
    seen: set[str] = set()
    prev_values = ["" for _ in range(width)]
    for row_idx in range(data_start, len(rows_prepared)):
        raw_row = rows_prepared[row_idx]
        values = [_clean_cell_value(raw_row[col]) if col < len(raw_row) else "" for col in range(width)]
        for col, value in enumerate(values):
            if not value and prev_values[col]:
                values[col] = prev_values[col]
            else:
                prev_values[col] = value
        summary.total_rows += 1
        row_map: dict[str, str] = {}
        for col_index, canonical in mapping.items():
            value = values[col_index] if col_index < len(values) else ""
            row_map[canonical] = value
        record = _build_issue_record(row_map)
        if record is None:
            summary.skipped_rows += 1
            continue
        if record.hash_key in seen:
            summary.duplicates += 1
            continue
        seen.add(record.hash_key)
        records.append(record)
        summary.parsed_rows += 1
    return records, summary


def _parse_issue_list_spreadsheet(file_path: str) -> tuple[list[IssueRecord], IssueSummary]:
    ext = os.path.splitext(file_path)[1].lower()
    all_records: list[IssueRecord] = []
    summary = IssueSummary()
    if ext == ".csv":
        rows = _prepare_rows(_load_csv_rows(file_path))
        records, part_summary = _parse_issue_sheet(rows)
        summary.total_rows += part_summary.total_rows
        summary.parsed_rows += part_summary.parsed_rows
        summary.skipped_rows += part_summary.skipped_rows
        summary.duplicates += part_summary.duplicates
        all_records.extend(records)
        return all_records, summary
    for sheet_name, raw_rows in _iter_excel_rows(file_path):
        rows = _prepare_rows(raw_rows)
        try:
            records, part_summary = _parse_issue_sheet(rows)
            summary.total_rows += part_summary.total_rows
            summary.parsed_rows += part_summary.parsed_rows
            summary.skipped_rows += part_summary.skipped_rows
            summary.duplicates += part_summary.duplicates
            if not records:
                continue
            all_records.extend(records)
        except ValueError as error:
            raise ValueError(f"{sheet_name}: {error}") from error
    return all_records, summary


def _chunk_text(text: str, limit: int = LLM_CHUNK_LIMIT) -> list[str]:
    if len(text) <= limit:
        return [text]
    segments: list[str] = []
    current: list[str] = []
    current_len = 0
    for line in text.splitlines(keepends=True):
        line_len = len(line)
        if current and current_len + line_len > limit:
            segments.append("".join(current))
            current = [line]
            current_len = line_len
        else:
            current.append(line)
            current_len += line_len
    if current:
        segments.append("".join(current))
    return segments


def _call_llm_for_issue_lists(text: str, file_label: str, progress_area) -> tuple[list[IssueRecord], IssueSummary]:
    host = resolve_ollama_host("ollama_9")
    try:
        client = OllamaClient(host=host)
    except Exception as error:
        progress_area.error(f"æ— æ³•è¿æ¥ gpt-oss: {error}")
        return [], IssueSummary()
    model = CONFIG["llm"].get("ollama_model", "gpt-oss:latest")
    segments = _chunk_text(text, LLM_CHUNK_LIMIT)
    aggregated: list[IssueRecord] = []
    summary = IssueSummary()
    seen: set[str] = set()
    for index, segment in enumerate(segments, start=1):
        prompt = (
            f"ä»¥ä¸‹æ˜¯å†å²é—®é¢˜æ¸…å•æ–‡æœ¬ï¼ˆç¬¬{index}/{len(segments)}æ®µï¼Œæ–‡ä»¶: {file_label}ï¼‰ã€‚"
            "è¯·æå–é—®é¢˜æ¡ç›®å¹¶è¿”å›JSONæ•°ç»„ã€‚"
            "å­—æ®µå¿…é¡»åŒ…å«failure_modeã€root_causeã€prevention_actionã€detection_actionï¼Œ"
            "å…¶ä½™å¯æ ¹æ®å†…å®¹è¡¥å……ï¼Œå¯é€‰å­—æ®µä½¿ç”¨ç©ºå­—ç¬¦ä¸²ã€‚"
            "ç¦æ­¢è¾“å‡ºé™¤JSONå¤–çš„ä»»ä½•æ–‡æœ¬ã€‚\n\n" + segment
        )
        try:
            result = client.chat(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                options={"temperature": 0.1},
            )
        except Exception as error:
            progress_area.error(f"è°ƒç”¨ gpt-oss å¤±è´¥ ({file_label} ç¬¬{index}æ®µ): {error}")
            continue
        content = ""
        if isinstance(result, dict):
            content = result.get("message", {}).get("content") or result.get("response") or ""
        if not content:
            progress_area.warning(f"gpt-oss æœªè¿”å›å†…å®¹ ({file_label} ç¬¬{index}æ®µ)")
            continue
        try:
            parsed = json.loads(content)
        except json.JSONDecodeError as error:
            progress_area.error(f"gpt-oss è¿”å›å†…å®¹æ— æ³•è§£æJSON ({file_label} ç¬¬{index}æ®µ): {error}")
            continue
        if isinstance(parsed, dict) and "records" in parsed:
            parsed = parsed.get("records")
        if not isinstance(parsed, list):
            progress_area.warning(f"gpt-oss è¿”å›æ ¼å¼å¼‚å¸¸ ({file_label} ç¬¬{index}æ®µ)")
            continue
        summary.total_rows += len(parsed)
        for item in parsed:
            if not isinstance(item, dict):
                summary.skipped_rows += 1
                continue
            normalized = {
                "failure_mode": _clean_cell_value(item.get("failure_mode")),
                "root_cause": _clean_cell_value(item.get("root_cause")),
                "prevention_action": _clean_cell_value(item.get("prevention_action")),
                "detection_action": _clean_cell_value(item.get("detection_action")),
            }
            for field in OPTIONAL_FIELDS:
                normalized[field] = _clean_cell_value(item.get(field)) if field in item else None
            record = _build_issue_record(normalized)
            if record is None:
                summary.skipped_rows += 1
                continue
            if record.hash_key in seen:
                summary.duplicates += 1
                continue
            seen.add(record.hash_key)
            aggregated.append(record)
            summary.parsed_rows += 1
    return aggregated, summary


def _process_issue_lists(source_dir: str | None, output_dir: str, progress_area) -> list[str]:
    os.makedirs(output_dir, exist_ok=True)
    if not source_dir or not os.path.isdir(source_dir):
        progress_area.warning("æœªæ‰¾åˆ°å†å²é—®é¢˜æ¸…å•ä¸Šä¼ ç›®å½•ï¼Œå·²è·³è¿‡ã€‚")
        return []
    created: list[str] = []
    files = [
        os.path.join(source_dir, name)
        for name in sorted(os.listdir(source_dir))
        if os.path.isfile(os.path.join(source_dir, name))
    ]
    if not files:
        progress_area.info("å†å²é—®é¢˜æ¸…å•ç›®å½•ä¸ºç©ºã€‚")
        return created
    for file_path in files:
        name = os.path.basename(file_path)
        ext = os.path.splitext(name)[1].lower()
        progress_area.write(f"å¤„ç†å†å²é—®é¢˜æ¸…å•: {name}")
        try:
            if ext in SPREADSHEET_EXTENSIONS:
                records, summary = _parse_issue_list_spreadsheet(file_path)
            else:
                text_output_dir = os.path.join(output_dir, "_tmp_txt")
                os.makedirs(text_output_dir, exist_ok=True)
                txt_path = os.path.join(text_output_dir, f"{name}.txt")
                if ext in PDF_EXTENSIONS:
                    zip_bytes = _mineru_parse_pdf(file_path)
                    _zip_to_txts(zip_bytes, txt_path)
                elif ext in WORD_PPT_EXTENSIONS:
                    _unstructured_partition_to_txt(file_path, txt_path)
                else:
                    with open(file_path, "rb") as f:
                        content = f.read()
                    try:
                        text = content.decode("utf-8")
                    except UnicodeDecodeError:
                        text = content.decode("gbk", errors="ignore")
                    os.makedirs(os.path.dirname(txt_path), exist_ok=True)
                    with open(txt_path, "w", encoding="utf-8") as tmp:
                        tmp.write(text)
                if not os.path.exists(txt_path):
                    progress_area.warning(f"æœªç”Ÿæˆæ–‡æœ¬ï¼Œè·³è¿‡: {name}")
                    continue
                with open(txt_path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
                records, summary = _call_llm_for_issue_lists(text, name, progress_area)
            payload = {
                "summary": summary.to_dict(),
                "records": [record.model_dump() for record in records],
            }
            out_path = os.path.join(output_dir, f"{name}.txt")
            with open(out_path, "w", encoding="utf-8") as out_f:
                json.dump(payload, out_f, ensure_ascii=False, indent=2)
            created.append(out_path)
            progress_area.success(
                f"å·²ç”Ÿæˆ: {os.path.basename(out_path)} â€” è§£æ{summary.parsed_rows}/{summary.total_rows}æ¡"
            )
        except ValueError as error:
            progress_area.error(f"{name}: {error}")
        except Exception as error:
            progress_area.error(f"å¤„ç† {name} å¤±è´¥: {error}")
    return created


def render_history_issues_avoidance_tab(session_id):
    if session_id is None:
        st.warning("è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        return

    st.subheader("ğŸ“‹ å†å²é—®é¢˜è§„é¿")

    base_dirs = {
        "generated": str(CONFIG["directories"]["generated_files"]),
    }
    session_dirs = ensure_session_dirs(base_dirs, session_id)

    issue_lists_dir = session_dirs.get("history_issue_lists")
    dfmea_dir = session_dirs.get("history_dfmea")
    pfmea_dir = session_dirs.get("history_pfmea")
    cp_dir = session_dirs.get("history_cp")
    generated_root = session_dirs.get("generated_history_issues_avoidance")
    if not generated_root:
        generated_base = session_dirs.get("generated")
        if generated_base:
            generated_root = os.path.join(generated_base, "history_issues_avoidance")
            os.makedirs(generated_root, exist_ok=True)

    upload_targets = [
        {"label": "å†å²é—®é¢˜æ¸…å•", "key": "issue_lists", "dir": issue_lists_dir},
        {"label": "DFMEA", "key": "dfmea", "dir": dfmea_dir},
        {"label": "PFMEA", "key": "pfmea", "dir": pfmea_dir},
        {"label": "æ§åˆ¶è®¡åˆ’ (CP)", "key": "cp", "dir": cp_dir},
    ]

    col_main, col_info = st.columns([2, 1])

    with col_main:
        st.markdown("è¯·ä¸Šä¼ å†å²é—®é¢˜æ¸…å•ã€DFMEAã€PFMEA ä¸æ§åˆ¶è®¡åˆ’æ–‡ä»¶ã€‚æ”¯æŒ PDFã€Word/PPTã€Excel ç­‰æ ¼å¼ã€‚")

        upload_columns = st.columns(2)
        for index, target in enumerate(upload_targets):
            column = upload_columns[index % len(upload_columns)]
            with column:
                uploaded_files = st.file_uploader(
                    f"ç‚¹å‡»ä¸Šä¼  {target['label']}",
                    type=None,
                    accept_multiple_files=True,
                    key=f"history_upload_{target['key']}_{session_id}",
                )
                if uploaded_files:
                    handle_file_upload(uploaded_files, target["dir"])
                    st.success(f"å·²ä¸Šä¼  {len(uploaded_files)} ä¸ª {target['label']} æ–‡ä»¶")

        st.divider()

        controls_row1 = st.columns(3)
        start_pressed = controls_row1[0].button(
            "â–¶ï¸ å¼€å§‹",
            key=f"history_start_{session_id}",
            use_container_width=True,
        )
        controls_row1[1].button(
            "â¸ æš‚åœ",
            key=f"history_pause_{session_id}",
            use_container_width=True,
        )
        controls_row1[2].button(
            "ğŸ¬ æ¼”ç¤º",
            key=f"history_demo_{session_id}",
            use_container_width=True,
        )
        controls_row2 = st.columns(2)
        controls_row2[0].button(
            "â¹ åœæ­¢",
            key=f"history_stop_{session_id}",
            use_container_width=True,
        )
        controls_row2[1].button(
            "â–¶ï¸ æ¢å¤",
            key=f"history_resume_{session_id}",
            use_container_width=True,
        )

        if start_pressed:
            area = st.container()
            with area:
                if not generated_root:
                    st.error("æœªèƒ½åˆå§‹åŒ–ç”Ÿæˆæ–‡ä»¶ç›®å½•ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
                else:
                    st.info("å¼€å§‹å¤„ç†æ–‡ä»¶ï¼šPDF ä½¿ç”¨ MinerUï¼ŒWord/PPT ä½¿ç”¨ Unstructuredï¼Œå†å²é—®é¢˜æ¸…å•è§£æä¸º JSONã€‚")
                    generated_dirs = _ensure_generated_dirs(generated_root)
                    total_created: list[str] = []
                    total_created.extend(
                        _process_issue_lists(
                            issue_lists_dir,
                            generated_dirs["issue_lists"],
                            area,
                        )
                    )
                    total_created.extend(
                        _process_category(
                            "DFMEA",
                            dfmea_dir,
                            generated_dirs["dfmea"],
                            area,
                        )
                    )
                    total_created.extend(
                        _process_category(
                            "PFMEA",
                            pfmea_dir,
                            generated_dirs["pfmea"],
                            area,
                        )
                    )
                    total_created.extend(
                        _process_category(
                            "æ§åˆ¶è®¡åˆ’ (CP)",
                            cp_dir,
                            generated_dirs["cp"],
                            area,
                        )
                    )
                    if total_created:
                        st.success("æ–‡æœ¬è§£æå®Œæˆã€‚")
                        kb_id = _sync_history_kb(
                            session_id,
                            {
                                "DFMEA": generated_dirs.get("dfmea", ""),
                                "PFMEA": generated_dirs.get("pfmea", ""),
                                "æ§åˆ¶è®¡åˆ’": generated_dirs.get("cp", ""),
                            },
                            area,
                        )
                        _evaluate_history_problems(
                            generated_dirs.get("issue_lists", ""),
                            generated_dirs.get("initial_results", ""),
                            generated_dirs.get("final_results", ""),
                            kb_id,
                            session_id,
                            area,
                        )
                    else:
                        st.info("æœªç”Ÿæˆä»»ä½•æ–‡æœ¬æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ä¸Šä¼ å†…å®¹åé‡è¯•ã€‚")

    with col_info:
        st.subheader("ğŸ“ ä¸Šä¼ æ–‡ä»¶")

        clear_columns = st.columns(2)
        for index, target in enumerate(upload_targets):
            column = clear_columns[index % len(clear_columns)]
            with column:
                if st.button(
                    f"ğŸ—‘ï¸ æ¸…ç©º{target['label']}",
                    key=f"history_clear_{target['key']}_{session_id}",
                ):
                    try:
                        if target["dir"] and os.path.isdir(target["dir"]):
                            for name in os.listdir(target["dir"]):
                                path = os.path.join(target["dir"], name)
                                if os.path.isfile(path):
                                    os.remove(path)
                        st.success(f"å·²æ¸…ç©º {target['label']} æ–‡ä»¶")
                        st.rerun()
                    except Exception as error:
                        st.error(f"æ¸…ç©ºå¤±è´¥: {error}")

        tabs = st.tabs([target["label"] for target in upload_targets])
        for tab, target in zip(tabs, upload_targets):
            with tab:
                files = _collect_files(target["dir"])
                if not files:
                    st.write("ï¼ˆæœªä¸Šä¼ ï¼‰")
                    continue
                for info in files:
                    display = _truncate_filename(info["name"])
                    with st.expander(f"ğŸ“„ {display}", expanded=False):
                        col_meta, col_actions = st.columns([3, 1])
                        with col_meta:
                            st.write(f"**æ–‡ä»¶å:** {info['name']}")
                            st.write(f"**å¤§å°:** {_format_file_size(info['size'])}")
                            st.write(f"**ä¿®æ”¹æ—¶é—´:** {_format_timestamp(info['modified'])}")
                        with col_actions:
                            delete_key = (
                                f"history_delete_{target['key']}_{info['name'].replace(' ', '_').replace('.', '_')}_{session_id}"
                            )
                            if st.button("ğŸ—‘ï¸ åˆ é™¤", key=delete_key):
                                try:
                                    os.remove(info["path"])
                                    st.success(f"å·²åˆ é™¤: {info['name']}")
                                    st.rerun()
                                except Exception as error:
                                    st.error(f"åˆ é™¤å¤±è´¥: {error}")
